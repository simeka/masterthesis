\chapter{Discussion}
Within this thesis the implementation of deep learning methods for \gls{snn} has been demonstrated on analog neuromorphic hardware. A rate-based approach was successfully executed as an on-chip experiment on the \gls{dls}. A second temporal-based method, SuperSpike, was implemented on a full-size prototype of the \gls{bss2} platform and performed well at solving an XOR-related problem on two distinct \gls{hx} setups.

\paragraph{Rate Coding with SGD} An on-chip realization of gradient descent for a rate-based \gls{snn} was developed for the \gls{dls}. Despite continuing stability issues of the \gls{fpga} and imperfections of the analog hardware, the Circles dataset could be classified with an accuracy of $(99.6 \pm 0.8) \%$. This experiment resembles the first successfull on-chip implementation of a deep learning task on the analog \gls{bss2} platform. However, a transferability of the results to other setups has not yet been achieved, since an extensive fine-tuning of the new setup would be required first.

A main improvement of the task can be achieved by transferring the implementation to the most recent chip version. The \gls{hx} features dedicated on-chip noise-generators with which a sigmoid activation function can be easily realized. Moreover, the difficulty of the task can be increased as with the newly available neuron resources, 512 instead of 32, the network can be scaled up with respect to the number and size of the individual layers.

The tape-out of the new chip version has shifted the research focus within the group. To that end, the software development for the \gls{dls} has been halted. The older chip is still capable of performing remarkably well, but its successor outperforms it in several regards. With the promising prospects given by the new chip no further improvements are planned for the current implementation.

%Transfer function needs to be non-linear the exact shape and its dependencies on noise rate, time constants, .. is not so important if it is somehow shaped like a non linear activation function.
%- working point of membrane -> noise and spread as soon as the working point is left (see transfer function with bias plot) -> not so bad actually for training. the error is trained as well. 
%- init conditions of task are important. somehow neurons dont learn if the init condition is "bad". but what does that even  mean.. the init condition is bad. the behaviour: the neuron stays silent throughout the learning process. 
%- ausblick: notes from mfp talk maybe?
\subsubsection{Temporal Coding with SuperSpike}
% blabla about why not on chip
Unlike the previous experiment, SuperSpike is not implemented in an on-chip fashion. ..explain what is all missing to do it...sth similar to the blackbox should be on-chip. ppu doesnt know the spike times of neither input nor output nor hidden layer. on-chip calibration. not so much agility when trying out and exploring things. numpy and python is quite convenient.
\subsubsection*{Future Projects}
\label{futureprojects}

%For the scope of this thesis a more advanced model is not yet required. All experiments are done using the simpler \gls{lif} model. However, an \gls{adex} based implementation of similar experiments are part of a future project (see \cref{futureprojects}).