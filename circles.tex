\chapter{Classification of the Circles Data Set on \acrshort{bss2}}
\label{circles}
At the beginnings of the machine learing era, \cite{perceptron} have shown that a simplified single-layer neuronal network, the perceptron, cannot solve the XOR problem or ``exclusive or". The XOR operator represents a class of problems, that are non-linear and thus require a multi-layer network structure with a non-linear activation function to be solved (\citealp{Goodfellow-et-al-2016}). In modern machine learning solving the XOR problem has therefore become a very first benchmark to test the functionality of novel network designs and algorithms.

\begin{wrapfigure}{R}{0.33\textwidth}
	\centering
	\input{figures/circles_task.pgf}
	\caption[Circles data set.]{Circles data set. Two circles are separated by a decision boundary (DB)} 
	\label{circlestasksketch}
\end{wrapfigure}
The classification of the \emph{Circles} data set is the first of two deep learning experiments implemented on the \gls{bss2} platform and focuses on a rate coding approach which has been motivated in \cref{ratecoding}. After successfully testing an on-chip \acrlong{sgd} implementation on the prototype \gls{dls} with a XOR problem, the difficulty of the task has been increased with the Circles data set. The task is implemented again as an on-chip experiment, i.e. the training of the network is executed using only on-chip available resources, such as the dedicated \acrlong{ppu}. Only for monitoring purposes of the training progress the chip's memomry is continuously read-out through the FPGA interface.


%A detailed description of the \textit{on-chip} implementation is presented in the next paragraphs.

%- single hidden layer network with 11 neurons (2 synapse lines necessary for each neuron -> 22)
%- 2 noise sources (exc/inh)
%- 2x2 input sources (exc/inh)
%(A single hidden layer network is trained using rate coding and feedback alignment.) maybe restructure the intro somehow...


\section{Circles Task}
\label{circlestask}

The Circles data set from \cref{circlestasksketch} describes in principle a set of points $p = p(x,y)$ in a two-dimensional plane, which are in either of two disjunct annuli, each representing a class
\begin{align}
\text{class}(p) =
\begin{cases}
0 ,&\quad \quad r_{\text{inner}}^2 < x^2 + y^2 < r_{\text{outer}}^2, \\
1 ,&\quad \quad R_{\text{inner}}^2 < x^2 + y^2 < R_{\text{outer}}^2,
\end{cases}
\end{align}
with the confining inner and outer radiuses of the first and second annulus $r_{\text{inner}}$, $r_{\text{outer}}$ and  $R_{\text{inner}}$, $R_{\text{outer}}$ respectively. The goal of the task is to find a decision boundary that successfully separates the both annuli.

For a rate-based task, the data points need to be translated into firing rates. A point $p$ is represented by two signed \SI{8}{\bit} integers resembling its coordinates $x$ and $y$ which fall in the range $x, y \in [-128,127]$. Each point is then mapped onto a frequency depending on either of the coordinates. The rate dependency of a coordinate $c \in \{x,y\}$ is given by
\begin{equation}\label{inputfrequency}
\nu_{\text{input, c}}(c) = \nu_\text{max} \cdot \frac{c + 128}{255},
\end{equation}
with the maximum firing rate $\nu_\text{max}$ that is given by a Poisson spike train source. 



The circles task slightly modified to reduce the implementation effort on the \gls{ppu} and to make it easier to find a decision boundary. The class $0$ is reduced to one confining circle by setting the inner radius to zero. The other class can also be reduced to one confining conditions by replacing the outer radius with the \SI{8}{\bit} limitations of the input. The remaining two boundaries are set far apart to $r_{\text{outer}} = \sqrt{8000}$ and $R_{\text{inner}} = \sqrt{13000}$. The area inbetween the classes is where the network will try to place the decision boundary. The more space is available, the easier it is for the network to succeed.

In \cref{circlesinputs} the frequency dependent input and the final shape of the input classes is illustrated by a representative data set of 100 samples per class. For a data point from the lower right corner e.g. $p(100,-100)$. The corresponding input rate generated by the first input node is given by $\nu_{\text{in, x}}(100) \approx \SI{450}{\kilo \Hz}$. The second node yields $\nu_{\text{in, y}}(-100) \approx \SI{55}{\kilo \Hz}$.
\begin{figure}[htb!]
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		%		\caption{}
		\inputpgf{figures}{nu_x_input.pgf}
		%		\label{nuxinput}
	\end{subfigure}
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		%		\caption{}
		\inputpgf{figures}{nu_y_input.pgf}	
		%		\label{nuyinput}
	\end{subfigure}
	\caption[Rate coding for the circles experiment.]{Rate coding for the circles experiment. Each point $p(x,y)$ of a representative validation data set with 100 points per class is mapped to a frequency depending on the x-direction. The figure resembles one of two input nodes in the input layer of the network. The second input node depends on the y-direction.}
	\label{circlesinputs}
\end{figure}
To identify the classes in the output layer only a single unit is required. Each data point $p$ yields a certain output firing rate $\gls{nuout}(p)$ in the output layer, which is compared to a target rate $\nu^*$ depending on the class of point $p$
\begin{align}
\nu^*(p) =
\begin{cases}
\nu_0^* = \SI{32.6}{\kilo \Hz} ,&\quad \quad \text{class}(p) = 0,\\
\nu_1^* = \SI{93.5}{\kilo \Hz} ,&\quad \quad \text{class}(p) = 1.
\end{cases}
\label{circlestarget}
\end{align}
The mismatch between the target and firing rate then determines the error of the output layer $e^{(o)}$
\begin{equation*}
e^{(o)}(p) = \gls{nuout}(p) - \nu^*(p).
\label{circleserror}
\end{equation*}
A reasonable choice of the target rates is crucial to the training success. If the rates are either too close or too far from each other, the network will have troubles to solve the task. The The decision boundary, the rate separating both classes, is chosen to be the mean of both target rates
\begin{equation*}
\nu_\text{DB} = \frac{\nu_0^* + \nu_1^*}{2} = \SI{63.05}{\kilo \Hz}.
\label{circlesdb}
\end{equation*}


\section{Poisson Spike Train Generator}
\label{poissonspiketrains}
In the introduction to rate coding (\cref{ratecoding}) the mean firing rate $\nu$ of a neuron has been defined by the number of spikes $n_\text{spikes}$ sent within a period $T$
\begin{equation*}
\nu = \frac{n_\text{spikes}}{T},
\end{equation*}
and it was mentioned that this formalism is only well defined, if the temporal distribution of the spikes within a spike train is based on Poisson processes.

One way to numerically generate such a Poisson spike train is to repeatedly perform a Bernoulli process on a short time interval $\Delta t$ over a period $T$. The Bernoulli process is equivalent to an unfair coin flip with probability $p$, which is set depending on the desired firing rate $\nu$ by $p(\nu) = \nu \cdot \Delta t$. Given that the probability equals one, i.e. at every time interval a spike will be fired, the firing rate reaches its maximum at $\nu_\text{max} = \nicefrac{1}{\Delta t}$.

With respect to the hardware limitations, in particular the time required to generate the shortest possible spike on the \gls{ppu} $\Delta t_\text{spike}$, the maximum firing rate is limited to 
\begin{align*}
\nu_\text{max, ppu} = \frac{1}{\Delta t_\text{spike}} = \frac{1}{\SI{0.44}{\mu \s}} = \SI{2.27}{\mega \Hz}.
\end{align*}
The duration of the shortest configurable spike on the \gls{ppu} is determined by measuring the duration of a constant spiking pattern with the highest possible frequency and for a known number of spikes. The number of spikes is determined by the on-chip spike counters and the duration is simply measured by recording the membrane potential with an oscilloscope.

However, at maximum rate, the spike generation can no longer be assumed to follow a Poisson Distribution and should only be used for frequencies $\nu \ll \nu_\text{max, ppu}$.

% Stopped here, rewrite again not schlÃ¼ssig yet...
The use case of the spike generator in the final setup is to generate noise spike trains for the sigmoid transfer function and to map the translate the circles data set into input rates. Only the latter requires high input rates up to $\SI{500}{\kilo \Hz}$, the noise rate is only in order of a few $\si{\kilo \Hz}$.

%In theory the maximum fire rate of a \gls{lif} neuron $\nu_\text{max}$ is give by $\nu_\text{max} = \nicefrac{1}{\gls{refrac}}$.
%In the final experiment setup, the output rate of an individual neuron is further constraint by a chosen 8-bit resolution of the spike counts and a measurement period of a few milliseconds to a measurable rate in order of few $\SI{100}{\kilo \Hz}$.
%will be divided into four independent channels, each producing at max a fourth of $\nu_\text{max, hw}$.
On the \gls{ppu} the Bernoulli process is implemented by comparing the frequency dependent probability $p(\nu)$ with a randomly drawn number. A popular method to generate random numbers on a system with limited memory and computational resources is the \emph{xorshift} (\citealp{marsaglia2003xorshift}). A random number is thereby generated by repeatedly applying the XOR operator on a seed variable and a bit-shifted versions of itself. The \emph{bit shift} of a variable $x$ to the left by $n$, written as $x\;{\scriptstyle<<}\;n$, corresponds to the multiplication by $2^n$. In analogy, a shift the right, $x\;{\scriptstyle>>}\;n$, is equivalent to the division by $2^n$. The xorshift can be implemented in C as follows.

%\begin{minted}{C}
%uint32_t xorshift32(uint32_t* seed)
%{
%*seed ^= *seed << 13;
%*seed ^= *seed >> 17;
%*seed ^= *seed << 5;
%return *seed;
%}
%\end{minted}

\section{Transfer Function on Chip}
The theoretical ground for a sigmoid activation function for a \gls{lif} neuron has been motivated in \cref{ratebasedtraining}: The continuous stimulation with Poisson spike trains leads to a Gaussian free membrane potential distribution $f_{\gls{v_mem}}$. In \cref{vleak_w_noise} the measured membrane potential on the hardware is shown, using noise spike trains with a frequency of \SI{70}{\kilo \Hz} and a synaptic strength of $w_\text{noise}= \pm 15$.

%The continuous stimulation with Poisson spike trains leads to a Gaussian free membrane potential distribution $f_{\gls{v_mem}}$ centered around the resting potential \gls{v_leak} as depicted in \cref{vleak_w_noise}). In a naive approach, the part of the distribution that exceeds a certain threshold potential correlates to the number of fired spikes. This neglects non vanishing effects from the fire dynamics of the membrane which have been investigated in more detail by \citealp{petrovici12phdthesis}. The impact of these dynamics can be reduced by the use of very short time constants for the synaptic input \gls{tau_syn} and the membrane \gls{tau_m}.
%
%However, despite the strongly simplified picture, this view still offers a correct intuition how the threshold and leak potential as well as the strength of the noise effect the free membrane potential and in turn change the shape of the transfer function: more noise leads to a broader distribution and thus a more gently incline of the sigmoid; synaptic input moves the distribution to either to a lower or higher mean value; moving the threshold corresponds to an additional bias term.
\begin{figure}
	\begin{center}
		\input{figures/activation_function_vmem_distr_with_thres.pgf}
	\end{center}
	\caption[Gaussian free membrane potential distribution on \gls{dls}.]{Gaussian free membrane potential distribution on \gls{dls}. In a naive approach, the part of the distribution that exceeds a certain threshold potential correlates to the number of fired spikes, which yields a perfect sigmoid for the activation function in return. The width of the distribution correlates to amount and impact of the injected noise spikes.}
	\label{vleak_w_noise}
\end{figure}

 The overall activation function then depends on several parameters
\begin{equation}
\gls{transfer} = \gls{transfer}(\gls{nuin}, b, \gls{refrac}, \gls{tau_m}, \gls{v_reset}, f_{\gls{v_mem}}),
\end{equation}
with the free membrane distribution $f_{\gls{v_mem}} = f_{\gls{v_mem}}(V; \gls{v_leak}, \nu_\text{noise}, w_\text{noise})$. 
The bias term $b$ in the neuron's activation is again replaced by the relative distance $\delta V$ between the resting potential and the threshold, since $b \propto \delta V = \gls{v_leak} - \gls{thres}$. 

Some parameters will be kept fixed, whereas others are required to operate within a certain range such as the threshold or the input frequency. The setup is hand tuned with the support of parameter sweeps to find a suitable configuration of the fixed parameters. The final set of parameters is displayed in \cref{totable}

\subsubsection*{Calibration}\label{calibration}
The manufacturing process of analog neuromorphic hardware causes systematic stochastic deviations in the neuron and synapse parameters. The thereby induced heterogeneity between neurons and synapses is referred to as \emph{fixed-pattern noise} and is constant in time. Despite the detuned parameters, it has been shown that plasticity rules can correct the intrinsic imperfections of the analog hardware to a certain degree (\citealp{wunderlich2019advantages}).

In an uncalibrated state, the neuron circuits exhibit transfer functions which do not align with regards to their maximum rate and the bias with which the sigmoid can be moved along the x-axis (see \cref{transferfunction_wout_calib}). Up to a certain degree the misalignment will be corrected during the training process. However, the dynamic range of the individual neurons on the chip is limited and to ensure properly overlapping activations function a calibration of the respective parameters is inevitable.
\begin{figure}
	%\captionsetup[subfigure]{justification=centering}
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\caption{}
		\input{figures/uncalibrated_activation_function.pgf}
		\label{transferfunction_wout_calib}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\textwidth}		
		\caption{}
		\input{figures/calibrated_activation_function.pgf}
		\label{transferfunction_w_calib}
	\end{subfigure}
	\caption[Calibration of the sigmoid activation function on \acrshort{dls}.]{Calibration of the activation function on \acrshort{dls}. \textbf{(\subref{transferfunction_wout_calib})}: In the uncalibrated state the maximum (\gls{refrac}) and center rate (\gls{v_leak}) deviates due to imperfection in the manufacturing process of the analog hardware.  \textbf{(\subref{transferfunction_w_calib})}: The calibration aligns the dynamic range of individual neurons, which will helps the training.}
\end{figure}

We have implemented a collection of simple, experiment-specific calibration routines, which are executed on the PPU to maintain a pure on-chip implementation. They are based on a binary search algorithm on-chip calibration uses a binary search algorithm to find suitable \gls{dac}-values for the analog neuron parameters (\citealp{binarysearchsource}). In general, the binary search algorithm compares a target value to the middle value of a given sorted array. Depending on the outcome of the comparison the lower or the upper half is eliminated from the search space and the search is repeated with new boundaries which are set by the remaining half. In a worst case scenario, the algorithm finds the target value after $\mathcal{O}(\log(n))$ repetitions, given the searched array has $n$ entries. Especially in the context of analog circuits it is very unlikely to find the target value, but it continuously approximates the searched value under a chosen assumptions.

For the calibration of the transfer function, the sorted array is replaced by the range of the 10-bit \gls{dac}-value and instead of direct comparison, the measured output rate of a neuron using the updated analog parameter is compared to a target rate.

In case of the maximum firing rate, the calibration is only depends in the limit of a high input rate only on one parameter, the refractory period \gls{refrac} (c.f. \cref{fireratehigh}). The maximum output rate is measured using a high input rate, zero bias and no noise
\begin{equation}
\gls{transfer}(\nu_\text{max, hw}, 0, \gls{refrac}, f_{\gls{v_mem}}) = \frac{1}{\gls{refrac}} \quad \text{ with } \nu_\text{noise} = 0.
\end{equation}
The target rate of the maximum output rate $\gls{nuout}$ is set to roughly \SI{111.3}{\kilo \Hz} which corresponds to $256$ recorded spikes over a measurement period of $T=\SI{2.3}{\milli \s}$.

The alignment along the x-axis is slightly more complicated. A neuron approximately yields half of its maximum rate if there is input except for the noise sources and the bias is set to zero, i.e. $\delta V = 0 \Leftrightarrow \gls{v_leak} = \gls{thres}$
\begin{equation}
\gls{transfer}(0, 0, \gls{refrac}, f_{\gls{v_mem}}) = \frac{1}{2\gls{refrac}} \quad \text{ with } \nu_\text{noise} = \SI{70}{kHz}.
\end{equation}

In a practical observation, the center of the sigmoid activation function could be placed at zero input by balancing a fixed position of the reset potential with a minor correction of the bias. This was achieved by formulating the condition of the binary search for the bias related parameter in a way that at zero input the activation function returns half the maximum rate. For a higher output rate, the bias had to be reduced and vice versa. On chip, a change of the bias is applied by adapting the \gls{dac} value of the resting potential while keeping the threshold fixed at chosen work point of approximately \SI{0.54}{\V}.

The choice of a work point for the absolute threshold and resting potential should in theory be arbitrary, as the whole operating range can be shifted. On analog hardware, this does not hold true, since some circuits exhibit limited operating ranges.

The calibration of the maximum firing rate and the centered alignment of the activation function around zero input is repeated and interleaved several times, to reduce the potential influence of the other yet uncalibrated parameter. In \cref{transferfunction_w_calib} the final calibrated state is shown. 



\begin{figure}
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\input{figures/single_calibrated_transfer_function_w_various_weights.pgf}
		\label{dlsactivationfunctionweight}
	\end{subfigure}	
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\input{figures/activation_function_w_bias.pgf}
		\label{dlsactivationfunctionbias}
	\end{subfigure}
	\caption[Measurement of the sigmoid transfer function on \gls{dls}]{Measurement of the sigmoid transfer function on \gls{dls}. A sigmoid activation function can be achieved by applying additional Poisson noise spike trains onto the membrane. \textbf{(\subref{dlsactivationfunctionweight})}: The dependency of the input weight is given due to its direct proportionality to the synaptic input current $\gls{isyn} \propto w \nu_\text{in}$. \textbf{(\subref{dlsactivationfunctionbias})}: Changing the value of \gls{thres} and thereby the potnetial difference $\delta V$ to resting potential is interchangeable with adding a bias to the synaptic input of the neuron. The activation function is thereby shifted in either direction along the x-axis. This has been measured for all neurons in use for the circles task. The shifted activation functions do not need to align per neuron, since the threshold parameter is set individually and can be easily adapted during the training.}
\end{figure}
%
%\begin{figure}
%	\begin{center}
%		\input{figures/activation_function_w_bias.pgf}
%	\end{center}
%	\caption[Sigmoid transfer function on \gls{dls}.]{Sigmoid transfer function on \gls{dls}. The resulting transfer function of the broadened free membrane distribution is set to  Shifting \gls{thres} and thereby changing the relative potential difference $\delta V$ is interchangeable with adding a bias term to the \gls{lif} neuron's activation.}
%	\label{transferfunction_with_bias}
%\end{figure}

%The bias term $b$ is again replaced by the relative distance $\delta V$ between the resting potential and the threshold $b \propto \delta V$. As the analog core brings more and less favorable operating points, the resting potential is kept constant and only the threshold is shifted to change the bias (c.f. \cref{transferfunction_with_bias}).



\section{Experiment Setup on \gls{dls}}
\label{circlesimplementation}
The circles task is solved by training a single hidden layer network on-chip using \acrlong{sgd}. The \gls{ppu} controls the experiment, including a forward and a backward pass. After a proper initialization routine, the training loop is started: the corresponding input rates of a randomly drawn data point $p(x,y)$ are fed into the network and the resulting output rates are evaluated by the \gls{sgd} routine on the \gls{ppu}. The computed parameter updates are then applied before the next iteration can start.

\subsubsection*{Forward Pass}

\begin{wrapfigure}{R}{0.35\textwidth}
	\centering
	\input{dlsv2overview.tex}
	\caption[Overview of the synapse array.]{Overview of the synapse array. Figure adapted from Sebastian Billaudelle} 
	\label{synapsearraysketch}
\end{wrapfigure}
The neuromorphic core of the \gls{dls} provides 32 analog neuron circuits which are interconnected by synapse array of $32 \times 32$ synapses (c.f. \cref{synapseschematics}). Any input to the network is injected row-wise by dedicated synapse drivers as either excitatory or inhibitory spikes. At a column, the label of an incoming spike is compared with local address field and in case of a match the spike is relayed down to the designated neuron.

As for \glspl{ann}, the synapse's weight must be able to seamlessly evolve from negative to positive and vice versa. By design a single synapse cannot alternate between the signs of the input without changing the synapse driver's configuration which is unfortunately time-consuming. Instead of a single input row, two rows are used for the same input - one in excitatory and one inhibitory mode. In each column only one of the double-synapses is active and corresponds to the current sign of the synaptic weight. As the weight changes the sign, the active synapse is silenced and the other one is reactivated. 

The first layer of the network, the input layer, is connected to the hidden units by using a total of four rows and eleven columns. A dedicated event router injects the generated spikes by the hidden neurons back into the network, with each hidden neuron requiring another double-row. The recurrent connections of the hidden layer are then summed by a single column over the respective $11 \times 2$ rows into the output unit. The weight of the noise sources remains fixed and thus they only require a single row, one for excitatory and one for inhibitory noise spikes.

A Poisson spike train generator on the \gls{ppu} provides a stream of random spikes over a certain measurement period. The provided spike resources are split into four branches, two for the excitatory and inhibitory noise source and two for both input units. Each noise branch has to supply twelve neurons with Poisson spike trains. In an attempt to reduce cross-correlation between the individual neurons the branches are further divided into eight channels and thus only four will share the noise with other ones.

%With the double-row structure, a total of six rows is used for the various inputs: two for the noise generation and four for the two-dimensional input. The recurrent connections of the hidden layer require another $22$ rows. In \cref{network_structure} the exact row assignment is sketched. Other than during the calibration process, where each neuron was calibrated individually, now the noise source needs to provide Poisson spike trains for all neurons in the hidden and output layer. As a compromise the source is divided further into eight channels. Four neurons will share the noise with other ones. The frequency of the noise in both layers is set to $\nu_\text{noise} \approx \SI{1.1}{\kilo \Hz}$ using weights with $w_\text{noise} = \pm 15$. The received input frequency varies for each layer. The input layer provides an input rate of up to $2 \times \SI{500}{\kilo \Hz}$ for each hidden unit. The output frequency of hidden units is limited by the spike counter and measurement period to $\gls{nuout} \approx \SI{111.3}{\kilo \Hz}$, leading to a theoretical maximum of $\gls{nuin}= 11 \times \gls{nuout} \approx 1219.6$ for the output layer. 
%A presynaptic spike train, e.g. generated by the \gls{ppu}, is injected row-wise into the $32\times32$ synapse array. Then, the synapses compare a dedicated address label of the incoming spike with their own. Once they match, the spike is weighted and forwarded to the column-wise connected neurons at the bottom as either inhibitory or excitatory current. Recurrent connections are made available by the use of a spike router, which routes the postsynaptic spike trains generated by a neuron back into the synapse array.
%A single input row cannot provide alternating negative and positive inputs to the same neuron. Changing the synapses configuration simply costs too much time. To ensure a continuous evolution of the weight from negative to positive, two rows are used simultaneously for the same input - one in excitatory and one inhibitory mode. As the weight changes sign, the unused row is set silent. At the start of the experiment the weights are randomly initialized using a uniform distribution limited to $w_{ij} \in [-25, 25]$.
%The initial conditions of the network have been proven to crucial for its learning success. A ``silent" hidden units has a hard to time become active. Adding a weight depending term to the initial zero bias $b_i^\text{init} \propto \sum_j w_{ij}$ establishes a certain initial fire rate and solves the problem for most cases.

\subsubsection*{Backward Pass}
The \textbf{backward pass} evaluates the output rates of each node in the network that have been obtained by the forward pass and computes an update for weights and biases according to \glsfirst{sgd} (\cref{stochasticgradientdescent}).

The general-purpose unit of the \gls{ppu} operates with a 32-bit architecture without hardware-support to floating point types. To increase the precision of the parameter updates, the computation is bit-shifted to the left. The calculated results are then stochastically rounded and and shifted back right before being applied to the hardware. Stochastic rounding has been proven to be a viable workaround for deep learning with parameters of limited precision (\citealp{limitedprecisionpaper}). Typical methods of directed rounding to an integer $x$ are rounding up $\lceil x\rceil$, down $\lfloor x\rfloor$ or to the nearest neighbor $\lceil\frac{\lfloor2x\rfloor}{2}\rceil$. Stochastic rounding on the other hand rounds $x$ with a probability $p$ corresponding to the proximity to the upper or lower neighbor
\begin{equation*}
x_\text{stoch}(x) = 
\begin{cases}
\lfloor x \rfloor \quad \quad &\text{with } p = 1 - (x - \lfloor x \rfloor), \\
\lfloor x \rfloor + 1 &\text{with } p = 1 - \lfloor x \rfloor. \\
\end{cases}
\end{equation*}

Direct access to the analog weight parameters from the general-purpose unit is time-consuming and computation intensive due to the lack of parallelizations. With feedback alignment, the need of the current weight information becomes irrelevant to the plasticity rule. Only the computed weight update needs to be applied somehow. This can be implemented by using the vector unit. The \gls{simd} instructions allow parallel access to all weights and therefore it is not only used to efficiently apply the weight updates but also to silence and reactivated the respective synapses in the double-row structure of the synapse array. The \emph{assembly} code-base for this implementation has been co-written by Sebastian Billaudelle and Benjamin Cramer.

During training the weights can max out, due to their limited \SI{6}{\bit} range. The implementation of a simple weight regularization using the convenient parallel access of the vector unit solves this issue. The regularization of both, excitatory and inhibitory weights is implemented using bit shifts to minimize the implementation effort and given by
\begin{align*}
w_{ij}^\text{inh},\; w_{ij}^\text{exc}\equiv w_{ij} \in [0,2^6) \quad  \Rightarrow \quad \text{Reg}(w_{ij}) &= w_{ij} - \left(\left(w_{ij} \ll 1\right) \gg 6\right) \nonumber \\
&= w_{ij} - \left\lfloor \frac{\lfloor 2 \cdot w_{ij}  \rfloor}{2^{6}} \right\rfloor.					
%	&= w_{ij} - \left\lfloor \lfloor 2 w_{ij} \rfloor \cdot 2^{-6} \right\rfloor
\end{align*}
The impact of the regularization is fine tuned by applying it only with a probability $p=\nicefrac{1}{4}$.

The bias is set by lowering or raising the \gls{dac}-value of the threshold potential. The resolution of the capacitive parameter memory equals to roughly \SI{1.7}{\milli \V}, which in turn translates to a change of the input frequency of about \SI{9.9}{\kilo \Hz}. As a comparison the maximum input frequency of a neuron is around \SI{1}{\mega \Hz}, but the magnitude of the typical presynaptic activity will only be at a few \SI{100}{\kilo \Hz}. Hence, the bias has a rather coarse resolution.

However, compared to the speed of the accelerated neuromorphic core, the \acrlong{dac} is slow. Instead of an instant and large change of the bias, a continuous and slow implementation can reduce potential negative effects of the coarse bias resolution, especially in combination with a low learning rate.

\subsubsection*{Initial Conditions and Hyperparameters}

The final experiment setup involves the setting of initial conditions, neuron parameters and the tuning of hyper-parameters. The latter are a set of parameters, that is not changed by during the training process but their choice often determines whether or not the training succeeds (\citealp{Goodfellow-et-al-2016}). A subset of the most relevant parameters is listed in \cref{circlesinitparameters}.

At the beginning of the experiment all weights of both layers are randomly initialized using a uniform distribution ranging from $-25$ to $25$. 

The initial bias of both layers is set to zero, i.e. $\gls{thres} = \gls{v_leak}$. In this setting it has been observed that a ``silent" hidden unit at the start of the training most likely remains inactive throughout the training process. As a workaround, a weight dependent term is added to the zero bias, such that $b_i^\text{init} \propto \sum_j w_{ij}$. This ensures an initial firing rate of every neuron at the beginning of the training.

In the final setting, the incline of the activation function needed to be raised by a fair amount resulting in a noise frequency $\nu_\text{noise}$ of roughly \SI{1.1}{\kilo \Hz}, which corresponds only to a fraction of the available resource. The impact of the noise spikes is tuned by the synaptic weight $w_\text{noise}$ which is set to $\pm 15$ for excitatory and inhibitory spikes respectively.

The input layer, on the other hand, exploit almost the maximum with firing rates ranging from \SIrange{0}{500}{\kilo \Hz}. The maximum input rate of a hidden unit is then given by the combined input of both input units. The same applies to the output layer where the output rates of the hidden units are combined to a maximum input of $11 \times \SI{111.3}{\kilo \Hz}$.

\begin{table}\centering\ra{1.3}
	\begin{tabular}{@{}rlll@{}}\toprule
		& Parameter								& 	hidden neurons 			& 	output neurons 	\\ \midrule
		& initial weights $w_{ij}^\text{init}$	& 	 $\in[-25, 25]$			&	$\in[-25, 25]$	\\
		& initial bias $b_{i}^\text{init}$		& $\propto \frac{1}{2} \sum_j w_{ij}$ &	0		\\
		& learning rate of bias  $\eta_\text{w}$&	$165e4$					& 	$5.5e4$			\\
		& learning rate of bias  $\eta_\text{b}$&	$2.3e4$					& 	$0.23e4$		\\
		& resting potential \gls{v_leak}		&	$\SI{0.54}{\V}$ 		& 	$\SI{0.54}{\V}$	\\
		& reset potential \gls{v_reset}			&	$\SI{0.01}{\V}$			& 	$\SI{0.01}{\V}$	\\
		& refractory period \gls{refrac}		&	$\SI{9}{\micro \s}$			& 	$\SI{0.01}{\V}$	\\
		& range of threshold \gls{thres} 				&	\SIrange{0.45}{0.63}{\V}&	\SIrange{0.45}{0.63}{\V}\\
		& max input frequency $\nu_{\text{in}}$	&	$2 \times \SI{500}{\kilo \Hz}$	& $11 \times \SI{111.3}{\kilo\Hz}$	\\
		\bottomrule
	\end{tabular}
	\caption[Initial, hyper and neuron parameters per layer.]{Initial, hyper and neuron parameters per layer. Some parameters require a different setting for the hidden and the output layer.}
	\label{circlesinitparameters}
\end{table}

The learning rates may appear unusually high, which is mainly due to the lack of normalization. The normalization was not done beforehand in order to reduce the risk of eliminating small contributions of variables in the on-chip computation of the parameter updates. Furthermore the learning rates are divided into layer and parameter, to cope with the individual magnitudes. Even without the necessity to norm resulting parameter updates, it is not unusual to tune learning rates separately (\citealp{Goodfellow-et-al-2016}). The final rates are set to provide a fair amount of activity for both layers and parameters (c.f. \cref{monitoringplots}).

\section{Training}
The implementation of a full on-chip training is achieved by generating the required training dataset on the fly. In this way, each forward pass is performed with a randomly drawn data point from either of the two annuli from \cref{circlestask}. The backward pass is performed directly after the first training sample, i.e. the minibatch size is one.

\begin{figure}
	\begin{center}
		\input{figures/circles_learning_performance.pgf}
	\end{center}
	\caption[Monitoring of the training performance.]{Monitoring of the training performance. The performance is monitored by the accuracy (left) and the \acrfull{rmse} (right).}
	\label{circles_acc}
\end{figure}

In the following the network is trained for 2500 iterations. For a better illustration of the training process, a balanced \emph{validation} dataset with $100$ randomly drawn data points per class is chosen to validating the current state of the network after every fifth iteration. The limitation to every fifth step was chosen to reduce the measurement time to a few hours and to cope with the increasing instability of the \gls{fpga} during long measurements.

The same dataset is also used to monitor the performance of the training process by computing the \gls{rmse} and the accuracy. The \gls{rmse} is given by
\begin{equation}
\text{\gls{rmse}} = \sqrt{\frac{\sum_p e^{(o)}(p)^2}{n_\text{points}}},
\end{equation}
with the error $e^{(o)}$ from \cref{circleserror} and the number of data points $n_\text{points}$ in the validation dataset and the accuracy yields
\begin{equation}
\text{Accuracy} = \frac{n_\text{true}}{n_\text{points}},
\end{equation}
with $n_\text{true}$ the number of correctly classified points with respect to decision boundary from \cref{circlesdb}.

\begin{figure}
	\include{learning_process_s5}
	\caption[Initial state of the deep network.]{Initial state of the deep network. For simplicity only the hidden and output layer are displayed. The arrows connecting the hidden units with output units resemble the synaptic connections, with red being an excitatory synapse and the thickness of the arrow corresponding to the synaptic strength.}
	\label{learning_process_s5}
\end{figure}

At the beginning of the training process, each hidden unit is initialized with linear a combination of the input schemes from \cref{circlesinputs} which is then transformed into a non-linear output by the activation function. The new representations of the input layer are again linearly combined into a net input for the output unit where another non-linear transformation is applied. The initial state of the network is shown in \cref{learning_process_s5}. At this stage of the training process the output unit cannot classify the task correctly. 

%\begin{figure}
%	\label{learning_process_s500}
%	\include{learning_process_s500}
%	\caption[Network state after 500 iterations.]{Network state after 500 iterations. With a decision boundary of roughly \SI{60}{\kilo \Hz}, the output unit starts to correctly identify the two classes.}
%\end{figure}

The evolved network after 500 iterations is depicted in \cref{learning_process_s500} and shows first signs of improvement. The \gls{rmse} has already been cut in half and also the accuracy has improved as shown in \cref{circles_acc}. Less useful representation in the hidden layer are further modified by changing the bias or the weights. The beneficial ones are rewarded with a stronger connection to the output layer and further refinement of their bias and input weights.

At 2500 iterations, see \cref{learning_process_s2500}, the network has converged. The accuracy has also stabilized at almost 100 \%. The average over the last 500 iterations yields an mean accuracy of $(98.2 \pm 2.9)\,\%$ and the average over last 250 iterations improves to  $(99.6 \pm 0.8)\,\%$. The \gls{rmse} is also reduced to a minimum, i.e. the output neuron resembles the target well. The average output error over the last 250 iterations yields $(9.6 \pm 1.4)\,\si{\kilo \Hz}$.

\begin{figure}
	\begin{subfigure}{\textwidth}
		\caption{}
		\centering
		\include{learning_process_s500}
		\label{learning_process_s500}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\caption{}
		\centering
		\include{learning_process_s2500}
		\label{learning_process_s2500}
	\end{subfigure}
	\caption[Evolution of the network during training.]{Evolution of the network during training. \textbf{(\subref{learning_process_s500})} Network state after 500 iterations. With a decision boundary of roughly \SI{60}{\kilo \Hz}, the output unit starts to correctly identify the two classes. \textbf{(\subref{learning_process_s2500})} Final trained state of the network. After 2500 iterations the network has succeed to separate both classes with a accuracy of almost 100 \%.}
\end{figure}

%For further monitoring purposes, the evolution of the weights and biases are tracked for the hidden and output layer (see \cref{network_monitoring}).
%\begin{figure}
%	\centering
%    \input{figures/network_evolution_circles.pgf}
%	\caption{Describe what happens in the picture...}
%	\label{network_monitoring}
%\end{figure}