\vspace{-1cm}
\chapter{Discussion and Outlook}
\label{discussionandoutlook}
\vspace{-0.5cm}
Within this thesis, the implementation of supervised deep learning methods for \glspl{snn} were demonstrated on analog neuromorphic hardware. A rate-based approach was successfully executed as an on-chip experiment on the \gls{dls}, classifying the Circles data set with an accuracy of $(99.6 \pm 0.8) \%$. A second temporal-based method, SuperSpike, was implemented on a full-size chip version of the \gls{bss2} platform and performed well at solving an XOR-related problem on two distinct \gls{hx} setups.

\paragraph{Rate Coding with SGD} An on-chip realization of gradient descent for a rate-based \gls{snn} was developed for the \gls{dls} chip. Despite continuing stability issues of the \gls{fpga} and imperfections of the analog hardware, the Circles dataset could be classified with an accuracy of $(99.6 \pm 0.8) \%$. This experiment resembles the first successful on-chip implementation of a deep learning task requiring a hidden layer structure and a non-linear activation on the analog \gls{bss2} platform. However, transferability of the results to other setups has not yet been achieved, since an extensive fine-tuning of the new setup would have been required first.

A large improvement of the implementation could be achieved by transferring the experiment to the most recent chip version. The \gls{hx} features dedicated on-chip noise-generators with which a sigmoidal activation function can easily be realized. Additionally, the difficulty of the task can be increased: with the newly available neuron resources, 512 instead of 32, the network can be scaled up with respect to the number and size of the individual layers. With the promising prospects given by the new chip no further improvements are planned for the current implementation on the \gls{dls}. Moreover with the arrival of the \gls{hx} the research focus within the group has shifted. To that end, the software development for the \gls{dls} has been halted. The older chip is still capable of performing remarkably well, but its successor outperforms it in several regards.

%Transfer function needs to be non-linear the exact shape and its dependencies on noise rate, time constants, .. is not so important if it is somehow shaped like a non linear activation function.
%- working point of membrane -> noise and spread as soon as the working point is left (see transfer function with bias plot) -> not so bad actually for training. the error is trained as well. 
%- init conditions of task are important. somehow neurons dont learn if the init condition is "bad". but what does that even  mean.. the init condition is bad. the behaviour: the neuron stays silent throughout the learning process. 
%- ausblick: notes from mfp talk maybe?
\paragraph{Temporal Coding with SuperSpike} Unlike the previous experiment, SuperSpike was not implemented in a fully on-chip fashion. This is largely due to hardware bugs in the current chip revision which have been discovered only recently during the commissioning of the manufactured chip. The two most relevant issues are an incorrect wiring of the \gls{cadc} channels and not directly accessible spike counters from the vector unit. The mingled \gls{cadc} channels cannot be unraveled using the vector unit. A software workaround on the general purpose unit, on the other hand, will not scale and thus greatly affect the processing speed of the backward pass.

However, the main obstacle remains the lack of spike times on the \gls{ppu}. In principle there are two possibilities to record spike events: the digital back end and the on-chip spike counters. The access of the digital back end by the \gls{ppu} operates at a limited speed due to a known software bug and has not yet been fixed, making its use unfeasible. On the contrary, the spike counters cannot be directly accessed by the vector unit. As before a solution based around the general purpose unit does not scale and simply will not provide the necessary time resolution of the spike times. The aforementioned problems will be fixed in a future tape-out. With the new revision a fully on-chip implementation of SuperSpike will be possible. In the meantime a chip-in-the-loop approach is taken.

%A fixed revision of the tap-out is already being manufactured. With the new chip a fully on-chip implementation of SuperSpike will be possible. 
 %  $(97.2 \pm 9.6) \%$ 73 100
 %  $(96.5 \pm 9.9) \%$ 73 250
 %  $(95.1 \pm 11.3) \%$ 63 100
 %  $(96.1 \pm 9.9) \%$ 63 250
The host-supported implementation of SuperSpike on the \gls{hx} successfully solved an XOR-related benchmark with an average test accuracy of $100.0_{-5.2}^{+0.0}\, \%$ using backpropagation. Motivated by the results from \cite{wunderlich2019advantages} only the neuron potentials have been calibrated. The performance was reproduced on another setup with a similar test accuracy of $100.0_{-11.2}^{+0.0}\, \%$ despite significantly detuned neuron potentials. In addition, the convergence of SuperSpike using feedback alignment was also shown for both setups. It can be concluded that the SuperSpike learning rule is a robust supervised training method for deep \glspl{snn} on analog neuromorphic hardware. In particular it has been shown that the algorithm does not require either calibrated time constants nor perfectly aligned neuron potentials for the given task.
%The overall measurement period can be reduced to the length of several spike duration equivalents for temporal coded \glspl{snn}, making SuperSpike a candidate to process streaming spike data \emph{online}.
\vspace{-0.2cm}
\paragraph{Future Projects} In the meantime, the developed framework for the hardware implementation of SuperSpike has been embedded in a PyTorch environment by Sebastian Billaudelle and Benjamin Cramer. Early results have shown a competitive test accuracy of over \SI{96}{\%} for the MNIST dataset. A more profound investigation of these preliminary results will be published in the next months.

Furthermore, the collaboration with Friedemann Zenke from the Friedrich Miescher Institute for Biomedical Research in Basel, author of the SuperSpike learning rule, will be continued. In a follow-up project the application of SuperSpike for real world problems such as speech recognition will be investigated by training recurrent \glspl{snn}. In an attempt to bridge the gap between biological time constants of a neuron (milliseconds) and the time constants of sensory input data (seconds), the slow neuronal adaption variable provided by the \gls{adex} neuron model will be used.
\vspace{-0.9cm}