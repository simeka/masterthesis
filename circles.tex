\section{Circles on \gls{bss1}}
As a first experiment a toy data set called circles has been chosen. The data set represents a non-linear exercise and thus requires at network structure with at least one hidden layer to be successfully solved. The detailed \textit{on-chip} implementation of circles on \gls{dls} with a single hidden layer network using feedback alignment is presented in the next paragraphs.

%- single hidden layer network with 11 neurons (2 synapse lines necessary for each neuron -> 22)
%- 2 noise sources (exc/inh)
%- 2x2 input sources (exc/inh)
%(A single hidden layer network is trained using rate coding and feedback alignment.) maybe restructure the intro somehow...



\subsection{Rate Coding with Poisson Spike Trains}

In rate coding, information is encoded in the fire rate $\nu$ of a neuron, which is determined by the number of spikes $n_\text{spikes}$ sent within a period $T$. The distribution of the spikes in the postsynaptic spike train is based on Poisson processes (ref here). One way to numerically generate such a Poisson spike train representing a fire rate is to repeatedly perform a Bernoulli process on a short time interval $\Delta t$, i.e. an unfair coin flip with probability $p = \nicefrac{n_\text{spikes} \cdot \Delta t}{T}$.

On hardware the Bernoulli process is implemented by comparing a certain probability with a randomly drawn number. A popular method to generate random numbers on a system with limited memory and computation resources is the Xorshift (\cite{marsaglia2003xorshift}). The random number is generated by repeatedly applying the XOR operator on a given seed and various bit-shifted versions of itself. A bit-shift to the left by the factor of five $\text{seed} << 5$ equals to $\text{seed} \cdot 2^5$. The xorshift can be implemented in C as follows.

\begin{minted}{C}
uint32_t xorshift32(uint32_t* seed)
{
*seed ^= *seed << 13;
*seed ^= *seed >> 17;
*seed ^= *seed << 5;
return *seed;
}
\end{minted}

In theory the maximum rate is given by $\nu_\text{max, th} = \nicefrac{1}{\gls{refrac}}$, i.e. when the coin's probability equals one. With respect to hardware limitations, for example the time required to generate an artificial spike with the \gls{ppu}, $\nu_\text{max, hw} = \nicefrac{1}{\SI{0.44}{\mu \s}} = \SI{2.27}{\mega \Hz}$.

In the final experiment setup the Poisson spike generator will be divided into four independent channels, each producing at max a fourth of $\nu_\text{max, hw}$.

However, another factor limits the rate as well - the 8-bit spike counters. With $T$ in the order of milliseconds and $\nu_\text{max} = \nicefrac{256}{T}$, a measurable rate is found around $\mathcal{O}(\SI{100}{\kilo \Hz})$.

\subsection{Circles Task}
A region bounded by two concentric circles with radius $r_{\text{inner}}$ and $r_{\text{outer}}$ is called annulus. The circles data set describes in principle a set of points $p = p(x,y)$ in a two-dimensional plane, which are in one of two disjunct annuli, each representing a class. The goal is to find a decision boundary that successfully separates the two classes.

\begin{align}
\text{class}(p) =
\begin{cases}
0 ,&\quad \quad r_{\text{inner}}^2 < x^2 + y^2 < r_{\text{outer}}^2 \\
1 ,&\quad \quad R_{\text{inner}}^2 < x^2 + y^2 < R_{\text{outer}}^2
\end{cases}
\end{align}

With the 8-bit resolution of the spike counters the range of the input is determind by a signed 8-bit integer ($x, y \in [-128,127]$). Both inputs are mapped to a separate channel of the Poisson spike train generator of rate $\nu_\text{max}$, e.g. for $x$

\begin{equation}\label{inputfrequency}
\nu_{\text{input, x}}(x) = \nu_\text{max} \cdot \frac{x + 128}{255}.
\end{equation}


Todo: somewhere put this $\gls{isyn} \propto w*\nu_{\text{input}}$, maybe just a short reference... \gls{isyn} is should have been explained already

\subsection{Transfer Function}

The sigmoidal transfer function \gls{transfer} has already been motivated in \cref{trainingANN}. With a few adjustments it can be implemented for a rate coding approach of \glspl{snn}.

The output of the neuron can be approximated by (c.f. \cite{brunel2000dynamics})
\begin{equation}\label{fireratehigh}
\frac{1}{\nu_\text{output}} \approx \gls{refrac} + \gls{tau_m} \frac{\gls{thres} - \gls{v_reset}}{\gls{isyn}}
\end{equation}

for high input rates. The transfer function saturates for high input currents and is only limited by the inverse of \gls{refrac}. The approximation holds only if the time constants \gls{tau_m} and \gls{tau_syn} are small enough, in particular if they are smaller than \gls{refrac}, which means that the membrane actually forgets its previous state after spiking. The limit $\gls{thres} - \gls{isyn} \gg \sigma$, i.e. for low rates, yields
\begin{equation}\label{fireratelow}
\nu_\text{output} \gls{tau_m} \approx \frac{(\gls{thres} - \gls{isyn})}{\sigma \sqrt{\pi}} \exp\left(-\frac{(\gls{thres} - \gls{isyn})^2}{\sigma^2}\right).
\end{equation}

The fluctuations $\sigma$ of a single excitatory input source are small, reflecting only the intrinsic noise of the hardware and thus it will increase the fire rate rapidly. A continuous inhibitory and excitatory noise source (i.e. Poisson spike trains with a fixed rate) increases $\sigma$ and thus flattens the slope of the \gls{transfer} for a low $\nu_\text{input}$. 

At rest and without additional input apart from the noise, the threshold is set such that $\nu_\text{output} = \nicefrac{\nu_\text{max}}{2}$. Varying the strength of the synaptic input directly changes the shape of \gls{transfer} as well (c.f. \cref{transferfunction}).

\begin{figure}
	\label{transferfunction}
	\begin{center}
		\input{figures/single_calibrated_transfer_function_w_various_weights.pgf}
	\end{center}
	\caption{The shape of \gls{transfer} depends on the strength of the synpatic input: $\gls{isyn} \propto w \cdot \nu_\text{input}$. The external  with external noise sources has a standard deviation of about \SI{20}{\milli\V}. Without noise only spread would be a magnitude lower. The part of the distribution that exceeds the threshold potential would lead to a spike.}
\end{figure}

The continuous stimulation with Poisson spike trains leads to a Gaussian free membrane potential distribution centered around the resting potential (c.f. \cite{mihaiphd}). In a naive approach, the area of the distribution that exceeds a certain threshold correlates to number of spikes fired. Thereby the dynamics of the membrane, once are a spike is fired, are not considered. Even for very short time constants close to zero the exact behavior is more complicated than this strongly simplified picture. However, it still offers a correct intuition what the individual parameters of the Gaussian free membrane potential distribution imply for the transfer function: more noise leads to a wider distribution and thus a more gently incline of \gls{transfer}; inhibitory synaptic input moves the distribution to a lower mean value and v.v.; moving the threshold corresponds to an additional bias term.

\begin{figure}
	\label{vleak_w_noise}
	\begin{center}
		\input{figures/activation_function_vmem_distr_with_thres.pgf}
	\end{center}
	\caption{The distribution of the membrane potential set to $V_{\text{leak}}$ with external noise sources has a standard deviation of about \SI{20}{\milli\V}. Without noise only spread would be a magnitude lower. The part of the distribution that exceeds the threshold potential would lead to a spike.}
\end{figure}

In \cite{petrovici2016stochastic} the bias term of a \gls{lif} neuron's activation is mapped to a shift of the resting potential. The analog core brings more and less favorable operating points. Therefore the resting potential is kept constant and the threshold shifted instead (c.f. \cref{transferfunction_with_bias}).

\begin{figure}
	\label{transferfunction_with_bias}
	\begin{center}
		\input{figures/activation_function_w_bias.pgf}
	\end{center}
	\caption{Shifting \gls{thres} by $\delta V$ is interchangeable with adding a bias term to the activation and thus $b \propto \delta V$.}
\end{figure}

\subsection{Calibration on \gls{dls}}
In an uncalibrated state the transfer function (c.f. \cref{transferfunction_wout_calib}) is quite well spread per neuron with regards to the maximum rate (set by \gls{refrac}) and the center rate (set by \gls{v_leak}). Up to a certain degree the training process of \gls{snn} is self-correcting the misalignment. In a perfect world with no noise it is even essential for \glspl{ann} to inject artificial noise in order to perform and thus one could argue that the uncalibrated state is actually beneficial to the training. However, the dynamic range of the individual neurons is limited and needs to overlap (c.f. \cref{transferfunction_w_calib}}).

\begin{figure}
	\begin{subfigure}[b]{0.47\textwidth}
		\begin{center}
			\input{figures/uncalibrated_activation_function.pgf}
		\end{center}
		%\caption{Picture 1}
		\label{transferfunction_wout_calib}
	\end{subfigure}
	%
	\begin{subfigure}[b]{0.47\textwidth}		
		\begin{center}
			\input{figures/calibrated_activation_function.pgf}
		\end{center}
		%\caption{Picture 2}
		\label{transferfunction_w_calib}
	\end{subfigure}
	\caption{\cref{transferfunction_wout_calib}: In the uncalibrated state the maximum (\gls{refrac}) and center rate (\gls{v_leak}) deviates due to analog imperfection of the hardware. \cref{transferfunction_w_calib}: The calibration aligns the dynamic range of individual neurons, which will helps the training}
\end{figure}

The analog neuron parameters are controlled by setting bias currents over a 10-bit \gls{dac}. The on-chip calibration uses a binary serach algorithm (\cite{binarysearchsource}). 

%	for(uint32_t i=0; i<n_bins; ++i) {
%xorshift32(&random_state);
%
%if(i%4 == 0) {
%my_spike.row_mask = (1 << 4) | (1 << 5);
%probability = input_a_probability;
%my_spike.addr = 10;
%} else if (i%4 == 1) {
%// potential excitatory background spike
%my_spike.row_mask = (1 << 0);
%probability = exc_noise_probability;
%my_spike.addr = 1 + random_state & 0b111; // 1,2,..,8
%} else if (i%4 == 2) {
%// potential inhibitory background spike
%my_spike.row_mask = (1 << 1);
%probability = inh_noise_probability;
%my_spike.addr = 1 + random_state & 0b111; // 1,2,..,8
%} else {
%my_spike.row_mask = (1 << 6) | (1 << 7);
%probability = input_b_probability;
%my_spike.addr = 10;
%}
%
%if(((random_state & 0xff00) >> 8) < probability) {
%send_spike(&my_spike);
%} else {


%The goal is to find a decision boundary that separates both annuli. 

%repeated coin flipping (possible with an unfair coin) poisson process is continuous time 

\subsection{Implementation on \gls{dls}}


arbitrary order:

- Calibration of Vleak and Bias (in particular the transfer function -> plot exists)

- Cite yannik for other calib?

- On Chip implementation of plasiticty rule

- On chip implementation of spike generator

- Stochastic rounding of weights

- Regularizer of weights

- adaption of circles task 

- init conditions of task

%In a slight adaption $r_{\text{inner}} = 0$ and $R_{\text{outer}}$ is replaced by the maximum range of $x$ and $y$ of e outer radius only the 2

\subsection{Results}

- training process (in picutres?)

- validation

- result of training

\begin{figure}
	\label{circles_acc}
	\begin{center}
		\input{figures/circles_learning_performance.pgf}
	\end{center}
	\caption{The learning performance of stochastic gradient descent on the circles task, i.e. batch size is equal to one, is monitored by two measures: the accuracy and the root mean square error (RMSE).}
\end{figure}
\subsection{Discussion}

- transfer function needs to be non-linear the exact shape and its dependencies on noise rate, time constants, .. is not so important if it is somehow shaped like a non linear activation function.

- working point of membrane -> noise and spread as soon as the working point is left (see transfer function with bias plot) -> not so bad actually for training. the error is trained as well. 

- init conditions of task are important. somehow neurons dont learn if the init condition is "bad". but what does that even  mean.. the init condition is bad. the behaviour: the neuron stays silent throughout the learning process. 

- ausblick: notes from mfp talk maybe?