\chapter{SuperSpike on \acrshort{bss2}}
\label{superspike}
The second experiment in this thesis is done on the most recent chip version of the \gls{bss2} platform, the \gls{hx}. In contrast to the first experiment the neural coding scheme is changed from rate based to temporal coding. The prerequisite for learning on \gls{snn} with temporal coding is to find an appropriate plasticity rule, that copes well with the non-differential nature of individual spikes. A possible candidate, SuperSpike, was presented in \cref{temporalcoding}. The latest version of the \gls{bss2} platform has now access to the temporal evolution of the membrane potential. This is a key element for a successful implementation of the surrogate gradient method.

% Ã¼berleitungssatz to the task
To benchmark the implementation a constructed task is taken from the original publication by \cite{zenke2018superspike} which is equivalent to solving the XOR problem on a time scale.

\section{Task}
A total of 96 input units, each firing once at a fixed random spike time, is split into four overlapping collections of different size. In \cref{superspiketaskoverview}, the various spike trains are visualized. As in the exclusive-or, the four different input patterns are assigned to two target classes which are represented by two distinct neurons in the output layer. Despite the multiple input sources, the task is by construction only two-dimensional. The first input pattern $p_1$ overlaps with all other patterns. The units involved in $p_1$ can therefore be interpreted as bias or reference units, since they will fire regardless of the active pattern. The second and third pattern do not overlap except for their bias units. In analogy to the XOR operator these patterns compare to $1 \veebar 0 = 0 \veebar 1 = 1$. Their combination then yields the fourth pattern resembling $1 \veebar 1 = 0$. In this way, $p_1$ and $p_4$ are associated with class $0$ whereas the remaining two input spike trains correspond to class $1$. 

\begin{figure}	
	\begin{subfigure}{0.5\textwidth}
		\caption{}
		\input{figures/superspiketasksector.pgf}
		\label{superspiketaskpicturesector}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\caption{}
		\input{figures/superspiketask.pgf}
		\label{superspiketaskpicture}
	\end{subfigure}
	\caption[XOR-related task for SuperSpike.]{XOR-related task for SuperSpike. \textbf{(\subref{superspiketaskpicturesector})} A representative sector of the input data from the XOR-related task shows how the input patterns overlap. The first pattern (\textit{black}) contains two spikes and overlaps with all other patterns. The second (\textit{red}) and third pattern (\textit{blue}) are disjoint apart from the bias units with four spikes each. Their combination yields the fourth input pattern (\textit{green}) with a total of six spikes. (\subref{superspiketaskpicture}) The total number of spikes in an input pattern is then multiplied by ten yielding 20, 40 or 60 input spikes respectively.
	\label{superspiketaskoverview}}
\end{figure} 

A feedforward pass of a single pattern spans roughly $\SI{250}{\micro \s}$ with the random input spike times being drawn from a window of $\SI{40}{\micro \s}$. In a slight modification to the derivation of SuperSpike, the error metric is changed from a single target spike train $S_i^*$ for the output $i$ to a target time window $[t^*_0, t^*_1]$. Depending on the input pattern $p_j$, the error of the output unit $i$ corresponding to the target class $i$ is given by
\begin{equation}
e_i(t) = \begin{cases}
\left(\alpha \ast \big(e_\text{outside, i} + e_\text{inside, i}\big)\right)(t),& \quad \quad \text{if} \quad \text{class}(p_j) == i, \\
- (\alpha \ast S_i)(t)	,& \quad \quad \text{else}. 
\end{cases}
\label{superspikeerror}
\end{equation}
In analogy to the van Rossum distance the error outside and inside the window can be written as 
\begin{align*}
e_\text{outside}(t) &= - S_i(t) \cdot \left(H(t^*_0 - t) + H(t - t^*_1)\right), \\
e_\text{inside}(t) &= 
\begin{cases}
0 ,&\quad \quad \text{if} \quad \exists \; t^{(s)}_i \in [t^*_0, t^*_1], \\
(\epsilon \ast t^*)(t) ,& \quad \quad \text{else}.
\end{cases}
\end{align*}
with the Heaviside step function $H$ and $H(0) = 1$ as well as the double-exponential kernels $\alpha$ and $\epsilon$ from \cref{temporalcoding}.
Despite extending the target spike to a target time window, the error still follows the same principles as the von Rossum distance: If the target neuron spikes within the designated time span, a constant error of zero is returned. In case there occurs no spike inside the window, a target spike at time $t^*$ will be added to the error instead. Any spikes outside the window sum with a negative sign. Given there is spiking activity in the inactive output unit, the negative convolution of the activity with the $\alpha$ kernel is returned as a penalty.

\section{Implementation on the \acrshort{bss2} Platform}
%Unlike the previous experiment, SuperSpike is not implemented in a fully on-chip fashion. During the commissioning of the manufactured \gls{hx} some hardware bugs were discovered. With regards to the use of software workarounds. To minimize the implementation effort the use of the chip is restricted to the upper half and only one \gls{ppu}. Unlike the previous experiment, SuperSpike is not implemented in an on-chip fashion. However, a key element of SuperSpike, the access to the membrane potential via the \gls{cadc} is implemented as an on-chip readout routine.

Unlike the previous experiment, SuperSpike is not implemented in a fully on-chip fashion. This is largely due to hardware bugs which have been discovered during the commissioning of the new \gls{hx}. The two most relevant issues are a reordering of the \gls{cadc} channels and inaccessible spike counters from the vector unit. With the symmetric design of the chip the \gls{cadc} is split into four respective quadrants, but the channels of the first and second as well as third and fourth quadrant are mingled with each other. In a scripting programming language such as Python this can be unraveled quite effortless. On the general purpose unit the programming capabilities are rather limited making it hard to develop a functioning experiment environment under these circumstances. More importantly, the software workarounds cannot be implemented on the vector unit and therefore the computation will not scale.

\begin{figure}[htb!]
	\centering
	\includegraphics[width=\textwidth]{figures/hxsetup_img2.jpg}
	\caption[Cube setup with \gls{hx}.]{Cube setup with \gls{hx} in the laboratory.}
	\label{cubesetupinlab}
\end{figure}

However, the main obstacle remains the lack of on-chip knowledge over spike times. In principle there are two available sources to record a spike event: the digital back end which registers every spike event precisely and the on-chip spike counters. The data transfer between the digital back end and the \gls{ppu} operates at a limited speed due to a known software bug that has not yet been fixed, making its use unfeasible. The spike counters on the other hand, cannot be accessed by the vector unit since they are not properly connected. With these constraints, an efficient implementation on \gls{hx} is beyond reach. Despite the handicaps, SuperSpike can still be set up as a chip-in-the-loop experiment, i.e. the weight updates are computed on the host and the forward pass is evaluated on-chip. 


In the following, the details of the experiment implementation are presented. The developed software framework is based on the new software stack for the \gls{bss2} platform, which is still under continuous development. A detailed description of the software stack's architecture is provided by \cite{mueller2020bss2ll}. Furthermore, the use of the chip is restricted to the upper half and only one \gls{ppu}, to reduce the implementation effort.

\paragraph{\gls{cadc} Readout}
A key element for the SuperSpike algorithm is the access to the membrane potential by the \gls{cadc}. The general purpose unit instructs the vector unit to trigger the \gls{cadc} and to transfer the resulting conversions back into the main memory of the \gls{ppu}. At that time the software stack did not fully support the use of the vector unit. A preliminary code base has therefore been developed for the \gls{cadc} readout by Aron Leibfried. 

For the purpose of the experiment, the membrane traces of 128 neurons are recorded at a high sampling rate of roughly \SI{400}{\kilo \Hz} to ensure a suitable time resolution of the accelerated dynamics. At the same time, the available memory on the \gls{ppu} restricts the maximum number of samples. Each sample uses one byte of memory and thus 100 samples of 128 neurons occupy already \SI{12.8}{\kilo \byte} of the available \SI{16}{\kilo \byte}, leaving enough space for the \gls{ppu} instructions which share the same memory. In the final configuration of the \gls{cadc} 100 samples are recorded per neuron with a temporal resolution of roughly \SI{2.5}{\micro \s}.

\begin{figure}[htb!]
	\begin{subfigure}{0.65\textwidth}
		\caption{}
		\vspace{-0.4cm}
		\input{figures/pre_cadc_calib.pgf}
		\label{precadccalib}
		\vspace{0.4cm}
	\end{subfigure}
	\begin{subfigure}{0.35\textwidth}
		\caption{}
		\input{figures/post_cadc_calib.pgf}
		\label{postcadccalib}
	\end{subfigure}
	\caption[Pre and post calibration state of the \gls{cadc}.]{Pre and post calibration state of the \gls{cadc}. \textbf{(\subref{precadccalib})} The pre calibration \gls{cadc} measurement of a reference voltage ranging from \SIrange{0.2}{1.0}{\V} are not aligned. The slope and offset parameter differ per quadrant. Both quadrants from the upper chip half are shown. \textbf{(\subref{postcadccalib})} Both quadrants are calibrated such that they share the same characteristic. The thereby implicated conversion from \gls{dac} LSB to \si{\V} will be used throughout this chapter.}
	\label{cadccalibration}
\end{figure}
Before the \gls{cadc} can be employed, its characteristic needs to be calibrated. This is done by setting various reference voltages and recording it by the \gls{cadc}. Since at the minimum voltage (\SI{0.0}{\V}) and maximum voltage (\SI{1.2}{\V}) the \gls{cadc} shows a non-linear behavior, the sweep range of the reference voltage is limited from \SIrange{0.2}{1.0}{\V}. Per quadrant, a slope and offset parameter of the characteristic is manually adjusted such that the full range of the LSBs is used. In addition to the per-quadrant settings, each channel has an individual offset parameter, that can be adjusted as well. Unlike the offset correction per channel, the ramp and slope parameters are not trivial to tune, since they show a sensitive cross-dependency in certain areas. A manual method was thereby preferred over an automated fit-routine. In \cref{cadccalibration} the pre and post calibration state of the characteristics are shown for both quadrants from the upper chip half. The thereby implicated conversion from \gls{dac} LSB to \si{\V} will be used for most of the data shown throughout this chapter.

\paragraph{Calibration of \acrshort{lif} Neurons}
%The manufacturing process of analog neuromorphic hardware causes systematic stochastic deviations in the neuron and synapse parameters. The thereby induced heterogeneity between neurons and synapses is referred to as \emph{fixed-pattern noise} and is constant in time. Despite the detuned parameters, it has been shown that plasticity rules can correct the intrinsic imperfections of the analog hardware to a certain degree (\citealp{wunderlich2019advantages}). 
In chapter \cref{circles} it has already been mentioned that analog neuromorphic hardware exhibits fixed-pattern noise, which in turn can be corrected up to a certain degree by the plasticity rule in place (\citealp{wunderlich2019advantages}). It is therefore a constant trade-off of between resources invested into the calibration and a potential degrade of the learning performance by less tuned analog setups.

At the time of the experiment, the development of chip-specific software for the new prototype was in an early stage. Among others, there was a lack of a calibration database and more generally, a lack of available calibrated hardware resources. This was largely due to an at the time impractical and slow calibration routine, that is based around the \gls{madc}. The \gls{madc} offers a precise measurement of an observable but does not provide any parallel readout.

The efficient parallelized readout of the \gls{cadc}, on the other hand, presented itself to be a viable basis to put a more practical calibration routine into action. The main objective of the new routine is to provide a fast calibration that requires little to no interaction in order to bring a setup into a usable state with respect to the specific experiment requirements. However, the convenience of the \gls{cadc} comes at the cost of the precision.

As a trade-off, only a subset of the available neuron parameters is tuned, namely the potentials of the \gls{lif} model \gls{v_leak}, \gls{v_reset} and \gls{thres}. By design, the potentials of the synaptic input $V_\text{syn, inh}$ and $V_\text{syn, exc}$ have an influence on the resting potential and thus need to be considered as well. The time constants \gls{tau_m} and \gls{tau_syn} are certainly important parameters in the \gls{lif} model. Motivated by the results from \cite{wunderlich2019advantages} the induced error by the misalignment of the time constants is knowingly accepted and traded for less implementation effort.

\begin{figure}[htb!]
	\begin{subfigure}{0.32\textwidth}
		\caption{}
		\centering
		\input{figures/vleak_pre_post_calibration.pgf}
		\label{hxprepostvleak}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\caption{}
		\centering
		\input{figures/vreset_pre_post_calibration.pgf}
		\label{hxprepostvreset}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\caption{}
		\centering
		\input{figures/vthreshold_pre_post_calibration.pgf}
		\label{hxprepostvthreshold}
	\end{subfigure}
	\caption[Pre and post calibration state of the analog \gls{lif} parameters.]{Pre and post calibration state of the analog \gls{lif} parameters. \textbf{(\subref{hxprepostvleak})} Before calibration to \SI{0.5}{\V}, the resting potential shows a spread over almost \SI{0.3}{\V} (\textit{gray}). \textbf{(\subref{hxprepostvreset})} The reset potential \gls{v_reset} is calibrated to \SI{0.4}{\V} (\textit{red}). \textbf{(\subref{hxprepostvthreshold})} The spread of the uncalibrated thresholds \gls{thres} is smaller than for the leak potential (\textit{gray} and \textit{blue}). The potential is calibrated to \SI{0.75}{\V} (\textit{red}) and \SI{0.8}{\V} (\textit{green}).}
	\label{hxprepostcalib}
\end{figure}
Similar to the previous task, a binary search algorithm is adopted to find the proper \gls{dac} values of the analog parameters. The cross talk between individual capacitive parameter memory (capmem) cells, that arises if several cells are set to the same value, requires a workaround. By the design of the binary search algorithm, this event occurs repeatedly making the binary search a suboptimal choice for the calibration of the chip. The development of a proper gradient based calibration has already been started, but for the use case in this experiment the existing code base could be sufficiently stabilized with two minor adaptations. First, a random variation is consequently applied to all ``unused" \gls{dac}-channels and second, the binary search algorithm is extended with a fall back option to the best parameter setting so far. The results of the final calibration for the \gls{lif} parameters are shown in \cref{hxprepostcalib}.


\subsection{Experimental Setup}
The \gls{hx} is mainly controlled by a Python class called \texttt{HXBlackbox} which implements a set of convenient tools that put the chip into operation. The code has been co-written by Sebastian Billaudelle and Benjamin Cramer and is based on the new software stack for the \gls{bss2} platform (\citealp{mueller2020bss2ll}). 

In a preliminary step, a Python-based simulation frame has been developed which is oriented on the original implementation of SuperSpike by \cite{zenke2018superspike}. The chip-in-the-loop experiment is embedded in a new class \texttt{HXSuperSpike} combining the SuperSpike simulation with an adapted version of \texttt{HXBlackbox}. The forward pass of the simulation is then replaced by measuring the analog traces and spike times on the \gls{hx} before the respective updates of the network parameters are computed in the backward pass on the host. In the following a network with a single hidden layer is set up containing 96 input sources, 30 hidden units and two output neurons which serve as target classes.

%The execution of the experiment is embedded in an Python-based simulation frame that is oriented on the implementation of SuperSpike in \citealp{zenke2018superspike}. The forward pass is then replaced by measuring the analog traces and spike times on the \gls{hx} before the respective updates of the network parameters are computed in the backward pass on the host. In the following a network with a single hidden layer containing 30 units, 96 input sources and two output neurons which serve as target classes is configured using the \texttt{HXBlackbox}.

\paragraph{Chip-based Forward Pass}
The implementation of the experiment on the basis of the \texttt{HXSuperSpike} class is straight forward. The setup of the multilayer network structure is implemented by the event router and the synapse array using corresponding weights and addresses to discriminate between input and recurrent connections. The issue that arises with alternating weights for a single synapse is solved by the same double-row approach from the previous experiment implementation (\cref{circlesimplementation}). However, in this case the workaround is already implemented within the \texttt{HXBlackbox} and the user only needs to fill a logical weight matrix $W_\text{logical}$ accordingly.

\begin{equation*}
W_\text{logical} = 
\begin{tikzpicture}[baseline=0cm,
node distance = 0mm,
every matrix/.style = {matrix of math nodes,
	nodes={minimum width=2em},
	left  delimiter={(},
	right delimiter={)},
},
B/.style = {decorate,
	decoration={brace, amplitude=2pt,
		pre=moveto,pre length=4pt,post=moveto,post length=4pt,
		raise=0mm,
	},
},  
]
\matrix (m)
{
	\Bigg(I \rightarrow H\Bigg) &   0   &   0   \\
	0   						&   \Big( H \rightarrow O\Big)   &   0   \\
	0   &   0   &   0   \\
};

\draw[B]    (m-1-1.north east) -- node[right=0mm] {$n_\text{h}$} (m-1-1.south east);
\draw[B]    (m-1-1.north west) -- node[above=0mm] {$n_\text{in}$} (m-1-1.north east);
\draw[B]    (m-2-2.north west) -- node[above=0mm] {$n_\text{h}$} (m-2-2.north east);
\draw[B]    (m-2-2.south west) -- node[left=0mm] {$n_\text{out}$} (m-2-2.north west);
%\draw[B] (m-1-1.south west) -- node[below=2mm] {label3} (m-2-1.south east);
\end{tikzpicture}=\begin{tikzpicture}[baseline=0cm,
node distance = 0mm,
inner sep=1pt,
row sep=0.5em,
every matrix/.style = {matrix of math nodes,
	nodes={minimum width=0em},
	left  delimiter={(},
	right delimiter={)},
},]
\matrix (m)
{
	W^{(h)} &   0\;   &   \;0   \\
	0\;   	&   W^{(o)}&   \;0   \\
	0\; 	&   0\;   &   \;0   \\
};
\end{tikzpicture}%\label{hxlogicalweights}
\end{equation*}

The $n_\text{in}$ input rows are connected to the $n_\text{h}$ units of the hidden layer ($I \rightarrow H$) by the weight matrix $W^{(h)}$. The feedforward connections of the hidden layer are mapped to $n_\text{out}$ units of the output layer ($H \rightarrow O$) by the matrix $W^{(o)}$.

The input pattern is formulated as an array containing tuples of target neuron and spike time. The forward pass is then evaluated by a designated \texttt{stimulate} function, which returns all recorded spikes at the end of the measurement. The membrane potential traces are measured by triggering the on-chip \gls{cadc} readout program on the \gls{ppu} as soon as the input pattern is injected by the host. 

\paragraph{Host-based Backward Pass}
Within \texttt{HXSuperSpike} the analog membrane traces and recorded spike events are gathered and processed to determine the updates of the weight matrices as derived in \cref{superspike}. In a practical approach, the integration of \cref{superspikeweightupdateeq} is done in finite temporal intervals which correspond to the length of a full \gls{cadc} readout period $T$ of roughly \SI{252}{\micro \s}. Moreover, the finite resolution of the \gls{cadc} turns the integral into a sum over the time steps $dt'  \approx \delta t = \SI{2.5}{\micro \s}$.
\begin{align}
\Delta w^k_{ij} &= \eta \int_{t_k}^{t_{k+1}} dt'
e^{(o)}_k \; \; \alpha \ast 
\left( \sigma'(V^{(o)}_{\text{m},i}) \left(\epsilon \ast S^{(o)}_j\right) \right) \nonumber \\
&\approx \eta \sum_{t'=0}^{T} \delta t e^{(o)}_k(t') \; \; \alpha \ast 
\left( \sigma'\big(V^{(o)}_{\text{m},i}\big) \left(\epsilon \ast S^{(o)}_j\right)\right)(t')
\label{superspikeweightupdatedivided}
\end{align}

\begin{wrapfigure}{R}{0.45\textwidth}
	\centering
	\vspace{-.75cm}
	\input{figures/cadcppuoffset.pgf}
	\vspace{-.15cm}
	\caption[Offset measurement between \acrshort{cadc} traces and digital spike times.]{Offset measurement between \acrshort{cadc} traces and digital spike times. The time delta between peaks in the \acrshort{cadc} traces and the recorded spike times by the digital back end needs to be corrected.} 
	\label{cadcppuoffset}
	\vspace{-.75cm}
\end{wrapfigure}

Before the updates can be computed, it is vital that both the measured \acrshort{cadc} traces and the recorded spike times by the digital back end are synchronized. In a respective offset measurement, the neurons are stimulated with a continuous stream of input spikes such that they fire repetitively but with a low frequency. The located peaks in the resulting membrane traces are then compared to the digitally recorded spike times, yielding an offset $\Delta T$ of $(2.7 \pm 1.1)\; \si{\micro\s}$, see \cref{cadcppuoffset}. In the \gls{cadc} traces, the membrane potential peaks always before or exactly at the actual spike time, as the reset mechanism is triggered as soon as the threshold potential is crossed. This causes a systematic error of the evaluated peak time. A linear increase of the membrane potential would add a systematic error of half a bin width, but since the membrane potential follows rather the shape of a double exponential the systematic error is estimated to be only a quarter of a bin width. The offset is thereby corrected to $\Delta T_\text{corr} = (2.1 \pm 1.1)\; \si{\micro\s}$.

In an attempt to optimize the performance of the Python-based simulation environment, Python bindings are used to speed up time consuming or repetitive code in C++. In particular, the stored bits of the analog \gls{cadc} traces are inverted due to a hardware bug and need to be reverted before further use. Another task where C++ improves the performance is the construction of input spike trains during the forward pass. In the final setup, the overall runtime has been improved by about 25 percent. 

\paragraph{Initial Conditions and Neuron Parameters}

The chosen initial conditions and neuron parameters for this experiment are inspired by the configuration used in \cite{zenke2018superspike}. However, some adaptations and a respective time-scaling have to be made, to cope with the analog hardware's acceleration factor of $10^3$. 

\begin{table}[h!]\centering\ra{1.3}
	\begin{tabular}{@{}rlll@{}}\toprule
		& Parameter		& 	\gls{hx} & 	SuperSpike\tablefootnote{Simulation parameters for SuperSpike can be found in \cite{zenke2018superspike}} \\ \midrule
		& membrane constant \gls{tau_m}		& 	$\SI{8}{\micro \s}$ & 	$\SI{10}{\milli \s}$\\
		& synaptic constant \gls{tau_syn}	&	$\SI{5}{\micro \s}$ & 	$\SI{5}{\milli \s}$\\
		& refractory period \gls{refrac}	&	$\SI{30}{\micro \s}$ & 	$\SI{5}{\milli \s}$\\
		& $\alpha$ kernel constant $\tau_\alpha$	&	$\SI{12}{\micro \s}$& 	$\SI{10}{\milli \s}$\\
		& $\epsilon$ kernel constant $\tau_\epsilon$ 	&	$\SI{12}{\micro \s}$& 	$\SI{10}{\milli \s}$\\
		\bottomrule
	\end{tabular}
	\caption[Time constants used for SuperSpike.]{Time constants used for SuperSpike. Shown is a comparison between the tuned hardware parameters used in this thesis and the original simulation.}
	\label{temporalconstants}
\end{table}
A \gls{lif} neuron is fully described by a set of time constants and neuron potentials. Their specific values have a significant impact on the training performance. If for instance the time constants are chosen to short, the individual parts of the learning rule in \cref{superspikeweightupdatedivided} do not overlap anymore and will yield a vanishing weight update, despite an incorrect spiking behavior of the network. On the other hand, longer time constants will blur the network's temporal perception. The time constants used in the backward pass are shown in the \cref{temporalconstants} and resemble an average estimate of the detuned hardware time constants of the recorded membrane traces. 

In the given task, the neurons are not required to fire multiple times. The refractory period has therefore been increased to \SI{30}{\micro \s} to suppresses repetitive fire patterns. Another way of avoiding repetitive fire patterns is to set the reset potential far below the equilibrium state.

The distance between the resting potential and the threshold defines the dynamic range of a non-spiking membrane. The choice of the distance follows two main motivations. A larger gap increases the dynamic range and therefore improves the \gls{snr} of the \gls{cadc} traces. At the same time, a greater potential difference between threshold and resting potential requires a stronger synaptic input current to trigger a fire response. A single input spike with a maximum excitatory weight and time constants set as in \cref{temporalconstants} increases the membrane potential by approximately \SI{0.3}{\V}. The final setting is a compromise of both assuring that the \SI{6}{\bit} weights are not maxing out during training and setting the potentials as far apart as possible. Since the overall input activity of the output layer is reduced compared to the hidden layer, the threshold of the output unit is further lowered by \SI{0.5}{\V}. The choice of the neuron potentials for the hidden and output layer is listed in \cref{initparameters}.

\begin{table}[htb!]\centering\ra{1.3}
	\begin{tabular}{@{}rlll@{}}\toprule
		& Parameter								& 	hidden layer 			& 	output layer \\ \midrule
		& initial weights $w_{ij}^\text{init}$	& 	$\in \mathcal{N}(0,13)$ & 	$\in \mathcal{N}(0,18)$\\
		& resting potential \gls{v_leak}		&	$\SI{0.5}{\V}$ 			& 	$\SI{0.5}{\V}$\\
		& reset potential \gls{v_reset}			&	$\SI{0.4}{\V}$			& 	$\SI{0.4}{\V}$\\
		& threshold \gls{thres} 				&	$\SI{0.75}{\V}$			& 	$\SI{0.7}{\V}$\\
		& learning rate $\eta$ 					&	2000					& 	30			\\
		& slope of fast sigmoid $\beta_\sigma$ 	&	$\SI{20}{\V^{-1}}$		& 	$\SI{20}{\V^{-1}}$	\\
		\bottomrule
	\end{tabular}
	\caption[Initial, hyper and neuron parameters per layer.]{Initial, hyper and neuron parameters per layer.}
	\label{initparameters}
\end{table}

In addition to a proper choice of the neuron parameters, the performance of the training strongly depends on the initialized weights and the chosen learning rate (\citealp{Goodfellow-et-al-2016}). In analogy to the simulation of SuperSpike, the weights are drawn from a normal distribution which is centered around zero (\citealp{zenke2018superspike}). The learning rates are set, such that both layers show learning activity but at the same time do not cause rapid weight updates that would in turn require a regularization to keep the weights within their limited range.

Another tunable parameter in the SuperSpike plasticity rule is the slope of the auxiliary function $\sigma(V)$, which is defined in \cref{auxilliaryfunction}. The choice of the parameter depends on the distance between the threshold and the resting potential. The slope is chosen such that at rest the surrogate gradient $\sigma'(V)$ provides little to no contribution and reaches its maximum when the membrane is about to cross the threshold.

Unlike the previous rate based plasticity rule from \cref{ratecoding}, SuperSpike does not contain a bias term. Instead, the bias is implicitly included into the input by dedicated input spikes that occur in all patterns. These ``bias spikes" can then excite or inhibit the membrane and thus establish a certain base level upon which further dynamics are induced by the remaining pattern-specific input spikes.

\begin{figure}[htb!]
	\centering
	\input{figures/debug_plot_0_100.pgf}
	\caption[Monitoring of the analog traces measured on the \gls{hx}.]{Monitoring of the analog traces measured on the \gls{hx}. Each column represents an important observable for the backward pass. The hidden and output layer are discriminated row-wise. The first column, contains the information of the presynaptic spike train spike train $S_j^{(l)}$, the second column shows the evolution of the membrane potential \gls{v_mem} with respect to the total synaptic input generated by the incoming spikes and in the third column the adapted van Rossum error $e^{(l)}$ is displayed.}
	\label{debugplot}
\end{figure}

\textbf{STOPPED HERE}

In \cref{debugplot} the relevant traces of the forward and backward pass are shown for the final parameter settings. In the first column the input spikes from the various sources are marked. Their contribution to the membrane potential depends on the type and strength of the respective synapse. Despite the calibration of the membrane potential, the fixed-pattern noise of the chip is still clearly visible. As soon as the membrane reaches the threshold, the reset mechanism is triggered. The thereby generated spikes are indicated in the second row of the first column, where they start to rise the membrane potential of the hidden neurons, which in turn can lead to spikes in the output layer.

The third column needs to be read starting from the second row. The error, as given in \cref{superspikeerror}, is defined by a temporal window within which the output unit needs to spike.
The interval is set in accordance with the duration of the input pattern and the time constants from \SI{0}{\micro \s} to \SI{80}{\micro \s}. In the example given in \cref{debugplot}, the output unit 1 is required to fire but the membrane potential falls just short to doing so. In this case, a target spike $t^*$ indicates where the silent output unit should have fired. The time is chosen to be near the end of input pattern at \SI{33}{\micro \s}. Depending on the preferred propagation method, the error of the output unit is then relayed backwards to the hidden unit either by transpose of the weight matrix (backpropagation) or by a random matrix (feedback alignment).

\begin{figure}
	\centering
	\input{figures/dweight_plot_0_100.pgf}
	\caption[Weight update in the backward pass using SuperSpike.]{Computation traces of the backward pass using SuperSpike. The integrand of the weight update $\Delta w_{ij}^{(o)}$ is given by the adapted von Rossum error $e^{(o)}$ (first column) and the convolution of the surrogate gradient with the presynaptic spike activity yielding the eligibility traces $\lambda_{ij}^{(o)}$ (second column). The final weight update is given by the integral over temporal traces of the last column.}
	\label{weightchangesplot}
\end{figure}

The final weight update depends on the error, a postsynaptic and a presynaptic term. The post and presynaptic term can be combined into so called eligibility traces $\lambda_{ij}^{(o)}$
\begin{equation*}
\lambda_{ij}^{(o)} = \alpha \ast 
\Big(\underbrace{\sigma'(V^{(o)}_{\text{m},i})}_{\text{Post}} 
\underbrace{\left(\epsilon \ast S_j^{(o)}\right)}_{\text{Pre}}\Big),
\end{equation*}
which can be interpreted as a memory over presynaptic events and how close the membrane is to spike . This memory then influences the update of the weight by how ``eligible" it is (c.f. \citealp{sutton2011reinforcement}). The composition of the weight update for the given examples in \cref{debugplot} are shown in \cref{weightchangesplot}

\section{Training and Results}

The classification task is now trained on the hidden layer network with SuperSpike and different propagation methods in the backward pass. For each method ten trials are executed with independent training data sets consisting of 2000 minibatches. 

\begin{figure}
	\centering
	\input{figures/superspiketaskconsecutive.pgf}
	\caption[Example of a training batch.]{Example of a training batch. After The chosen batch-size for the XOR-related task is eight. The network evaluates}
	\label{batchpatterns}
\end{figure}

A minibatch is created by randomly drawing eight patterns from a uniform distribution, resulting in an overall balanced training data set, but unbalanced batches. The length of an input pattern is set to \SI{40}{\micro \s} and within a batch the patterns are repeated approximately every \SI{250}{\micro \s}. The duration of a batch then sums up to roughly \SI{2}{\milli \s}, as shown in \cref{batchpatterns}.

The performance of each method is verified using a validation data set. After every update step, the progress of the training is evaluated by testing the network with each pattern exactly one time. The accuracy is then given by 

As in the previous experiment the accuracy of the validation or training batch is determined by the fraction of correctly identified inputs $n_\text{true}$ over the total number of inputs $n_\text{inputs}$ in a batch.
\begin{equation}
\text{Accuracy} = \frac{n_\text{true}}{n_\text{points}}.
\end{equation}
The overall accuracy is then averaged over the trials and the error is determined by a 95 \% confidence interval, which uses a t-score (c.f. \citealp{Smithson2011}). The final plot data is then resampled from 2000 to 500 samples using a polyphase method for better displaying (c.f. \citealp{scipypolyresample}).

\paragraph{Backpropagation vs. Feedback Alignment}
First, the standard backpropagation method is compared to feedback alignment, where instead of the transpose of the connecting weight matrix a random matrix is taken to propagate the error from the output to the hidden layer (c.f. \cref{superspike}). The random matrix is drawn from either a uniform distribution ranging from -25 to 25 or from a centered normal distribution $\mathcal{N}(0, 20)$.

\begin{figure}
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\input{figures/accuracy_train_73_BP_FA_small.pgf}
		\label{bptrain73}
	\end{subfigure}	
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\input{figures/accuracy_test_73_BP_FA_small.pgf}
		\label{bptest73}
	\end{subfigure}
	\caption[Train and test accuracy of SuperSpike using backpropagation and feedback alignment.]{Train and test accuracy of SuperSpike using backpropagation and feedback alignment. \textbf{(\subref{bptrain73})} The training accuracy is shown for various training methods. As expected, backpropagation (\textit{black}) outperforms feedback alignment (\textit{red} and \textit{blue}). The underlying distribution of the randomly drawn feedback weights for feedback alignment does not effect the convergence. \textbf{(\subref{bptest73})} The test accuracy does not differ much from the training accuracy.}
	\label{BPvsFA73}
\end{figure}

The accuracy of the training aligns well with the validation (see \cref{BPvsFA73}). The backpropagation method outperforms the use of a random feedback matrix clearly. Interestingly the choice of the underlying distribution of the random matrix has not a great impact on the performance of the training. Both distributions converge at the same velocity, despite an observed sensitivity on the range of the distributions during the parameter tuning.

\paragraph{Hidden Learning}
As a sanity check, the learning rate of the hidden layer is set to zero. This validates, that the initial state of the hidden layer is not yet solving the task and thus hidden learning is not even required by the setup.

In a more drastic approach the whole hidden layer is removed. According to the theory, the network should not be able to perform under these circumstances as the XOR-related classification task specifically requires a hidden layer structure with a non-linear activation function (c.f. \citealp{Goodfellow-et-al-2016}).

The performance of the setup with the applied limitations for the hidden layer is shown in \cref{hiddenlearning}. With the inability to train the hidden layer the accuracy cannot increase. The initial conditions are set in a way, that only little activity is generated by the hidden layer. In the following, the output layer receives only few spikes and fails to build a fire response upon them. In case of the removed hidden layer, the accuracy settles below 50 \%. This reflects the nature of the XOR-related classification task. In the best case only two out four patterns are correctly identified. 
All in all, this confirms that the presented experimental implementation of SuperSpike requires not only a deep network structure but also training activity within the hidden layer.

\begin{figure}[htb!]
		\centering
		\input{figures/accuracy_63_hiddentraining.pgf}
	\caption[Performance of SuperSpike without hidden learning or hidden layer.]{Performance of SuperSpike without hidden learning or hidden layer. The accuracy without the possibility to train the hidden layer remains low, as the initial state does not allow many spikes to pass the hidden layer. The accuracy of with a missing hidden layer settles below 50 \%. Due to the nature of the XOR related task, only two out of four answers can be correctly given without a deep structure.}
	\label{hiddenlearning}
\end{figure}

\paragraph{Transferability between Chips}
Throughout the development process of the experimental setup for SuperSpike several \gls{hx} setups have been in use. During this phase, the differences between the individual chips could be noticed to some extent, especially when hand tuned calibration routines had to be redone for a new setup.
\begin{figure}[b!]
	\begin{subfigure}{0.5\textwidth}
		%		\caption{}
		\centering
		\input{figures/hidden_mem_tra_73.pgf}
		%		\label{hx63vs73bp}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		%		\caption{}
		\centering
		\input{figures/hidden_mem_tra_63.pgf}
		%		\label{hx63vs73bp}
	\end{subfigure}
	\caption[Membrane trace comparison between \gls{hx} setups.]{Membrane trace comparison between \gls{hx} setups. The calibration of the resting potential for the setup ``63" (right) did not work as well as for ``73" (left).}
	\label{hxsetupmemtracescomparison}
\end{figure}
In a direct comparison of the calibrated membrane traces, the second chip is less precise. Specifically the resting potential is well spread (\cref{hxsetupmemtracescomparison}). As a consequence, the threshold level in the hidden and output layer has been increased by \SI{0.05}{V}. Overall, the performance appeared to not suffer too much. In an attempt to create some evidence, the presented data from above is remeasured on different setup. In \cref{hxsetuptransferability} the individual performance of each setup is compared per measurement method.

Given the fair amount of spread in the resting potential, the second chip performs still reasonable well.

As shown in \cref{backpropresults}, both setups achieve a similar accuracy with backpropagation. The ``73" setup achieved a slightly better average test accuracy over the last 100 and 250 steps. With respect to induced uncertainty by the various seeds, the difference between the setups is negligible. However, the convergence speed in \cref{hxsetuptransferability} is visibly different for both setups. 
\begin{table}[h!]\centering\ra{1.3}
	\begin{tabular}{@{}rlcc@{}}\toprule
		& \gls{hx}& mean Acc. (100 steps)	 & 	mean Acc. (250 steps) \\ \midrule
		& no. 73			& 	 $(97.2 \pm 9.6) \%$ & 	$(96.5 \pm 9.9) \%$\\
		& no. 63			&	$(95.1 \pm 11.3) \%$ & 	$(96.1 \pm 9.9) \%$\\
		\bottomrule
	\end{tabular}
	\caption[Performance of SuperSpike with Backpropagation.]{Performance of SuperSpike with Backpropagation. The test accuracy is averaged over the last 100 respectively 250 steps.}
	\label{backpropresults}
\end{table}
 %  $(97.2 \pm 9.6) \%$ 73 100
%  $(96.5 \pm 9.9) \%$ 73 250
%  $(95.1 \pm 11.3) \%$ 63 100
%  $(96.1 \pm 9.9) \%$ 63 250

The performance of a random propagation matrix based on a normal distribution (``FA normal") is almost identical for both chips. If feedback alignment is performed with a uniformly drawn random matrix (``FA uniform") the detuned chip converged slower. For this specific task, a normal distributed random weights is more robust to fixed-pattern noise than a uniformly drawn one. 
\begin{figure}[htb!]
	\begin{subfigure}{0.245\textwidth}
		\caption{}
		\vspace{-0.3cm}
		\centering
		\input{figures/accuracy_63VS73_BP_small.pgf}
		\label{hx63vs73bp}
	\end{subfigure}
	\begin{subfigure}{0.245\textwidth}
		\caption{}
		\vspace{-0.3cm}
		\centering
		\input{figures/accuracy_63VS73_FAnormal_small.pgf}
		\label{hx63vs73fanormal}
	\end{subfigure}
	\begin{subfigure}{0.245\textwidth}
		\caption{}
		\vspace{-0.3cm}
		\centering
		\input{figures/accuracy_63VS73_FAuniform_small.pgf}
		\label{hx63vs73fauniform}
	\end{subfigure}
	\begin{subfigure}{0.245\textwidth}
		\caption{}
		\vspace{-0.3cm}
		\centering
		\input{figures/accuracy_63VS73_nohiddenlearning_small.pgf}
		\label{hx63vs73nohidden}
	\end{subfigure}
	\caption[Setup transferability for SuperSpike.]{Setup transferability for SuperSpike. The \gls{hx} setups ``63" and ``73" are compared to each other. The performances of backpropagation, feedback alignment with a normal and uniform distribution and the no hidden learning are shown.}
	\label{hxsetuptransferability}
\end{figure}
All in all, the transferability between setups can not only be subjectively observed, but actually works without causing too much issues. One should keep in mind, that the calibration routine in use is heavily exposed to the cross-talk problematic, which can differ from setup to setup. Furthermore, the reduction of the fixed-pattern noise is limited to the neuron potentials. With the use of a more extensive and robust calibration routines, the performance and transferability will most certainly be increased. 

