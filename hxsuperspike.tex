\chapter{Surrograte Gradient Descent on \acrshort{bss2}}
\label{superspike}
The second experiment in this thesis is done on the most recent tape-out of the \gls{bss2} platform the \gls{hx}. In contrast to the first experiment the neural coding scheme is changed from rate based to temporal based. The prerequisites for a temporal coding on \glspl{snn} is to find a suitable plasticity rule, that copes well with the non-differential nature of individual spikes. A possible candidate, SuperSpike, was presented in \cref{superspike}. With the access to the temporal evolution of the membrane potential, it is now possible to implement the learning rule for the \gls{bss2} platform.

% Ã¼berleitungssatz to the task
To challenge the surrogate gradient approach a constructed task that is equivalent to solving the XOR operator is taken from its original publication by \cite{zenke2018superspike}.

\section{Task}
A total of 96 input units, each firing once at a fixed random spike time, is split into four overlapping collections of different size. As in the exclusive-or, the four different input patterns are assigned to two target classes which are represented as two distinct neurons in the output layer. Despite the multiple input sources, the task is by construction only two-dimensional. The first input spike train $S_1$ overlaps with all other patterns. The units involved in $S_1$ can therefore be interpreted as bias units, since they will fire regardless of the active pattern. In analogy to the XOR operator yielding $1 \veebar 0 = 0 \veebar 1 = 1$ for a single active input, the second and third spike train don't overlap except for the bias units. The union of $S_2$ and $S_3$ yields the final pattern, equaling to two active inputs $1 \veebar 1 = 0$. With respect to the XOR operator, $S_1$ and $S_4$ yield class $0$ whereas the remaining two are part of class $1$. 

\begin{figure}	
	\begin{subfigure}{0.5\textwidth}
		\caption{}
		\input{figures/superspiketasksector.pgf}
		\label{superspiketaskpicturesector}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\caption{}
		\input{figures/superspiketask.pgf}
		\label{superspiketaskpicture}
	\end{subfigure}
	\caption[XOR-related task for SuperSpike.]{XOR-related task for SuperSpike. (\subref{superspiketaskpicturesector}) A representative sector of the input data from the XOR-related task contains $2 + 4 + 4 + 6 = 16$ spikes from $2\times3$ different units and spike times. The first pattern ($S_1$) overlaps with all others and can be interpreted as a bias. The second and third pattern are disjoint apart from the  in the bias units and with the reference pattern, corresponding to $1 \veebar 0 = 0 \veebar 1 = 1$. The reference spike train is always on, equaling $1 \veebar 1 = 0$. (\subref{superspiketaskpicture}) The total number of input spikes per pattern is given by $20, 40, 40, 60$.
	\label{superspiketaskoverview}}
\end{figure} 

The fixed set of random times is picked from a $\SI{40}{\micro \s}$ time window, while the duration of a single measurement period is set to around $\SI{250}{\micro \s}$. In a slight modification to the derivation of SuperSpike, the error metric is changed from a single target spike $S_i^*$ to a target time window $[t^*_0, t^*_1]$. Depending on the input pattern $p_j$, the error of the output unit $i$ corresponding to the target class $i$ is given by
\begin{equation}
e_i = \begin{cases}
\alpha \ast \left(e_\text{outside, i} + e_\text{inside, i}\right),& \quad \quad \text{if} \quad \text{class}(p_j) == i \\
- \alpha \ast S_i,& \quad \quad \text{else} 
\end{cases}
\label{superspikeerror}
\end{equation}
In analogy to the van Rossum distance the error outside and inside the window can be written as 
\begin{align*}
e_\text{outside}(t) &= - S_i(t) \cdot \left(H(t^*_0 - t) + H(t - t^*_1)\right) \\
e_\text{inside}(t) &= 
\begin{cases}
0 ,&\quad \quad \text{if} \quad \exists \; t^{(s)}_i \in [t^*_0, t^*_1], \\
\epsilon \ast t^* ,& \quad \quad \text{else}.
\end{cases}
\end{align*}
Despite the technical notation, the error follows the same principles as the von Rossum distance: If the target neuron spikes within the designated time span, zero error is returned. In case there occurs no spike inside the window, a target spike at time $t^*$ will be added to the error instead. Any spikes outside the window sum with a negative sign. The error for the other output neuron is simple. Since it should remain silent, the error correlates to the negative spiking activity.

\section{Implementation on the \acrshort{bss2} Platform}
The tape-out of the \gls{hx} came with some flaws requiring several software workarounds. To minimize the implementation effort the use of the chip is restricted to the upper half and only one \gls{ppu}. Unlike the previous experiment, SuperSpike is not implemented in an on-chip fashion. However, a key element of SuperSpike, the access to the membrane potential via the \gls{cadc} is implemented as an on-chip readout routine.


\subsection{\gls{cadc} Readout}
With a few \gls{ppu} instructions for the general purpose part and the vector unit, the readout of the \gls{cadc} is performed. The code base for the readout has been developed as a quick workaround by Aron Leibfried, since the software stack did not support the use of the vector unit at that time.

For the purpose of the experiment, the membrane traces are required to be sampled at high rate and to cover as much data as possible. The available memory on the \gls{ppu} limits the readout to 100 samples of 128 different neurons with a temporal resolution of roughly \SI{2.5}{\micro \s}. 

Before usable traces can be recorded with the \gls{cadc}, its characteristic needs to be calibrated. The \gls{cadc} has tunable an analog slope and offset parameter per quadrant. In addition to the per-quadrant settings, each channel has an individual offset parameter, that can be adjusted as well.

In the \cref{cadccalibration} the final state of the characteristics is plotted two quadrants from the chip half in use. The thereby implicated conversion from \gls{dac} lsb to \si{\V} will be used for most of the data shown throughout this chapter.
\begin{figure}
	\begin{subfigure}{0.65\textwidth}
		\caption{}
		\input{figures/pre_cadc_calib.pgf}
		\label{precadccalib}
	\end{subfigure}
	\begin{subfigure}{0.35\textwidth}
		\caption{}
		\input{figures/post_cadc_calib.pgf}
		\label{postcadccalib}
	\end{subfigure}
	\caption[Pre and post calibration state of the \gls{cadc}]{Pre and post calibration state of the \gls{cadc}. \textbf{(\subref{precadccalib})} The raw cadc data of a reference voltage source ranging from 0.2 to \SI{1.0}{\V}. At the minimum voltage (\SI{0.0}{\V}) and maximum voltage (\SI{1.2}{\V}) the \gls{cadc} shows a non-linear behavior is therefore limited to the depicted range. The \gls{cadc} is divided into four quadrants. Only the ones from the used chip half are measured and calibrated. \textbf{(\subref{postcadccalib})} The cadc parameters are manually adjusted, such that they cover a useful dynamic range. The offset per channel can then be easily computed and corrected. The manual method was preferred over an automated fit-routine, since the ramp and slope parameters showed a sensitive cross-dependency in certain areas.}
	\label{cadccalibration}
\end{figure}

\subsection{Calibration of \gls{lif} Neuron}
%The manufacturing process of analog neuromorphic hardware causes systematic stochastic deviations in the neuron and synapse parameters. The thereby induced heterogeneity between neurons and synapses is referred to as \emph{fixed-pattern noise} and is constant in time. Despite the detuned parameters, it has been shown that plasticity rules can correct the intrinsic imperfections of the analog hardware to a certain degree (\citealp{wunderlich2019advantages}). 
During the calibration routine for the \gls{dls} it has already been mentioned, that analog neuromorphic hardware contains fixed-pattern noise, which in turn can be corrected up to a certain degree by the plasticity rule (\citealp{wunderlich2019advantages}). It is therefore a constant trade-off of between resources invested into the calibration and a decreasing accuracy that comes with less tuned analog setups.

At the time of the experiment, the development of chip-specific software for the new prototype was in an early stage. Among others, there was a lack of a calibration database and more generally, a lack of available calibrated hardware resources. This was largely due to an at the time slow and static calibration routine, that required the allocation of human and hardware resources for several hours to tune a setup.

The efficient parallelized readout of the \gls{cadc} presented itself to be a viable basis to put a quick alternative calibration routine into action. The main objective of the new routine is to provide a fast calibration that requires little interaction to bring a setup into a usable state with respect to the specific experiment requirements. 

As a trade-off, only a subset of the available parameters is tuned, namely the potentials of the \gls{lif} model \gls{v_leak}, \gls{v_reset} and \gls{thres}. By design, the potentials of the synaptic input $V_\text{syn, inh}$ and $V_\text{syn, exc}$ have a cross-dependency to the resting potential and thus need to be considered as well. The temporal constants \gls{tau_m} and \gls{tau_syn} are certainly important parameters in the \gls{lif} model. Motivated by the results from \citealp{wunderlich2019advantages} the induced error by the misalignment of the temporal constants is knowingly accepted and traded for less implementation effort.

Similar to the previous task, a binary search algorithm is chosen to find the proper \gls{dac} values of the analog parameters. A workaround was needed due to the cross talk between individual capacitive parameter memory cells, that arises if several cells are set to same value. By the design of the binary search algorithm, this event occurs repeatedly making the binary search a suboptimal choice for the calibration of the chip. The development of a proper gradient based calibration has already been started, but for the use case in this experiment the existing code base could be sufficiently stabilized with two minor adaptations. First, a random variation is consequently applied to all ``unused" \gls{dac}-values and second, the binary search algorithm is extended with a fall back option of the best parameter setting so far. The results of the final calibration for the \gls{lif} parameters are shown in \cref{hxprepostcalib}.
\begin{figure}
	\begin{subfigure}{0.32\textwidth}
		\caption{}
		\centering
		\input{figures/vleak_pre_post_calibration.pgf}
		\label{hxprepostvleak}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\caption{}
		\centering
		\input{figures/vreset_pre_post_calibration.pgf}
		\label{hxprepostvreset}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
	\caption{}
	\centering
	\input{figures/vthreshold_pre_post_calibration.pgf}
	\label{hxprepostvthreshold}
\end{subfigure}
	\caption[Pre and post calibration state of the analog \gls{lif} parameters.]{Pre and post calibration state of the analog \gls{lif} parameters. (\subref{precadccalib}) The raw cadc data of an controlled voltage ranging from 0 to \SI{1.2}{\V}. (\subref{postcadccalib}) the cadc parameters are manually adjusted, such that they cover a useful dynamic range. The offset per channel can then be easily computed and corrected. The manual method was preferred over an automated fit-routine, since the ramp and slope parameters showed a sensitive cross-dependency in certain areas.}
	\label{hxprepostcalib}
\end{figure}


\subsection{Experimental Setup}
The experiment is mainly controlled by a Python class called \texttt{HXBlackbox} implementing a set of tools that puts the \gls{hx} into operation. The code has been co-written by Sebastian Billaudelle and Benjamin Cramer. These tools are in turn based on a new software stack for the \gls{bss2} platform, which is still under continuous development. A detailed description of the software layers' architecture is provided by \citealp{mueller2020bss2ll}.

The execution of the experiment is embedded in an existing Python-based simulation frame that is oriented on the implementation of SuperSpike in \citealp{zenke2018superspike}. The forward pass is then replaced by measuring the analog traces and spike times on the \gls{hx} before the respective updates of the network parameters are computed in the backward pass on the host. In the following a network with a single hidden layer containing 30 units, 96 input sources and two output neurons which serve as target classes is configured using the \texttt{HXBlackbox}.

\subsubsection{Chip based Forward Pass}
The implementation of the experiment using the  is straight forward. The setup of the multilayer network structure is implemented by the event router and the synapse array using corresponding weights and addresses to discriminate between input and recurrent connections. The issue that arises with alternating weights for a single synapse is solved by the same double-row approach from the previous experiment implementation (\cref{circlesimplementation}). However, in this case the workaround is already implemented within the \texttt{HXBlackbox} and the user only needs to properly fill a logical weight matrix that is divided into input and recurrent connections as shown in \cref{hxnetworksetup}.
\begin{figure}
\[
\sbox0{$\begin{matrix}1&2&3\\0&1&1\\0&0&1\\0&0&1\end{matrix}$}
\sbox1{$\begin{matrix}1&2&3\\0&1&1\\0&0&1\end{matrix}$}
\sbox2{$\begin{matrix}1&2\\0&1\end{matrix}$}
%
W_\text{logical}=\left(
\begin{array}{c c c}
\vphantom{\usebox{0}}\underbrace{\Bigg(\makebox[\wd0]{\large$I \rightarrow H$}\Bigg)}_{n_\text{h}}\Bigg\}\scriptstyle{n_\text{in}}&
\makebox[\wd0]{\Large0\quad} & 
\makebox[0.5\wd0]{\Large0}\\
%\hdashline[0.3pt/5pt]
\vphantom{\usebox{1}}	\makebox[\wd1]{\Large0\quad}&
\underbrace{\Big(\makebox[1.3\wd0]{\large$H \rightarrow O$}\Big)}_{n_\text{out}}\Big\}\scriptstyle{n_\text{h}} &
\makebox[0.5\wd1]{\Large0}\\

\vphantom{\usebox{2}}	\makebox[\wd2]{\Large0\quad}&
\makebox[\wd2]{\Large0\quad}&
\makebox[0.5\wd2]{\Large0}

\end{array}
\right)
=
\left(
\begin{array}{c c c}
\vphantom{\usebox{2}}\makebox[\wd2]{\Large$W^{(h)}$}&
\makebox[\wd2]{\Large0\quad} & 
\makebox[0.5\wd2]{\Large0}\\
%\hdashline[0.3pt/5pt]
\vphantom{\usebox{2}}	\makebox[\wd2]{\Large0\quad}&
\makebox[\wd2]{\Large$W^{(o)}$} &
\makebox[0.5\wd2]{\Large0}\\
\vphantom{\usebox{2}}	\makebox[\wd2]{\Large0\quad}&
\makebox[\wd2]{\Large0\quad} &
\makebox[0.5\wd2]{\Large0}

\end{array}
\right)
	\]
\caption[Structure of the logical weight matrix on \gls{hx}]{Structure of the logical weight matrix on \gls{hx}. The first $n_\text{in}$ rows are reserved for the inputs and connected to the $n_\text{h}$ hidden units ($I\rightarrow H$) and are set by the weight matrix $W^{(h)}$. The recurrent connections  $H\rightarrow O $ are managed the remaining rows ... Todo sort out where exactly (0,0) is...probably on the bottom left coroner}
\label{hxnetworksetup}
\end{figure}

The input pattern is formulated as an array containing tuples of target neuron and spike time. The forward pass is then evaluated by a designated \texttt{stimulate} function, which returns all recorded spikes at the end of the measurement. The membrane potential traces are measured by triggering the on-chip \gls{cadc} readout program on the \gls{ppu} as soon as the input pattern is injected by the host. 

\subsubsection*{Host based Backward Pass}
Within the simulation framework of SuperSpike experimental data of the membrane traces and the spike events is gathered and processed to determine the updates of the weight matrices as derived in \cref{superspike}. In a practical approach, the integration of \cref{superspikeweightupdateeq} is done in finite temporal intervals which correspond to the length of the \gls{cadc} readout period.
\begin{equation}
\Delta w^k_{ij} = \eta \int_{t_k}^{t_{k+1}} dt'
e^{(o)}_k \; 
\; \alpha \ast 
\left( \sigma'(V^{(o)}_{\text{m},i}) \left(\epsilon \ast S^{(o)}_j\right) \right).
\label{superspikeweightupdatedivided}
\end{equation}

\begin{wrapfigure}{R}{0.45\textwidth}
	\centering
	\input{figures/cadcppuoffset.pgf}
	\caption[Offset measurement between \acrshort{cadc} traces and digital back end.]{Offset measurement between \acrshort{cadc} traces and digital back end. The time delta between peaks in the \acrshort{cadc} traces and the recorded spike times needs to be corrected.} 
	\label{cadcppuoffset}
\end{wrapfigure}

Before the updates can be computed, it is vital that both the measured \acrshort{cadc} traces and the recorded spike times by the digital back end are synchronized. In a respective offset measurement, the neurons are stimulated with a continuous input current such that they spike repetitively but with a low frequency. The located peaks in the resulting membrane traces are then compared to the digitally recorded spike times, yielding an offset $\Delta T$ of $(2.7 \pm 1.1)\; \si{\milli \s}$, see \cref{cadcppuoffset}. In the \gls{cadc} traces, the membrane potential maxes always before or exactly at the actual spike time, which results in a systematic error of the evaluated peak time. The offset has therefore been corrected to $\Delta T_\text{corr} = (2.1 \pm 1.1)\; \si{\milli \s}$. 

In an attempt to optimize the performance of the Python based simulation environment, Python bindings are used to speed up time consuming code in C++. In particular, the stored bits of the analog \gls{cadc} traces are inverted due to a hardware bug and need to be reverted before further use. Another task where C++ improves the performance is the injection of input spike trains during the forward pass. In the final setup, the overall runtime has been improved by about 25 percent. 

\subsubsection*{Initial Conditions and Neuron Parameters}

The chosen initial conditions and neuron parameters for this experiment are inspired by the configuration used in \citealp{zenke2018superspike}. However, some adaptations and a respective time-scaling have to be made, to cope with the analog hardware's acceleration factor of $10^3$. 

The temporal constants used in the backward pass are shown in the \cref{temporalconstants} and resemble an estimate of the detuned hardware time constants. .... 


The \gls{lif} neuron can be fully described by a set of temporal and potential parameters. The potential parameters have been calibrated are set as indicated in \cref{initparameters}. It is important the distance between the resting potential and the threshold is not big, since the synaptic strength is limited on the chip. Yet, a greater gap between these potentials provides a better accuracy for the computation of the backward pass. The final setting is a compromise of both assuring that the 6-bit weights are not exceeding their maximum during training, despite the lack of weight regularization.

In addition to a proper choice of the neuron parameters, the performance of the training strongly depends on the initialized weights and the chosen learning rate (ref to citations for init stuff). In analogy to the simulation of SuperSpike\footnote{\label{fnote}\citealp{zenke2018superspike}} the weights are drawn from a normal distribution which is centered around zero. The learning rates are set, such that both layers show activity but at the same time do not cause rapid weight updates that would in turn required a regularize to keep the weights within their limited range.

\begin{table}[t!]\centering\ra{1.3}
	\begin{tabular}{@{}rlll@{}}\toprule
		& Parameter		& 	\gls{hx} & 	$\text{SuperSpike}^1$ \\ \midrule
		& membrane constant \gls{tau_m}		& 	$\SI{8}{\micro \s}$ & 	$\SI{10}{\milli \s}$\\
		& synaptic constant \gls{tau_syn}	&	$\SI{5}{\micro \s}$ & 	$\SI{5}{\milli \s}$\\
		& refractory period \gls{refrac}	&	$\SI{30}{\micro \s}$ & 	$\SI{5}{\milli \s}$\\
		& $\alpha$ kernel constant $\tau_\alpha$	&	$\SI{12}{\micro \s}$& 	$\SI{10}{\milli \s}$\\
		& $\epsilon$ kernel constant $\tau_\epsilon$ 	&	$\SI{12}{\micro \s}$& 	$\SI{10}{\milli \s}$\\
		\bottomrule
	\end{tabular}
	\caption[Temporal parameters for SuperSpike.]{Temporal parameters for SuperSpike.}
	\label{temporalconstants}
\end{table}

Another tunable parameter in the SuperSpike plasticity rule is the slope of the auxiliary function $\beta$, which is defined in \cref{auxilliaryfunction}. This parameter is chosen with respect to the distance between the resting potential and the threshold. The surrogate gradient increases, the closer the membrane is about the cross the threshold. At rest, the surrogate gradient provides little to no contribution. 

\begin{table}\centering\ra{1.3}
	\begin{tabular}{@{}rlll@{}}\toprule
		& Parameter								& 	hidden neurons 			& 	output neurons \\ \midrule
		& initial weights $w_{ij}^\text{init}$	& 	$\in \mathcal{N}(0,13)$ & 	$\in \mathcal{N}(0,18)$\\
		& resting potential \gls{v_leak}		&	$\SI{0.5}{\V}$ 			& 	$\SI{0.5}{\V}$\\
		& reset potential \gls{v_reset}			&	$\SI{0.4}{\V}$			& 	$\SI{0.4}{\V}$\\
		& threshold \gls{thres} 				&	$\SI{0.75}{\V}$			& 	$\SI{0.7}{\V}$\\
		& learning rate $\eta$ 					&	2000					& 	30			\\
		& slope of fast sigmoid $\beta_\sigma$ 	&	$\SI{20}{\V^{-1}}$		& 	$\SI{20}{\V^{-1}}$	\\
		\bottomrule
	\end{tabular}
	\caption[Initial, hyper and neuron parameters per layer.]{Initial, hyper and neuron parameters per layer.}
	\label{initparameters}
\end{table}

Unlike the previous rate based plasticity rule from \cref{ratecoding} SuperSpike does not contain a bias term. Instead, the bias is implicitly included into the input by dedicated input spikes that occur in all patterns. These ``bias spikes" can then excite or inhibit the membrane and thus establish a certain base level upon which further dynamics is induced by the remaining pattern-specific input spikes.

\begin{figure}
	\centering
	\input{figures/debug_plot_0_100.pgf}
	\caption[Monitoring of the analog traces measured on the \gls{hx}.]{Monitoring of the analog traces measured on the \gls{hx}. Each column represents an important observable for the backward pass. The hidden and output layer are discriminated row-wise. The first column, contains the information of the presynaptic spike train spike train $S_j^{(l)}$, the second column shows the evolution of the membrane potential \gls{v_mem} with respect to the total synaptic input generated by the incoming spikes and in the third column the adapted van Rossum error $e^{(l)}$ is displayed.}
	\label{debugplot}
\end{figure}

The final traces of a forward and backward pass are depicted in \cref{debugplot}. In the first column the input spikes from the various sources are marked. Their contribution to the membrane potential depends on the type and strength of the respective synapse. Despite the calibration of the membrane potential, the fixed-pattern noise of the chip is still clearly visible. As soon as the membrane reaches the threshold, the reset mechanism is triggered. The thereby generated spikes are indicated in the second row of the first column, where they start to rise the membrane potential of the hidden neurons, which in turn can lead to spikes in the output layer.

The third column needs to be read starting from the second row. The error, as given in \cref{superspikeerror}, is defined by a temporal window within which the output unit needs to spike.
The interval is set in accordance with the duration of the input pattern and the temporal constants from \SI{0}{\micro \s} to \SI{80}{\micro \s}. In the example given in \cref{debugplot}, the output unit 1 is required to fire but the membrane potential falls just short to doing so. In this case, a target spike $t^*$ indicates where the silent output unit should have fired. The time is chosen to be near the end of input pattern at \SI{33}{\micro \s}. Depending on the preferred propagation method, the error of the output unit is then relayed backwards to the hidden unit either by transpose of the weight matrix (backpropagation) or by a random matrix (feedback alignment).

\begin{figure}
	\centering
	\input{figures/dweight_plot_0_100.pgf}
	\caption[Weight update in the backward pass using SuperSpike.]{Computation traces of the backward pass using SuperSpike. The integrand of the weight update $\Delta w_{ij}^{(o)}$ is given by the adapted von Rossum error $e^{(o)}$ (first column) and the convolution of the surrogate gradient with the presynaptic spike activity yielding the eligibility traces $\lambda_{ij}^{(o)}$ (second column). The final weight update is given by the integral over temporal traces of the last column.}
	\label{weightchangesplot}
\end{figure}

The final weight update depends on the error, a postsynaptic and a presynaptic term. The post and presynaptic term can be combined into so called eligibility traces $\lambda_{ij}^{(o)}$
\begin{equation*}
\lambda_{ij}^{(o)} = \alpha \ast 
\Big(\underbrace{\sigma'(V^{(o)}_{\text{m},i})}_{\text{Post}} 
\underbrace{\left(\epsilon \ast S_j^{(o)}\right)}_{\text{Pre}}\Big),
\end{equation*}
which can be interpreted as a memory over presynaptic events and how close the membrane is to spike . This memory then influences the update of the weight by how ``eligible" it is (c.f. \citealp{sutton2011reinforcement}). The composition of the weight update for the given examples in \cref{debugplot} are shown in \cref{weightchangesplot}

\section{Training and Results}

The classification task is now trained on the hidden layer network with SuperSpike and different propagation methods in the backward pass. For each method ten trials are executed with independent training data sets consisting of 2000 minibatches. 

\begin{figure}
	\centering
	\input{figures/superspiketaskconsecutive.pgf}
	\caption[Example of a training batch.]{Example of a training batch. After The chosen batch-size for the XOR-related task is eight. The network evaluates}
	\label{batchpatterns}
\end{figure}

A minibatch is created by randomly drawing eight patterns from a uniform distribution, resulting in an overall balanced training data set, but unbalanced batches. The length of an input pattern is set to \SI{40}{\micro \s} and within a batch the patterns are repeated approximately every \SI{250}{\micro \s}. The duration of a batch then sums up to roughly \SI{2}{\milli \s}, as shown in \cref{batchpatterns}.

The performance of each method is verified using a validation data set. After every update step, the progress of the training is evaluated by testing the network with each pattern exactly one time. The accuracy is then given by 

As in the previous experiment the accuracy of the validation or training batch is determined by the fraction of correctly identified inputs $n_\text{true}$ over the total number of inputs $n_\text{inputs}$ in a batch.
\begin{equation}
\text{Accuracy} = \frac{n_\text{true}}{n_\text{points}}.
\end{equation}
The overall accuracy is then averaged over the trials and the error is determined by a 95 \% confidence interval, which uses a t-score (c.f. \citealp{Smithson2011}). The final plot data is then resampled from 2000 to 500 samples using a polyphase method for better displaying (c.f. \citealp{scipypolyresample}).

\subsubsection*{Backpropagation vs. Feedback Alignment}
First, the standard backpropagation method is compared to feedback alignment, where instead of the transpose of the connecting weight matrix a random matrix is taken to propagate the error from the output to the hidden layer (c.f. \cref{superspike}). The random matrix is drawn from either a uniform distribution ranging from -25 to 25 or from a centered normal distribution $\mathcal{N}(0, 20)$.

\begin{figure}
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
%		\caption{}
		\input{figures/accuracy_train_73_BP_FA_small.pgf}
		\label{bptrain73}
	\end{subfigure}	
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
%		\caption{}
		\input{figures/accuracy_test_73_BP_FA_small.pgf}
		\label{bptest73}
	\end{subfigure}
	\caption[Train and test accuracy of SuperSpike using backpropagation and feedback alignment.]{Train and test accuracy of SuperSpike using backpropagation and feedback alignment. The training accuracy (left) does not differ much from the validation accuracy (right). As expected, the backpropagation method (BP) outperforms feedback alignment (FA). Despite the quick success of BP, the noise sources on the chip can be clearly seen in the course of the accuracy.}
	\label{BPvsFA73}
\end{figure}

The accuracy of the training aligns well with the validation (see \cref{BPvsFA73}). The backprogagation method outperforms the use of a random feedback matrix clearly. Interestingly the choice of the underlying distribution of the random matrix has not a great impact on the performance of the training. Both distributions converge at the same velocity, despite an observed sensitivity on the range of the distributions during the parameter tuning.

\subsubsection*{Hidden Learning}
As a sanity check, the learning rate of the hidden layer is set to zero. This validates, that the initial state of the hidden layer is not yet solving the task and thus hidden learning is not even required by the setup.

In a more drastic approach the whole hidden layer is removed. According to the theory, the network should not be able to perform under these circumstances as the XOR-related classification task specifically requires a hidden layer structure with a non-linear activation function (c.f. \citealp{Goodfellow-et-al-2016}).

The performance of the setup with the applied limitations for the hidden layer is shown in \cref{hiddenlearning}. With the inability to train the hidden layer the accuracy cannot increase. The initial conditions are set in a way, that only little activity is generated by the hidden layer. In the following, the output layer receives only few spikes and fails to build a fire response upon them. In case of the removed hidden layer, the accuracy settles below 50 \%. This reflects the nature of the XOR-related classification task. In the best case only two out four patterns are correctly identified. 
All in all, this confirms that the presented experimental implementation of SuperSpike requires not only a deep network structure but also training activity within the hidden layer.

\begin{figure}[htb!]
		\centering
		\input{figures/accuracy_63_hiddentraining.pgf}
	\caption[Performance of SuperSpike without hidden learning or hidden layer.]{Performance of SuperSpike without hidden learning or hidden layer. The accuracy without the possibility to train the hidden layer remains low, as the initial state does not allow many spikes to pass the hidden layer. The accuracy of with a missing hidden layer settles below 50 \%. Due to the nature of the XOR related task, only two out of four answers can be correctly given without a deep structure.}
	\label{hiddenlearning}
\end{figure}

\subsubsection*{Transferability between Chips}
Throughout the development process of the experimental setup for SuperSpike several \gls{hx} setups have been in use. During this phase, the differences between the individual chips could be noticed to some extent, especially when hand tuned calibration routines had to be redone for a new setup.
\begin{figure}[b!]
	\begin{subfigure}{0.5\textwidth}
		%		\caption{}
		\centering
		\input{figures/hidden_mem_tra_73.pgf}
		%		\label{hx63vs73bp}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		%		\caption{}
		\centering
		\input{figures/hidden_mem_tra_63.pgf}
		%		\label{hx63vs73bp}
	\end{subfigure}
	\caption[Membrane trace comparison between \gls{hx} setups.]{Membrane trace comparison between \gls{hx} setups. The calibration of the resting potential for the setup ``63" (right) did not work as well as for ``73" (left).}
	\label{hxsetupmemtracescomparison}
\end{figure}
In a direct comparison of the calibrated membrane traces, the second chip is less precise. Specifically the resting potential is well spread (\cref{hxsetupmemtracescomparison}). As a consequence, the threshold level in the hidden and output layer has been increased by \SI{0.05}{V}. Overall, the performance appeared to not suffer too much. In an attempt to create some evidence, the presented data from above is remeasured on different setup. In \cref{hxsetuptransferability} the individual performance of each setup is compared per measurement method.

Given the fair amount of spread in the resting potential, the second chip performs still reasonable well.

As shown in \cref{backpropresults}, both setups achieve a similar accuracy with backpropagation. The ``73" setup achieved a slightly better average test accuracy over the last 100 and 250 steps. With respect to induced uncertainty by the various seeds, the difference between the setups is negligible. However, the convergence speed in \cref{hxsetuptransferability} is visibly different for both setups. 
\begin{table}[h!]\centering\ra{1.3}
	\begin{tabular}{@{}rlcc@{}}\toprule
		& \gls{hx}& mean Acc. (100 steps)	 & 	mean Acc. (250 steps) \\ \midrule
		& no. 73			& 	 $(97.2 \pm 9.6) \%$ & 	$(96.5 \pm 9.9) \%$\\
		& no. 63			&	$(95.1 \pm 11.3) \%$ & 	$(96.1 \pm 9.9) \%$\\
		\bottomrule
	\end{tabular}
	\caption[Performance of SuperSpike with Backpropagation.]{Performance of SuperSpike with Backpropagation. The test accuracy is averaged over the last 100 respectively 250 steps.}
	\label{backpropresults}
\end{table}
 %  $(97.2 \pm 9.6) \%$ 73 100
%  $(96.5 \pm 9.9) \%$ 73 250
%  $(95.1 \pm 11.3) \%$ 63 100
%  $(96.1 \pm 9.9) \%$ 63 250

The performance of a random propagation matrix based on a normal distribution (``FA normal") is almost identical for both chips. If feedback alignment is performed with a uniformly drawn random matrix (``FA uniform") the detuned chip converged slower. For this specific task, a normal distributed random weights is more robust to fixed-pattern noise than a uniformly drawn one. 

All in all, the transferability between setups can not only be subjectively observed, but actually works without causing too much issues. One should keep in mind, that the calibration routine in use is heavily exposed to the cross-talk problematic, which can differ from setup to setup. Furthermore, the reduction of the fixed-pattern noise is limited to the neuron potentials. With the use of a more extensive and robust calibration routines, the performance and transferability will most certainly be increased. 

\begin{figure}[htb!]
	\begin{subfigure}{0.24\textwidth}
%		\caption{}
		\centering
		\input{figures/accuracy_63VS73_BP_small.pgf}
%		\label{hx63vs73bp}
	\end{subfigure}
	\begin{subfigure}{0.24\textwidth}
%		\caption{}
		\centering
		\input{figures/accuracy_63VS73_FAnormal_small.pgf}
%		\label{hx63vs73bp}
	\end{subfigure}
	\begin{subfigure}{0.24\textwidth}
%		\caption{}
		\centering
		\input{figures/accuracy_63VS73_FAuniform_small.pgf}
%		\label{hx63vs73bp}
	\end{subfigure}
	\begin{subfigure}{0.24\textwidth}
%		\caption{}
		\centering
		\input{figures/accuracy_63VS73_nohiddenlearning_small.pgf}
%		\label{hx63vs73}
	\end{subfigure}
	\caption[Setup transferability for SuperSpike.]{Setup transferability for SuperSpike. The \gls{hx} setups ``63" and ``73" are compared to each other. The performances of backpropagation, feedback alignment with a normal and uniform distribution and the no hidden learning are shown.}
	\label{hxsetuptransferability}
\end{figure}