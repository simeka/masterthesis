\chapter*{Introduction}

\epigraph{``So that I may perceive whatever holds \\The world together in its inmost folds."}{--- \textup{Johann W. v. Goethe}, Faust I}

Humankind has been striving for a conceptional understanding of the physical world, even long before Faust famously questioned his knowledge about what it holds together. The scientific progress of the last decades has shaped the earth into a new digitally interconnected place. Yet, our own tool, the human brain with which we have acquired this vast amount of knowledge, remains elusive. However, today's generation of neuroscientists is closer more than ever/is dedicated to bring light into the dark.

A promising approach to gain a better insight into the brain's mechanisms are \glspl{snn}, which are the basis of how information is shared and processed among neurons. \glspl{snn} not only provide a guiding principle to study the brain, but are of great interest for the efficient implementation of deep neural networks, as they inherit multiple favorable characteristics from their biological counterpart such as parallelized and event-based information processing, noise tolerance and a low power consumption.

With the ever growing demand of computational resources for conventional deep learning with \glspl{ann}, more efficient alternatives such as neuromorphic hardware solutions see an increased interest. As of today, several well known tech companies have launched their own neuromorphic platform. IBM started to work on \emph{TrueNorth} in 2008, Intel presented the \emph{Loihi} chip in 2018 and Google started selling the \emph{Coral} dev-board in 2019. But even before neuromorphic hardware caught the attention of the big industry names, academic projects had already been started. Among others, the EU's Human Brain Project (HBP) funds two promising approaches: \emph{SpiNNaker}, a digital based neuromorphic supercomputer located in Manchester and \emph{\gls{bss}}, a mixed-signal accelerated emulation for spiking neural networks based in Heidelberg.

In recent years, the field of neuromorphic computing has focused to find novel training algorithms for \glspl{snn} that can compete with the widely successful conventional deep learning methods. The ideas range from imitating \glspl{ann} with \glspl{snn}, to novel algorithms using a surrogate gradient descent (quote).

In this research two candidates of supervised training variants are presented in more detail and then implemented on different prototypes of the \gls{bss2} platform. In a first approach, a classical deep \gls{ann} is converted into a rate-based \gls{snn} which is then successfully trained by an \textit{on-chip} implementation of gradient descent on the prototype chip \acrshort{dls}, i.e. only on-chip resources are used to compute and apply the plasticity rule. A second experiment is conducted on a new revision of the \gls{bss2} platform, the \acrshort{hx}. With additional built-in observation features, e.g. of the membrane potential, a spiking variant of gradient descent called \emph{SuperSpike} can be used to train a deep \gls{snn}. A complete and high-performance on-chip implementation is not yet possible, despite the steady progress of the hardware. In the end, the bottlenecks of the different learning methods and potential future workarounds will be discussed.\\