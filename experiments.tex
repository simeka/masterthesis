\chapter{Experiments}
In 1969 Minsky and Papert have shown that a simplified version of a one-layer ANN, the perceptron, cannot solve the XOR operator. The XOR operator or "exclusive or" is identical to the combination of the logical operators OR and NOT AND (maybe small grafic?). In \textit{deep} network however, the task can be solved. In modern machine learning, solving the XOR operator has become somewhat the proof that the novel design of a network structure or the adapted learning algorithm still works - i.e. it can solve not linearly separable tasks.\\

As all deep layer networks can solve such tasks, spiking neural networks can as well. This offers a wide range of possibilities for the experiment design with SNNs. Throughout this master project two distinguished task have been implemented on the neuromorphic platforms built by the Electronic Vision(s) group.\\

In the introduction to deep learning (cf. \ref{deeplearning}) different ways of spike coding are presented. The first task, called \textit{circles}, uses a rate coding approach. An \textit{online} training method based on gradient descent with feedback alignment is implemented on the neuromorphic platform BSS-1 (DLSv2).\\

A second experiment is conducted on the first revision of the successor platform BSS-2 (HICANN-X). With additional built-in observation features, \textit{sparse coding} and a more complex online learning rule, SuperSpike, is chosen. A full online implementation of training method was not possible, despite the steady progress of the group's neuromorphic platform. The bottlenecks and potential future workarounds will be discussed at the end of the chapter.\\



Describe high level modules of BSS1/BSS2 like Neurons, Synapses, CAPMEM, PPU, CADC, MADC, ...



\include{circles}

\newpage

\include{hxsuperspike}
