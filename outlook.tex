\chapter{Discussion}
Within this thesis the implementation of deep learning methods for \gls{snn} has been demonstrated on analog neuromorphic hardware. A rate-based approach was successfully executed as an on-chip experiment on the \gls{dls}. A second temporal-based method, SuperSpike, was implemented on a full-size prototype of the \gls{bss2} platform and performed well at solving an XOR-related problem on two distinct \gls{hx} setups.

\paragraph{Rate Coding with SGD} An on-chip realization of gradient descent for a rate-based \gls{snn} was developed for the \gls{dls}. Despite continuing stability issues of the \gls{fpga} and imperfections of the analog hardware, the Circles dataset could be classified with an accuracy of $(99.6 \pm 0.8) \%$. This experiment resembles the first successfull on-chip implementation of a deep learning task on the analog \gls{bss2} platform. However, a transferability of the results to other setups has not yet been achieved, since an extensive fine-tuning of the new setup would be required first.

A main improvement of the task can be achieved by transferring the implementation to the most recent chip version. The \gls{hx} features dedicated on-chip noise-generators with which a sigmoid activation function can be easily realized. Moreover, the difficulty of the task can be increased as with the newly available neuron resources, 512 instead of 32, the network can be scaled up with respect to the number and size of the individual layers.

The tape-out of the new chip version has shifted the research focus within the group. To that end, the software development for the \gls{dls} has been halted. The older chip is still capable of performing remarkably well, but its successor outperforms it in several regards. With the promising prospects given by the new chip no further improvements are planned for the current implementation.

%Transfer function needs to be non-linear the exact shape and its dependencies on noise rate, time constants, .. is not so important if it is somehow shaped like a non linear activation function.
%- working point of membrane -> noise and spread as soon as the working point is left (see transfer function with bias plot) -> not so bad actually for training. the error is trained as well. 
%- init conditions of task are important. somehow neurons dont learn if the init condition is "bad". but what does that even  mean.. the init condition is bad. the behaviour: the neuron stays silent throughout the learning process. 
%- ausblick: notes from mfp talk maybe?
\subsubsection{Temporal Coding with SuperSpike}
% blabla about why not on chip
Unlike the previous experiment, SuperSpike is not implemented in a fully on-chip fashion. This is largely due to the hardware bugs in the current tape-out. The two most relevant issues are a reordering of the \gls{cadc} channels and inaccessible spike counters. With the symmetric design of the chip the \gls{cadc} is split into four respective quadrants, but the channels of the first and second as well as third and fourth quadrant are mingled with each other. In a scripting programming language such as Python this can be rather quickly unraveled, but will remain a potential source for hidden programming errors. In comparison, the programming capabilities of the general purpose unit are rather limited making it hard to develop a functioning experiment environment under these circumstances. 

However, the main obstacle remains the lack of on-chip knowledge over spike times. In principle there are two available sources to record the event of a spike: the digital back end which registers every spiking event precisely by the \gls{fpga} and the on-chip spike counters. The connection to the digital back end from a \gls{ppu} operates at a limited speed due to a known software bug and has not yet been fixed, making its use not feasible. The spike counters on the other hand, cannot be accessed by the \gls{cadc} since they are not properly connected. A fixed revision of the chip is already being manufactured. With the new tap-out a fully on-chip implementation of SuperSpike will be possible. 

 %  $(97.2 \pm 9.6) \%$ 73 100
 %  $(96.5 \pm 9.9) \%$ 73 250
 %  $(95.1 \pm 11.3) \%$ 63 100
 %  $(96.1 \pm 9.9) \%$ 63 250
The host supported implementation of SuperSpike on the \gls{hx} successfully solved an XOR-related benchmark with an average test accuracy of $(96.5 \pm 9.9) \%$ using backpropagation. Motivated by the results from \cite{wunderlich2019advantages} only the neuron potentials have been calibrated. The performance was reproduced on another setup with a similar test accuracy of $(96.1 \pm 9.9) \%$ despite significantly detuned neuron potentials. In addition, the convergence of SuperSpike using feedback alignment was also shown for both setups. It can be concluded that the SuperSpike learning rule is a robust supervised training solution for deep \glspl{snn} on analog neuromorphic hardware. In particular it has been shown, that the algorithm does not require calibrated time constants nor perfectly aligned neuron potentials.
%The overall measurement period can be reduced to the length of several spike duration equivalents for temporal coded \glspl{snn}, making SuperSpike a candidate to process streaming spike data \emph{online}.

\subsubsection*{Future Projects}
\label{futureprojects}
In the meantime, the developed framework for the hardware implementation of SuperSpike has been embedded in a TensorFlow environment by Sebastian Billaudelle and Benjamin Cramer. Early results have shown a competitive test accuracy of over \SI{96}{\%} for the MNIST dataset. A more profound investigation of these preliminary results will preprinted in a respective publication in the next months.

Furthermore, the started collaboration with Friedemann Zenke from the Friedrich Miescher Institute for Biomedical Research in Basel, co-author of the SuperSpike learning rule, will be continued. In a follow-up project the application of SuperSpike for real world problems such as speech recognition will be investigated by training recurrent \glspl{snn}. In an attempt to bridge the gap between biological time constants of a neuron (milliseconds) and the time constants of sensory input data (seconds), the slow neuronal adaption variable provided by the \gls{adex} neuron model will be used.