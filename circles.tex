\section{Circles on \gls{bss1}}
As a first experiment a toy data set called circles has been chosen. The data set represents a non-linear exercise and thus requires at network structure with at least one hidden layer to be successfully solved. The detailed \textit{on-chip} implementation of circles on \gls{dls} with a single hidden layer network using feedback alignment is presented in the next paragraphs.

%- single hidden layer network with 11 neurons (2 synapse lines necessary for each neuron -> 22)
%- 2 noise sources (exc/inh)
%- 2x2 input sources (exc/inh)
%(A single hidden layer network is trained using rate coding and feedback alignment.) maybe restructure the intro somehow...



\subsection{Rate Coding with Poisson Spike Trains}
\label{poissonspiketrains}

In rate coding, information is encoded in the fire rate $\nu$ of a neuron, which is determined by the number of spikes $n_\text{spikes}$ sent within a period $T$. 
\begin{equation}
\nu = \frac{n_\text{spikes}}{T}
\end{equation}
The distribution of the spikes in the postsynaptic spike train is based on Poisson processes (ref here). One way to numerically generate such a Poisson spike train representing a fire rate is to repeatedly perform a Bernoulli process on a short time interval $\Delta t$, i.e. an unfair coin flip with probability $p = \nu \cdot \Delta t$.	

On hardware the Bernoulli process is implemented by comparing a certain probability with a randomly drawn number. A popular method to generate random numbers on a system with limited memory and computation resources is the Xorshift (\cite{marsaglia2003xorshift}). The random number is generated by repeatedly applying the XOR operator on a given seed and various bit-shifted versions of itself. A bit-shift to the left by the factor of five $\text{seed} << 5$ equals to $\text{seed} \cdot 2^5$. The xorshift can be implemented in C as follows.

\begin{minted}{C}
uint32_t xorshift32(uint32_t* seed)
{
*seed ^= *seed << 13;
*seed ^= *seed >> 17;
*seed ^= *seed << 5;
return *seed;
}
\end{minted}

In theory the maximum rate is given by $\nu_\text{max, th} = \nicefrac{1}{\gls{refrac}}$, i.e. when the coin's probability equals one. With respect to hardware limitations, for example the time required to generate an artificial spike with the \gls{ppu}, the maximum frequency is limited to $\nu_\text{max, hw} = \nicefrac{1}{\SI{0.44}{\mu \s}} = \SI{2.27}{\mega \Hz}$.

In the final experiment setup the Poisson spike generator will be divided into four independent channels, each producing at max a fourth of $\nu_\text{max, hw}$.

However, another factor limits the rate as well - the 8-bit spike counters. With $T$ in the order of milliseconds and $\nu_\text{max} = \nicefrac{256}{T}$, a measurable rate is found around $\mathcal{O}(\SI{100}{\kilo \Hz})$.

\subsection{Circles Task}
\label{circlestask}
A region bounded by two concentric circles with radius $r_{\text{inner}}$ and $r_{\text{outer}}$ is called annulus. The circles data set describes in principle a set of points $p = p(x,y)$ in a two-dimensional plane, which are in one of two disjunct annuli, each representing a class. The goal is to find a decision boundary that successfully separates the two classes.

\begin{align}
\text{class}(p) =
\begin{cases}
0 ,&\quad \quad r_{\text{inner}}^2 < x^2 + y^2 < r_{\text{outer}}^2 \\
1 ,&\quad \quad R_{\text{inner}}^2 < x^2 + y^2 < R_{\text{outer}}^2
\end{cases}
\end{align}

With the 8-bit resolution of the spike counters the range of the input is determined by a signed 8-bit integer ($x, y \in [-128,127]$). Both inputs are mapped to a separate channel of the Poisson spike train generator of rate $\nu_\text{max}$, e.g. for $x$

\begin{equation}\label{inputfrequency}
\nu_{\text{input, x}}(x) = \nu_\text{max} \cdot \frac{x + 128}{255}.
\end{equation}

In a slight simplification of the task $r_{\text{inner}}$ is set to zero and $R_{\text{outer}}$ is replaced by the limitations of the input. The remaining two boundaries are set to $r_{\text{outer}} = \sqrt{8000}$ and $R_{\text{inner}} = \sqrt{13000}$.

\begin{figure}
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\inputpgf{figures}{nu_x_input.pgf}
		\label{nuxinput}
	\end{subfigure}
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\inputpgf{figures}{nu_y_input.pgf}	
		\label{nuyinput}
	\end{subfigure}
	\caption{Each point $p(x,y)$ is mapped to a frequency in "x-direction" (\subref{nuxinput}) and "y-direction" (\subref{nuyinput}). The value of the frequency is color coded. As an example let's assume a point from the lower right corner is drawn e.g. $p(100,-100)$: a unit in the hidden layer receives two independent inputs. One with a high frequency $\nu_{\text{in, x}}(100) \approx \SI{450}{\kilo \Hz}$ and the other with a low frequency $\nu_{\text{in, y}}(-100) \approx \SI{55}{\kilo \Hz}$.}
	\label{circlesinputs}

\end{figure}

The classes correspond to a certain target rate $\nu^*$ in the output unit.

\begin{align}
\label{circlestarget}
\nu^*(p) =
\begin{cases}
\SI{32.6}{\kilo \Hz} ,&\quad \quad \text{class}(p) = 0\\
\SI{93.5}{\kilo \Hz} ,&\quad \quad \text{class}(p) = 1
\end{cases}
\end{align}
A reasonable choice for the target rates is crucial for the training performance.

The error is then defined by the mismatch between target and fire rate in the output layer
\begin{equation}
e_\text{out}(p) = \gls{nuout}(p) - \nu^*(p)
\end{equation}
and the decision boundary, separating both classes, is chosen to be the mean of the target rates $\nu_\text{DB} = \frac{\nu_0^* + \nu_1^*}{2} = \SI{63.05}{\kilo \Hz}$.

\subsection{Transfer Function on \gls{dls}}



The continuous stimulation with Poisson spike trains leads to a Gaussian free membrane potential distribution centered around the resting potential (c.f. \cite{petrovici12phdthesis}). In a naive approach, the area of the distribution that exceeds a certain threshold correlates to number of spikes fired. Thereby the dynamics of the membrane, once are a spike is fired, are not considered. Even for very short time constants close to zero the exact behavior is more complicated than this strongly simplified picture. However, it still offers a correct intuition what the individual parameters of the Gaussian free membrane potential distribution imply for the transfer function: more noise leads to a wider distribution and thus a more gently incline of \gls{transfer}; inhibitory synaptic input moves the distribution to a lower mean value and v.v.; moving the threshold corresponds to an additional bias term.

\begin{figure}
	\label{vleak_w_noise}
	\begin{center}
		\input{figures/activation_function_vmem_distr_with_thres.pgf}
	\end{center}
	\caption{The distribution of the membrane potential set to $V_{\text{leak}}$ with external noise sources has a standard deviation of about \SI{20}{\milli\V}. Without noise only spread would be a magnitude lower. The part of the distribution that exceeds the threshold potential would lead to a spike.}
\end{figure}

In \cite{petrovici2016stochastic} the bias term of a \gls{lif} neuron's activation is mapped to a shift of the resting potential. The analog core brings more and less favorable operating points. Therefore the resting potential is kept constant and the threshold shifted instead (c.f. \cref{transferfunction_with_bias}).

\begin{figure}
	\label{transferfunction_with_bias}
	\begin{center}
		\input{figures/activation_function_w_bias.pgf}
	\end{center}
	\caption{Shifting \gls{thres} by $\delta V$ is interchangeable with adding a bias term to the activation and thus $b \propto \delta V$.}
\end{figure}

The overall transfer function depends several parameters $\gls{transfer} = \gls{transfer}(\gls{nuin}, b, \gls{refrac}, f_{\gls{v_mem}})$ and $f_{\gls{v_mem}} = f_{\gls{v_mem}}(V; \gls{v_leak}, \nu_\text{noise})$. In \cref{calibration} the calibration of \gls{transfer} is introduced.

\subsection{Calibration of \gls{transfer}}\label{calibration}
In an uncalibrated state the transfer function (c.f. \cref{transferfunction_wout_calib}) is quite well spread per neuron with regards to the maximum rate (set by \gls{refrac}) and the center rate (set by \gls{v_leak}). Up to a certain degree the training process of \gls{snn} is self-correcting the misalignment. In a perfect world with no noise it is even essential for \glspl{ann} to inject artificial noise in order to perform and thus one could argue that the uncalibrated state is actually beneficial to the training. However, the dynamic range of the individual neurons is limited and needs to overlap (c.f. \cref{transferfunction_w_calib}).

\begin{figure}
	%\captionsetup[subfigure]{justification=centering}
	\centering
	\begin{subfigure}[b]{0.47\textwidth}
	\caption{}
	\input{figures/uncalibrated_activation_function.pgf}
	\label{transferfunction_wout_calib}
	\end{subfigure}
	\begin{subfigure}[b]{0.47\textwidth}		
	\caption{}
	\input{figures/calibrated_activation_function.pgf}
	\label{transferfunction_w_calib}
	\end{subfigure}
	\caption{\subref{transferfunction_wout_calib}: In the uncalibrated state the maximum (\gls{refrac}) and center rate (\gls{v_leak}) deviates due to analog imperfection of the hardware. \subref{transferfunction_w_calib}: The calibration aligns the dynamic range of individual neurons, which will helps the training}
\end{figure}

The on-chip calibration uses a binary search algorithm (\cite{binarysearchsource}) to find suitable \gls{dac}-values for the analog neuron parameters of \gls{refrac} and \gls{v_leak}. In a binary search a target value is compared with the middle value of an sorted array. Depending on the outcome the lower or upper half is eliminated and the search comparison is repeated with the boundaries of the remaining half. After $\mathcal{O}(\log(n))$ repetitions the algorithm will find the target value in an array with $n$ entries. For the calibration to work, the sorted array is replaced by the 10-bit \gls{dac}-value and the measurement of \gls{transfer} is compared with a target rate.

Setting $\gls{nuin} = \nu_\text{max, hw}$, $b=0$, the resting potential to a working point of $\gls{v_leak} = \SI{300}{\milli \V}$ and disabling the noise $\nu_\text{noise} = 0$ yields a maximum $\gls{nuout} = \nicefrac{1}{\gls{refrac}}$. With the limitation of the spike counters and $T = \SI{2.3}{\milli \s} $ the target rate is $\gls{nuout}^* \approx \SI{111.3}{\kilo \Hz}$. To determine \gls{v_leak}, the input and bias is set to zero but the noise is enabled instead ($\nu_\text{noise} = \SI{70}{kHz}$). In this configuration \gls{transfer} will return the center rates which equals to half of the maximum frequency.
\begin{align*}\label{calibration_config}
\gls{refrac}&:\quad \gls{transfer}(\nu_\text{max, hw}, 0, \gls{refrac}, f_{\gls{v_mem}}) = \frac{1}{\gls{refrac}} \quad \text{ with } \nu_\text{noise} = 0\\
\gls{v_leak}&:\quad \gls{transfer}(0, 0, \gls{refrac}, f_{\gls{v_mem}}) = \frac{1}{2\gls{refrac}} \quad \text{ with } \nu_\text{noise} \neq 0
\end{align*}

The calibration of \gls{refrac} and \gls{v_leak} is repeated several times to reduce the potential the influence of the other yet uncalibrated parameter. The calibration of \gls{thres} is done implicitly by calibrating \gls{v_leak} onto the center rate. In \cref{transferfunction_w_calib} the calibrated state is displayed. 

\subsection{Implementation on \gls{dls}}

Todo: explain stochastic gradient descent, iterations, batchsize, ... in deep learning.

%The \gls{ppu} coordinates the experiment. After a proper initialization routine, the training loop is started: In a first step the input rates of a randomly drawn data point $p(x,y)$ are generated (c.f. \cref{poissonspiketrains}) and the forward pass is performed. The observables are then evaluated with the feedback alignment routine on the \gls{ppu} (backward pass) and the resulting in updates for weights and biases are applied, before the next iteration starts. 

The circles data set is solved by training a single hidden layer network with two inputs, eleven hidden units and one output.
The forward pass of this network is implemented using the neuromorphic architecture. A presynaptic spike train, e.g. generated by the \gls{ppu}, is injected row-wise into the $32\times32$ synapse array. Then, the synapses compare a dedicated address label of the incoming spike with their own. Once they match, the spike is weighted and forwarded to the column-wise connected neurons at the bottom as either inhibitory or excitatory current. Recurrent connections are made available by the use of a spike router, which routes the postsynaptic spike trains generated by a neuron back into the synapse array.


A single input row cannot provide alternating negative and positive inputs to the same neuron. Changing the synapses configuration simply costs too much time. To ensure a continuous evolution of the weight from negative to positive, two rows are used simultaneously for the same input - one in excitatory and one inhibitory mode. As the weight changes sign, the unused row is set silent. At the start of the experiment the weights are randomly initialized using a uniform distribution limited to $w_{ij} \in [-25, 25]$.


With the double-row structure, a total of six rows is used for the various inputs: two for the noise generation and four for the two-dimensional input. The recurrent connections of the hidden layer require another $22$ rows. In \cref{network_structure} the exact row assignment is sketched. Other than during the calibration process, where each neuron was calibrated individually, now the noise source needs to provide Poisson spike trains for all neurons in the hidden and output layer. As a compromise the source is divided further into eight channels. Four neurons will share the noise with other ones. The frequency of the noise in both layers is set to $\nu_\text{noise} \approx \SI{1.1}{\kilo \Hz}$ using weights with $w_\text{noise} = \pm 15$. The received input frequency varies for each layer. The input layer provides an input rate of up to $2 \times \SI{500}{\kilo \Hz}$ for each hidden unit. The output frequency of hidden units is limited by the spike counter and measurement period to $\gls{nuout} \approx \SI{110.9}{\kilo \Hz}$, leading to a theoretical maximum of $\gls{nuin}
= 11 \times \gls{nuout} \approx 1219.6$ for the output layer. 

The initial conditions of the network have been proven to crucial for its learning success. A ``silent" hidden units has a hard to time become active. Adding a weight depending term to the initial zero bias $b_i^\text{init} \propto \sum_j w_{ij}$ establishes a certain initial fire rate and solves the problem for most cases.


$\gls{v_leak} = \SI{300}{\milli \V}$
\begin{table}\centering\ra{1.3}
	\begin{tabular}{@{}rcllcllcll@{}}\toprule
		&layer	 & & $w_{ij}^\text{init}$		& $b_i^\text{init}$ & $\nu_{\text{in}}$	& & $\eta_\text{w}$	& $\eta_\text{b}$\\ \midrule
		&hidden  & & $\in[-25, 25]$	& $\propto \frac{1}{2} \sum_j w_{ij}$ & $\SI{500}{\kilo \Hz}$	& & $165e4$			& $2.3e4$		\\
		&output  & & $\in[-25, 25]$	& $0$ & $\SI{110.9}{\kilo\Hz}$& & $5.5e4$			& $0.23e4$		\\ \bottomrule
	\end{tabular}
	\caption{The different settings for the layers is listed in the table. The weights are initialized as a uniform distribution within the given boundaries. The bias set to zero, that means $\gls{thres} = \gls{v_leak}$ but for the hidden layer a weight dependent factor $\frac{1}{2} \sum_j w_{ij}$ ensures that a certain fire rate is established at the initial state. The input rates are given per input, such that the total possible input is multiplied by the number of input connections. The total possible input for a hidden unit is thus $2 \times \nu_\text{in}$. The learning rates may appear unusually high but they incorporate the norming as well and are tuned in a way that both layers are active (see \cref{network_monitoring}).}
\end{table}


The backward pass processes the observables (\gls{nuin} and \gls{nuout}) obtained by the forward pass to calculate the updates for weights and biases as derived in \cref{traningann}. The general-purpose unit of the \gls{ppu} operates with a 32-bit architecture. To ensure sufficiently small updates, the calculation is bit-shifted to the left and its results are then stochastically rounded before being shifted back and applied. Stochastic rounding has been proven by \cite{limitedprecisionpaper} to be a viable workaround for deep learning with parameters of limited precision.

The new bias is set by lowering or raising the \gls{dac}-value of the threshold. Compared to the speed of the accelerated neuromorphic core, the digital-to-analog conversion is slow. With the nature of its mechanism, the bias slowly and continuously evolves towards the new target value. As seen in \cref{transferfunction_with_bias} the impact of one \gls{dac} lsb (can i leave it like this or not?) corresponds to roughly a $\delta V$ of $\SI{1.75}{\milli \V}$ and thus the frequency changes by $\delta \nu \approx \SI{9.9}{\kilo \Hz}$. Instead of an instant and large bias update, a continuous and slow implementation can be beneficial.

Settings weights by the general-purpose unit is time and computation intensive due to the lack of parallelization. The vector unit is a viable alternative. The \gls{simd} instructions allow parallel access to all weights and therefore it is not only used to apply the weight changes but also to efficiently silence or unmute the respective synapses of the weights' double-row implementation. The necessary \emph{Assembly} code-base for this approach has been written by Sebastian Billaudelle and Benjamin Cramer. As an extension, a stochastic weight regularization has been added later on.

The regularization of both, excitatory and inhibitory weights is given by
\begin{align}
w_{ij}^\text{inh},\; w_{ij}^\text{exc}\equiv w_{ij} \in [0,2^6) \quad  \Rightarrow \quad \text{Reg}(w_{ij}) &= w_{ij} - \left(\left(w_{ij} \ll 1\right) \gg 6\right) \nonumber \\
								&= w_{ij} - \left\lfloor \frac{\lfloor 2 \cdot w_{ij}  \rfloor}{2^{6}} \right\rfloor								
							%	&= w_{ij} - \left\lfloor \lfloor 2 w_{ij} \rfloor \cdot 2^{-6} \right\rfloor
\end{align}
and is only applied with the probability $p=\nicefrac{1}{4}$.

\subsection{Training}
For a better illustration of the training process, a balanced data set of $200$ randomly drawn points is chosen to reflecting the current state at each node (see \cref{learning_process_s5}). Depending on the configuration of weights and bias, each hidden unit forms a tilted or even inverted version of the input schemes in \cref{circlesinputs}. These variations are then composed into a combination that is displayed in the output layer. At this stage of the learning process, the output unit doesn't reflect the target well, resulting in a high error and therefore also high updates for weights and biases.
\begin{figure}
	\label{learning_process_s5}
	\include{learning_process_s5}
	\caption{The initial state of the network shows several variations of the inputs in the hidden layer and a combination of them in the output layer, that does not yet compare to the target. Similar to the contour lines on a map one can guess the sigmoidal step in the hidden units. }
\end{figure}

 After the first 500 iterations, the total loss has almost been cut in half and the network starts to find a setting of avail. Less useful variants in the hidden layer are further modified, in particular their bias and the weights of their input connections. The beneficial ones are rewarded with a stronger connection to the output layer and further refinement of their bias and input weights.

\begin{figure}
	\label{learning_process_s500}
	\include{learning_process_s500}
	\caption{After 500 iterations some training success are noticeable: The hidden units six and seven show an interesting behavior. Despite of their relatively same shape only one unit keeps the shape (seven). The other one changes the input weights and the bias by quite a bit which could mean, that the randomly assigned weight of the feedback matrix is negative and thus results in an inverted feedback. However, the network still has plenty of other hidden units to choose from.}
\end{figure}

At 2500 iterations, see \cref{learning_process_s2500}, the network found a set of hidden units and a combination with which the target can be recreated in the output unit. The accuracy has almost reached $100 \%$.

\begin{figure}
	\label{learning_process_s2500}
	\include{learning_process_s2500}
	\caption{At 2500 iterations the accuracy of the networks is almost at $100 \%$. The network}
\end{figure}

The same data set that has been used to visualize the training process is now used to monitor the  performance of the training after each iteration by calculating the \gls{rmse} and the accuracy (see \cref{circles_acc}). The \gls{rmse} is given by
\begin{equation}
	\text{\gls{rmse}} = \sqrt{\frac{\sum_p e(p)^2}{n_\text{points}}}
\end{equation}

The accuracy is derived by comparing the number of correctly identified points $n_{true}$ to the total number of points in the set. Whether a point is correctly identified is determined by the decision boundary (see \cref{circlestask}).

\begin{equation}
\text{Accuracy} = \frac{n_\text{true}}{n_\text{points}}
\end{equation}

\begin{figure}
	\label{circles_acc}
	\begin{center}
		\input{figures/circles_learning_performance.pgf}
	\end{center}
	\caption{The learning performance of stochastic gradient descent on the circles task, i.e. batch size is equal to one, is monitored by two measures: the accuracy and the root mean square error (RMSE).}
\end{figure}

For further monitoring purposes, the evolution of the weights and biases are tracked for the hidden and output layer (see \cref{network_monitoring}).
\begin{figure}
	\centering
	\label{network_monitoring}
    \input{figures/network_evolution_circles.pgf}
	\caption{Describe what happens in the picture...}
\end{figure}

\subsection{Discussion}
MOVE TO VERY END

- transfer function needs to be non-linear the exact shape and its dependencies on noise rate, time constants, .. is not so important if it is somehow shaped like a non linear activation function.

- working point of membrane -> noise and spread as soon as the working point is left (see transfer function with bias plot) -> not so bad actually for training. the error is trained as well. 

- init conditions of task are important. somehow neurons dont learn if the init condition is "bad". but what does that even  mean.. the init condition is bad. the behaviour: the neuron stays silent throughout the learning process. 

- ausblick: notes from mfp talk maybe?