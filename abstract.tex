\begin{abstract}
%\textbf{Novel Deep Learning Algorithms for Neuromorphic Hardware}
Neurons and synapses are the biological basis for information flow and processing in the brain. By forming spiking neural networks, the brain is able to learn and remember. These networks have not only provided an insight into the brain's mechanisms but offer an alternative to artificial neural networks in deep learning. When implemented on analog neuromorphic hardware, they inherit a variety of favorable properties from their biological counterpart, such as parallel and event based information processing, noise robustness and a high energy efficiency. These characteristics are of great interest, since using conventional deep learning for real-world problems requires the resources of large supercomputer clusters. However, modern learning algorithms for neuromorphic hardware still have a hard time competing with the great success of deep learning. In this thesis, the performance of two candidates for training spiking neural networks is evaluated on the analog neuromorphic hardware platform BrainScales2. First, a rate-based spiking neural network is trained using gradient descent as a standalone on-chip experiment. Second, a surrogate gradient algorithm, SuperSpike, is implemented as an chip-in-the-loop experiment.
\end{abstract}
\renewcommand{\abstractname}{Zusammenfassung}
\begin{abstract}
%\textbf{Neuartige Deep Learning Algorithmen für Neuromorphe Hardware}
Neuronen und Synapsen bilden die biologische Basis f\"{u}r den Austausch und die Verarbeitung von Informationen im Gehirn. Mithilfe dieser spikenden neuronalen Netzen kann das Gehirn lernen und sich erinnern. Diese Netze dienen nicht nur dazu die Vorg\"{a}nge im Gehirn zu veranschaulichen, sondern sind auch eine Alternative zu Deep Learning mit k\"{u}nstlichen neuronalen Netzen. Im Einsatz auf analoger neuromorpher Hardware \"{u}bernehmen spikende neuronale Netze einige vorteilhafte Eigenschaften ihrer biologischen Vorbilder, wie zum Beispiel paralleles und eventbasiertes Verarbeiten von Informationen, eine erh\"{o}hte Fehlertoleranz sowie eine hohe Energieeffizienz. Unter dem Gesichtspunkt, dass die Ressourcen eines H\"{o}chstleistungsrechners ben\"{o}tigt werden, um mit klassischem Deep Learning echte Probleme l\"{o}sen zu k\"{o}nnen, sind die zuvor genannten Leistungsmerkmale besonders interessant. Allerdings k\"{o}nnen sich moderne Lern-Algorithmen f\"{u}r neurmorphe Hardware noch nicht mit dem Erfolg von k\"{u}nstlichen Neuronalen Netzen auf konventioneller Hardware messen. In dieser Arbeit wird die Funktion und Performance von zwei Algorithmen auf der analogen neuromorphen Plattform BrainScales2 evaluiert. In einem ersten Experiment wird ein raten-basiertes spikindes neuronales Netz mit einem Gradientenverfahren in einer alleinstehenden ``on-Chip" Implementierung trainiert. Das zweite Verfahren beruht auf einem Surrogate-Modell des Gradientenverfahrens, SuperSpike, welches als Chip-in-the-Loop Experiment durchgeführt wird.
\end{abstract}


