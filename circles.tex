\chapter{Classification of the Circles Data Set on \acrshort{bss2}}
\label{circles}
In the first of two deep learning experiments which are implemented on the \gls{bss2} platform, a spiking feedforward network is emulated as an \gls{ann} using rate coding. As derived in \cref{ratecoding} the \gls{snn} can be then trained with conventional backpropagation. In addition the experiment focuses on a fully on-chip implementation. %, i.e. the training of the network is executed using only on-chip available resources, such as the dedicated \acrlong{ppu}.
Only for monitoring purposes of the training progress the \gls{ppu}'s memory is continuously read out through the \gls{fpga} interface.

\begin{table}[htb!]\centering\ra{1.3}
	\begin{tabular}{@{}rccc@{}}\toprule
		& Input A								& Input B 			& Output A $\veebar$ B	\\ \midrule
		& 0	& 	0	&	0	\\
		& 0	& 	1 	&	1	\\
		& 1 &	0	& 	1\\
		& 1&	1	& 	0\\
		\bottomrule
	\end{tabular}
	\caption[XOR classification.]{The XOR classification follows the prediction given in the table: if the binary inputs are not equal the XOR operator returns true and false otherwise.} 
	\label{xortruthtable}
\end{table}

In deep learning solving the XOR or ``exclusive or" problem (c.f. \cref{xortruthtable}) has become a very first benchmark to test the functionality of novel network designs and algorithms, since it requires a multi-layer network structure with a non-linear activation function to be solved (\citealp{Goodfellow-et-al-2016}). As part of a preliminary work, an on-chip implementation of \glsfirst{sgd} on the prototype \gls{dls} has been successfully tested with the XOR problem.

In a next step, the difficulty of the task has been increased with the classification of the \emph{Circles} data set. The data set consists of two classes representing an inner and outer circle as shown in \cref{circlestasksketch}. A deep network then tries to place a decision boundary in between the two circles. Before discussing the details of the training process, the rate coding of the task and the implementation of the network's training method \gls{sgd} are presented in the following sections. 

%At the beginnings of the machine learing era, \cite{perceptron} have shown that a simplified single-layer neuronal network, the perceptron, cannot solve the XOR problem or ``exclusive or". The XOR operator represents a class of problems, that are non-linear and thus require a multi-layer network structure with a non-linear activation function to be solved (\citealp{Goodfellow-et-al-2016}). In modern machine learning solving the XOR problem has therefore become a very first benchmark to test the functionality of novel network designs and algorithms.


%The classification of the \emph{Circles} data set is the first of two deep learning experiments implemented on the \gls{bss2} platform and focuses on a rate coding approach which has been motivated in \cref{ratecoding}. After successfully testing an on-chip \acrlong{sgd} implementation on the prototype \gls{dls} with an XOR problem, the difficulty of the task has been increased with the Circles data set. The task is implemented again as an on-chip experiment, i.e. the training of the network is executed using only on-chip available resources, such as the dedicated \acrlong{ppu}. Only for monitoring purposes of the training progress the chip's memory is continuously read out through the FPGA interface.
%A detailed description of the \textit{on-chip} implementation is presented in the next paragraphs.

%- single hidden layer network with 11 neurons (2 synapse lines necessary for each neuron -> 22)
%- 2 noise sources (exc/inh)
%- 2x2 input sources (exc/inh)
%(A single hidden layer network is trained using rate coding and feedback alignment.) maybe restructure the intro somehow...


\section{Circles Task}
\label{circlestask}
\begin{wrapfigure}{R}{0.33\textwidth}
	\centering
	\vspace{-0.6cm}
	\input{figures/circles_task.pgf}
	\caption[Circles data set.]{The Circles data set has two classes (\textit{red} and \textit{black}) which can be separated by a decision boundary (\textit{blue}).} 
	\label{circlestasksketch}
	\vspace{-1cm}
\end{wrapfigure}
The Circles data set describes a set of points $p = p(x,y)$ in a two-dimensional plane, which are in either of two disjunct rings, each representing a class
\begin{align}
\text{class}(p) =
\begin{cases}
0 ,&\quad \quad r_{\text{inner}}^2 < x^2 + y^2 < r_{\text{outer}}^2, \\
1 ,&\quad \quad R_{\text{inner}}^2 < x^2 + y^2 < R_{\text{outer}}^2,
\end{cases}
\end{align}
with the confining inner and outer radii of the first and second ring $r_{\text{inner}}$, $r_{\text{outer}}$ and  $R_{\text{inner}}$, $R_{\text{outer}}$ respectively. The goal of the task is to find a decision boundary that successfully separates the both rings as shown in \cref{circlestasksketch}.

The Circles task is slightly modified to reduce the implementation effort on the \gls{ppu} and to make it easier to find a decision boundary. The class $0$ is reduced to one confining circle by setting the inner radius to zero. The outer radius of the second class is replaced by the \SI{8}{\bit} limitations of the input. The remaining two boundaries are set far apart to $r_{\text{outer}} = \sqrt{8000}$ and $R_{\text{inner}} = \sqrt{13000}$. The area in between the classes is where the network should place the decision boundary. The more space is available, the easier it is for the network to succeed. 

\begin{figure}[b!]
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		%		\caption{}
		\inputpgf{figures}{nu_x_input.pgf}
		%		\label{nuxinput}
	\end{subfigure}
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		%		\caption{}
		\inputpgf{figures}{nu_y_input.pgf}	
		%		\label{nuyinput}
	\end{subfigure}
	\caption[Input coding for the Circles experiment.]{Input coding for the Circles experiment. Each point $p(x,y)$ of a representative data set with 100 points per class is mapped to two input frequencies each depending on one of the coordinates. In the left figure, the $x$-dependency is shown and to the right the $y$-dependency.}
	\label{circlesinputs}
\end{figure}

With a rate-based coding, the data points need to be translated into firing rates first. A point $p$ is represented by two signed \SI{8}{\bit} integers which correspond to the coordinates $x$ and $y$ with $x, y \in [-128,127]$. The corresponding firing rate of a coordinate $c \in \{x,y\}$ is then given by
\begin{equation}\label{inputfrequency}
\nu_{\text{input, c}}(c) = \nu_\text{max} \cdot \frac{c + 128}{255},
\end{equation}
with the maximum firing rate $\nu_\text{max}$ set by a respective spike train source. A representative data set of 100 samples per class illustrates in \cref{circlesinputs} how the points of the modified task are distributed. Moreover, the translation of a point into an input rate is shown. For instance, a point from the lower right corner p(100,100) is associated with two input rates: one depends on the x coordinate $\nu_{\text{in, x}}(100) \approx \SI{450}{\kilo \Hz}$ and one on the y coordinate $\nu_{\text{in, y}}(-100) \approx \SI{55}{\kilo \Hz}$.

As for a supervised learning method, the input data is labeled. To identify the class of an input pattern in the output layer, a single readout neuron is trained to yield a target rate $\nu^*$
\begin{align}
\nu^*(p) =
\begin{cases}
\nu_0^* = \SI{32.6}{\kilo \Hz} ,&\quad \quad \text{class}(p) = 0,\\
\nu_1^* = \SI{93.5}{\kilo \Hz} ,&\quad \quad \text{class}(p) = 1.
\end{cases}
\label{circlestarget}
\end{align}
The mismatch between the target rate and firing rate of the readout neuron \gls{nuout} then determines the error of the output layer $e^{(o)}$
\begin{equation*}
e^{(o)}(p) = \gls{nuout}(p) - \nu^*(p).
\label{circleserror}
\end{equation*}
The decision boundary, the rate separating both classes, is chosen to be the mean of both target rates
\begin{equation*}
\nu_\text{DB} = \frac{\nu_0^* + \nu_1^*}{2} = \SI{63.05}{\kilo \Hz}.
\label{circlesdb}
\end{equation*}

%A reasonable choice of the target rates is crucial for the training success. If the rates are either too close or too far from each other, the network will have troubles to solve the task. 

\section{Poisson Spike Train Generator}
\label{poissonspiketrains}
The on-chip implementation of the experiment requires the generation of spikes for the input data and noise spikes for sigmoid activation function. The latter relies on spike trains with an underlying Poisson distribution of the spikes. With the lack of dedicated spike generators on-chip, the \gls{ppu} has to fulfill the task.

One way to numerically generate a Poisson spike train is to repeatedly perform a Bernoulli process on a short time interval $\Delta t$ over a period $T$. The Bernoulli process is equivalent to an unfair coin flip with probability $p$, which is set depending on the desired firing rate $\nu$ by $p(\nu) = \nu \cdot \Delta t$. Given that the probability equals one, i.e. at every time interval a spike will be fired, the firing rate reaches its maximum at $\nu_\text{max} = \nicefrac{1}{\Delta t}$. However, the spike train is no longer Poisson-based for high firing rates but rather fires with a fixed \glsfirst{isi}. Therefore, the spike generator will only be used at low frequencies for the noise generation ($\nu_\text{noise} \ll \nu_\text{max}$). The input spike trains on the other hand do not rely on pure Poisson spike trains and thus can use the full frequency range of the spike train generator.

With respect to hardware limitations, the maximum fire rate of the spike train generator $\nu_\text{max, ppu}$ is defined by the shortest possible \gls{isi} on the \gls{ppu} which is in turn limited by the time required to generate a single spike $\Delta t_\text{spike}$
\begin{align*}
\nu_\text{max, ppu} = \frac{1}{\Delta t_\text{spike}} = \frac{1}{\SI{0.44}{\mu \s}} = \SI{2.27}{\mega \Hz}.
\end{align*}
The duration of the spike generation $\Delta t_\text{spike}$ on the \gls{ppu} is determined by recording a constant spiking pattern with the highest possible frequency. The total number of spikes is counted by on-chip spike counters and the duration is simply measured by recording the membrane potential with an oscilloscope.

% Stopped here, rewrite again not schlüssig yet...
%The use case of the spike generator in the final setup is to generate noise spike trains for the sigmoid activation function and to translate the Circles data set into input rates. Only the latter requires high input rates up to $\SI{500}{\kilo \Hz}$, the noise rate is only in the order of a few $\si{\kilo \Hz}$.

%In theory the maximum fire rate of a \gls{lif} neuron $\nu_\text{max}$ is give by $\nu_\text{max} = \nicefrac{1}{\gls{refrac}}$.
%In the final experiment setup, the output rate of an individual neuron is further constraint by a chosen 8-bit resolution of the spike counts and a measurement period of a few milliseconds to a measurable rate in order of few $\SI{100}{\kilo \Hz}$.
%will be divided into four independent channels, each producing at max a fourth of $\nu_\text{max, hw}$.
On the \gls{ppu} the Bernoulli process is implemented by comparing the frequency dependent probability $p(\nu)$ with a randomly drawn number. A popular method to generate random numbers on a system with limited memory and computational resources is the \emph{xorshift} (\citealp{marsaglia2003xorshift}). A random number is thereby generated by repeatedly applying the XOR operator on a seed variable and a bit-shifted versions of itself. %The \emph{bit shift} of a variable $x$ to the left by $n$, written as $x\;{\scriptstyle<<}\;n$, corresponds to the multiplication by $2^n$. In analogy, a shift to the right, $x\;{\scriptstyle>>}\;n$, is equivalent to the division by $2^n$. 


%The xorshift can be implemented in C as follows.
%\begin{minted}{C}
%uint32_t xorshift32(uint32_t* seed)
%{
%*seed ^= *seed << 13;y
%*seed ^= *seed >> 17;
%*seed ^= *seed << 5;
%return *seed;
%}
%\end{minted}

\section{Activation Function on Chip}
The theoretical background of how the activation function of a \gls{lif} neuron becomes sigmoidal has been motivated in \cref{ratecoding}. The main tool to create such an activation function is the continuous stimulation of the neuron with Poisson noise spike trains. Thereby a broad Gaussian distribution of the membrane potential is established, which in turn yields a sigmoid activation function. In \cref{vleak_w_noise}, the recorded free membrane potential on the \gls{dls} is shown while being stimulated by excitatory and inhibitory noise spike trains with a frequency of \SI{70}{\kilo \Hz} and a synaptic strength of $w_\text{noise}= 15$. 

The overall activation function depends on several parameters such as the neuron potentials, the synaptic weights and frequencies of the spike sources as well as the time constants \gls{tau_m}, \gls{tau_syn} and \gls{refrac}. Most of these parameters are not required to be changed during the experiment. However, their choice has a great impact on the shape of the activation function. A suitable setting for these parameters was found by performing various parameter sweeps on the \gls{dls}. In the appendix in \cref{hardwarevssimulationtable} the final choice of parameters for the activation function are compared to ones chosen for the simulation in \cref{ratecoding}.
%The continuous stimulation with Poisson spike trains leads to a Gaussian free membrane potential distribution $f_{\gls{v_mem}}$ centered around the resting potential \gls{v_leak} as depicted in \cref{vleak_w_noise}). In a naive approach, the part of the distribution that exceeds a certain threshold potential correlates to the number of fired spikes. This neglects non vanishing effects from the fire dynamics of the membrane which have been investigated in more detail by \citealp{petrovici12phdthesis}. The impact of these dynamics can be reduced by the use of very short time constants for the synaptic input \gls{tau_syn} and the membrane \gls{tau_m}.
%
%However, despite the strongly simplified picture, this view still offers a correct intuition how the threshold and leak potential as well as the strength of the noise effect the free membrane potential and in turn change the shape of the transfer function: more noise leads to a broader distribution and thus a more gently incline of the sigmoid; synaptic input moves the distribution to either to a lower or higher mean value; moving the threshold corresponds to an additional bias term.
\begin{figure}
	\begin{center}
		\input{figures/activation_function_vmem_distr_with_thres.pgf}
	\end{center}
	\caption[Gaussian free membrane potential distribution on \gls{dls}.]{Gaussian free membrane potential distribution on \gls{dls}. A broad distribution of the free membrane is achieved by sufficiently stimulating the membrane with Poisson noise spikes. By changing the frequency and impact of the noise spikes the width of the distribution can be changed. In a naive approach, the part of the distribution that exceeds a certain threshold potential correlates to the number of fired spikes (\textit{black}).}
	\label{vleak_w_noise}
\end{figure}

%As shown in the simulation of the activation function in \cref{ratecoding}, the slope and of the activation function and its alignment along the x-axis can be easily modified.

%\begin{equation}
%\gls{transfer} = \gls{transfer}(\gls{nuin}, b, \gls{refrac}, \gls{tau_m}, \gls{v_reset}, f_{\gls{v_mem}}),
%\end{equation}
%with the free membrane distribution $f_{\gls{v_mem}} = f_{\gls{v_mem}}(V; \gls{v_leak}, \nu_\text{noise}, w_\text{noise})$. 
%The bias term $b$ in the neuron's activation is again replaced by the relative distance $\delta V$ between the resting potential and the threshold, since $b \propto \delta V = \gls{v_leak} - \gls{thres}$. 
%
%Some parameters will be kept fixed, whereas others are required to operate within a certain range such as the threshold or the input frequency. The setup is hand tuned with the support of parameter sweeps to find a suitable configuration of the fixed parameters. The final set of parameters is listed in \cref{hardwarevssimulationtable}
%
\paragraph{Calibration}\label{calibration}
The manufacturing process of analog neuromorphic hardware causes systematic stochastic deviations in the neuron and synapse parameters. The thereby induced heterogeneity between neurons and synapses is referred to as \emph{fixed-pattern noise} and is constant in time. In an uncalibrated state, the neuron circuits exhibit activation functions which do not align with regards to their maximum rate and alignment along the x-axis (see \cref{transferfunction_wout_calib}). Despite the detuned parameters, it has been shown that plasticity rules can correct the intrinsic imperfections of the analog hardware up to a certain degree (\citealp{wunderlich2019advantages}). However, the dynamic range of the individual neurons on the chip is limited and to ensure properly overlapping activation functions a calibration of the respective parameters is inevitable.

A collection of simple experiment-specific \gls{ppu}-based calibration routines is implemented to maintain a pure on-chip implementation. The calibration is based on a binary search algorithm to find the suitable \gls{dac}-values for the analog neuron parameters. In general, the binary search algorithm compares a target value to the middle value of a given sorted array. Depending on the outcome of the comparison the lower or the upper half is eliminated from the search space and the search is repeated with new boundaries which are set by the remaining half. In a worst case scenario, the algorithm finds the target value after $\mathcal{O}(\log(n))$ repetitions, given the searched array has $n$ entries  (\citealp{binarysearchsource}).

\begin{figure}[tb!]
	%\captionsetup[subfigure]{justification=centering}
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\caption{}
		\input{figures/uncalibrated_activation_function.pgf}
		\label{transferfunction_wout_calib}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\textwidth}		
		\caption{}
		\input{figures/calibrated_activation_function.pgf}
		\label{transferfunction_w_calib}
	\end{subfigure}
	\caption[Calibration of the sigmoid activation function on \acrshort{dls}.]{Calibration of the activation function on \acrshort{dls}. \textbf{(\subref{transferfunction_wout_calib})}: In the uncalibrated state the maxima of the activation function and the position of the inflection point deviates due to fixed pattern noise of the hardware. \textbf{(\subref{transferfunction_w_calib})}: The calibration of the refractory time and the resting potential aligns the activation functions well.}
\end{figure}

For the calibration of the activation function the sorted array is replaced with the \SI{10}{\bit} range of the targeted analog parameter. Instead of the direct comparison with a target value, a condition is chosen which resembles the desired calibration outcome. For instance, the maximum output frequency of the neuron is calibrated
by changing the \gls{dac} value of the refractory period \gls{refrac}, since in the limit of high frequencies the output rate mainly depends on the refractory period, c.f. with \cref{fireratehigh}. As condition for the binary search, the maximum output rate \gls{nuout} is measured at a high input rate and compared to a target rate $\gls{nuout}^*$. Depending on the outcome, either half of the parameter range is eliminated from the search space.
For the experiment, the target rate is set to \SI{111.3}{\kilo \Hz} which corresponds to $256$ recorded spikes over a measurement period of $T=\SI{2.3}{\milli \s}$.

The calibration of the alignment along the x-axis is slightly more complicated. The activation function is centered if its inflection point is positioned at zero. In theory, the firing rate at the inflection point equals half the maximum rate. As shown in the simulation of the activation function in \cref{ratecoding}, the curve can be shifted along the x-axis by changing the potential difference $\delta V$ between the resting potential and the threshold. Since the threshold is already used as the bias parameter, the \gls{dac}-value of the resting potential will be calibrated. As condition for the binary search, the rate of the activation function is measured with no additional input except for the noise sources and is compared to the half of the maximum rate.

During the calibration of the resting potential with respect to a centered inflection point of the activation function, the positioning of the sigmoid only depends on the relative potential difference $\delta V$. The choice of a work point for the absolute threshold and resting potential should in theory be arbitrary, as the whole operating range can be shifted. On analog hardware, this does not hold true, since some circuits exhibit limited operating ranges. As a consequence the work point is fixed by a threshold potential of \SI{0.54}{\V}.

%In a practical observation, the center of the sigmoid activation function could be placed at zero input by balancing a fixed position of the reset potential with a minor correction of the bias. This was achieved by formulating the condition of the binary search for the bias related parameter in a way that at zero input the activation function returns half the maximum rate. For a higher output rate, the bias had to be reduced and vice versa. On chip, a change of the bias is applied by adapting the \gls{dac} value of the resting potential while keeping the threshold fixed at a chosen work point of approximately \SI{0.54}{\V}.
%The choice of a work point for the absolute threshold and resting potential should in theory be arbitrary, as the whole operating range can be shifted. On analog hardware, this does not hold true, since some circuits exhibit limited operating ranges.

In the context of analog circuits it is very unlikely to find the exact target \gls{dac}-value due to a temporal variability during the measurements. However, the algorithm continuously approximates the searched value under the chosen condition and serves its purpose well. Furthermore, the calibration of the refractory period and the resting potential are repeated and interleaved several times. This is done to reduce the influence of the other yet uncalibrated parameter. In the final calibration state, the activation functions align well (see \cref{transferfunction_w_calib}).

\begin{figure}[tb!]
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\input{figures/single_calibrated_transfer_function_w_various_weights.pgf}
		\label{dlsactivationfunctionweight}
	\end{subfigure}	
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\input{figures/activation_function_w_bias.pgf}
		\label{dlsactivationfunctionbias}
	\end{subfigure}
	\caption[Changing the shape of the sigmoid activation function on \gls{dls}.]{Changing the shape of the sigmoid activation function on \gls{dls}. \textbf{(\subref{dlsactivationfunctionweight})}: The synaptic weight of the input is direct proportionality to the synaptic input current $\gls{isyn} \propto w \nu_\text{in}$ and effects the slope of the activation function. \textbf{(\subref{dlsactivationfunctionbias})}: The activation function can be shifted along the x-axis by changing the threshold. The dotted lines represent a higher threshold and thus a negative bias whereas the dashed lines correlates to a lower threshold and a positive bias.}
\end{figure}

\paragraph{Synaptic Weights and Bias} As shown in the simulation of the activation function in \cref{ratecoding}, the slope and of the activation function and its alignment along the x-axis can be easily modified. By lowering the synaptic weight of the input the slope of the declines, since the total synaptic input exhibited by the membrane is reduce. A lower synaptic weight can also be interpreted as a stretch of the x-axis in \cref{dlsactivationfunctionweight}.

More importantly, the threshold can be used as the bias. Changing the potential difference $\delta V$ between the threshold and the resting potential is interchangeable with adding a bias $b$ to the synaptic input current of the neuron, c.f. \cref{biasproptothreseq}. In \cref{dlsactivationfunctionbias} the activation function has been shifted in both direction by changing the threshold. Due to the fixed pattern noise, the shifted sigmoids do not align per neuron, but since the threshold parameter is set and trained individually the plasticity rule will correct the offset if necessary.



\section{Experiment Setup on \gls{dls}}
\label{circlesimplementation}
For the classification of the Circles dataset a single hidden layer network is trained on-chip with \glsfirst{sgd}. The \gls{ppu} executes both the forward and a backward pass. This also includes generating the dataset and the input spikes. After a proper initialization routine, the training loop is started: the corresponding input rates of a randomly drawn data point $p(x,y)$ are fed into the network and the resulting output rates are evaluated by the \gls{sgd} routine on the \gls{ppu}. The computed parameter updates are then applied before the next iteration can start.

\paragraph{Forward Pass}

%\begin{wrapfigure}{R}{0.35\textwidth}
%	\centering
%	\vspace{-1cm}
%	\input{dlsv2overview.tex}
%	\caption[Overview of the synapse array.]{Overview of the synapse array. Figure adapted from \cite{billaudelle2019versatile}.} 
%	\label{synapsearraysketcha}
%	\vspace{-1cm}
%\end{wrapfigure}
%The neuromorphic core of the \gls{dls} provides 32 analog neuron circuits which are interconnected by synapse array of $32 \times 32$ synapses (c.f. \cref{synapseschematics}). Any input spike to the network is injected row-wise by dedicated synapse drivers as either excitatory or inhibitory input current. At a column, the label of an incoming spike is compared with local address field and in case of a match the spike is relayed down to the designated neuron.
\begin{wrapfigure}{R}{0.4\textwidth}
	\centering
	\vspace{-0.5cm}
	\includegraphics[width=0.4\textwidth]{figures/overview_circles_synapses3.png}
	\caption[Configuration of the synapse array for the Circles data set.]{Configuration of the synapse array for the Circles data set. } 
	\label{overview_circles_synapses}
	\vspace{-.5cm}
\end{wrapfigure}
As for \glspl{ann}, the synapse must be able to seamlessly evolve from an inhibitory weight to an excitatory weight and vice versa. By design a single synapse cannot alternate between the type of the weight without changing the synapse driver's configuration which is time-consuming and can only be done row-wise. Instead of a single input row, two rows are used for the same input - one in excitatory and one inhibitory mode. In each column only one of the double-synapses is active and corresponds to the current type of the synaptic weight. As the weight changes from excitatory to inhibitory, the excitatory synapse is silenced by setting its weight to zero while the other one is reactivated.

As shown in \cref{overview_circles_synapses} the first layer of the network, the input layer, is connected to the hidden units by two rows per $x$ and $y$ input over eleven columns. A dedicated spike router injects the generated spikes by the hidden neurons back into the network, with each hidden neuron requiring another double-row. The feedforward connections of the hidden layer are then relayed to a single column over the $11 \times 2$ rows, where the activity is combined for a single output unit. The synaptic weights of the excitatory and inhibitory noise spike trains remains fixed throughout the experiment and require each a single row with twelve columns.

A Poisson spike train generator on the \gls{ppu} provides a stream of random spikes over a certain measurement period. The provided spike resources are split into four branches, two for the excitatory and inhibitory noise source and two for both input units. Each noise branch has to supply twelve neurons with Poisson spike trains. In an attempt to reduce correlated input between the individual neurons the branches are further divided into eight channels and thus only four will share the noise with other ones.

%With the double-row structure, a total of six rows is used for the various inputs: two for the noise generation and four for the two-dimensional input. The recurrent connections of the hidden layer require another $22$ rows. In \cref{network_structure} the exact row assignment is sketched. Other than during the calibration process, where each neuron was calibrated individually, now the noise source needs to provide Poisson spike trains for all neurons in the hidden and output layer. As a compromise the source is divided further into eight channels. Four neurons will share the noise with other ones. The frequency of the noise in both layers is set to $\nu_\text{noise} \approx \SI{1.1}{\kilo \Hz}$ using weights with $w_\text{noise} = \pm 15$. The received input frequency varies for each layer. The input layer provides an input rate of up to $2 \times \SI{500}{\kilo \Hz}$ for each hidden unit. The output frequency of hidden units is limited by the spike counter and measurement period to $\gls{nuout} \approx \SI{111.3}{\kilo \Hz}$, leading to a theoretical maximum of $\gls{nuin}= 11 \times \gls{nuout} \approx 1219.6$ for the output layer. 
%A presynaptic spike train, e.g. generated by the \gls{ppu}, is injected row-wise into the $32\times32$ synapse array. Then, the synapses compare a dedicated address label of the incoming spike with their own. Once they match, the spike is weighted and forwarded to the column-wise connected neurons at the bottom as either inhibitory or excitatory current. Recurrent connections are made available by the use of a spike router, which routes the postsynaptic spike trains generated by a neuron back into the synapse array.
%A single input row cannot provide alternating negative and positive inputs to the same neuron. Changing the synapses configuration simply costs too much time. To ensure a continuous evolution of the weight from negative to positive, two rows are used simultaneously for the same input - one in excitatory and one inhibitory mode. As the weight changes sign, the unused row is set silent. At the start of the experiment the weights are randomly initialized using a uniform distribution limited to $w_{ij} \in [-25, 25]$.
%The initial conditions of the network have been proven to crucial for its learning success. A ``silent" hidden units has a hard to time become active. Adding a weight depending term to the initial zero bias $b_i^\text{init} \propto \sum_j w_{ij}$ establishes a certain initial fire rate and solves the problem for most cases.

\paragraph{Backward Pass}
In the backward pass, the output rates which have been obtained by the forward pass are used to compute the updates for weights and biases according to \glsfirst{sgd}. A fully on-chip implementation of the required computation comes with some compromises, such as a limited precision or not directly accessible weights. In the following, the developed workarounds are presented.

The general-purpose unit of the \gls{ppu} operates with a 32-bit architecture without hardware-support for floating point types. To increase the precision of the parameter updates, the computation is bit-shifted to the left. The calculated results are then stochastically rounded and shifted back right before being applied to the network. Stochastic rounding has been proven to be a viable workaround for deep learning with parameters of limited precision (\citealp{limitedprecisionpaper}). Typical methods of directed rounding to an integer $x$ are rounding up $\lceil x\rceil$, down $\lfloor x\rfloor$ or to the nearest neighbor $\lceil\frac{\lfloor2x\rfloor}{2}\rceil$. Stochastic rounding on the other hand rounds $x$ with a probability $p$ corresponding to the proximity to the upper or lower neighbor
\begin{equation*}
x_\text{stoch}(x) = 
\begin{cases}
\lfloor x \rfloor \quad \quad &\text{with } p = 1 - (x - \lfloor x \rfloor), \\
\lfloor x \rfloor + 1 &\text{with } p = 1 - \lfloor x \rfloor. \\
\end{cases}
\end{equation*}

Unfortunately a direct access to the analog weight parameters from the general-purpose unit is time-consuming and computation intensive due to the lack of parallelizations. With feedback alignment, the need of the current weight information becomes irrelevant to the plasticity rule, but the computed weight update still needs to be applied to the network. This can be implemented by using the vector unit. The \gls{simd} instructions allow parallel access to all weights and therefore it is not only used to efficiently apply the weight updates but also to silence and reactivated the respective synapses in the double-row structure of the synapse array. The \emph{assembly} code-base for this implementation has been co-written by Sebastian Billaudelle and Benjamin Cramer.

During training the weights can max out due to their limited \SI{6}{\bit} range. The implementation of a simple weight regularization using the convenient parallel access of the vector unit solves this issue. The regularization of both, excitatory and inhibitory weights $w_{ij}^\text{inh},\; w_{ij}^\text{exc}\equiv w_{ij} \in [0,2^6)$ is implemented using bit shifts to minimize the implementation effort and is given by
\begin{equation*}
\text{Reg}(w_{ij}) = w_{ij} - \left\lfloor \frac{\lfloor 2 \cdot w_{ij}  \rfloor}{2^{6}} \right\rfloor.					
%	&= w_{ij} - \left\lfloor \lfloor 2 w_{ij} \rfloor \cdot 2^{-6} \right\rfloor
\end{equation*}
The impact of the regularization is then fine tuned by applying it only with a certain probability $p$.

The bias is set by lowering or raising the \gls{dac}-value of the threshold potential. The resolution of the capacitive parameter memory equals roughly \SI{1.7}{\milli \V}, which in turn translates to a change of the input frequency of about \SI{9.9}{\kilo \Hz}. As a comparison the maximum input frequency of a neuron is around \SI{1}{\mega \Hz}, but the magnitude of the typical presynaptic activity will only be at a few \SI{100}{\kilo \Hz}. Hence, the bias has a rather coarse resolution.

However, compared to the speed of the accelerated neuromorphic core, the \acrlong{dac} is slow. Instead of a rapid change, a continuous and slow implementation can reduce potential negative effects of the coarse bias resolution, especially in combination with a low learning rate.

\paragraph{Initial Conditions and Hyperparameters}
The final experiment setup involves the setting of initial conditions, neuron parameters and the tuning of hyper-parameters. The latter describe a set of parameters, that is not changed during the training process but their choice often determines whether or not the training succeeds (\citealp{Goodfellow-et-al-2016}). A subset of the most relevant parameters is listed in \cref{circlesinitparameters}.

\begin{table}[t!]\centering\ra{1.3}
	\begin{tabular}{@{}rlll@{}}\toprule
		& Parameter								& 	hidden layer 			& 	output layer 	\\ \midrule
		& initial weights $w_{ij}^\text{init}$	& 	 $\in[-25, 25]$	LSB		&	$\in[-25, 25]$ LSB	\\
		& initial bias $b_{i}^\text{init}$		& $\propto \frac{1}{2} \sum_j w_{ij}$ &	0		\\
		& learning rate of bias  $\eta_\text{w}$&	$165e4$					& 	$5.5e4$			\\
		& learning rate of bias  $\eta_\text{b}$&	$2.3e4$					& 	$0.23e4$		\\
		& resting potential \gls{v_leak}		&	$\SI{0.54}{\V}$ 		& 	$\SI{0.54}{\V}$	\\
		& reset potential \gls{v_reset}			&	$\SI{0.01}{\V}$			& 	$\SI{0.01}{\V}$	\\
		& refractory period \gls{refrac}		&	$\SI{9}{\micro \s}$			& 	$\SI{0.01}{\V}$	\\
		& range of threshold \gls{thres} 				&	\SIrange{0.45}{0.63}{\V}&	\SIrange{0.45}{0.63}{\V}\\
		& max input frequency $\nu_{\text{in}}$	&	$2 \times \SI{500}{\kilo \Hz}$	& $11 \times \SI{111.3}{\kilo\Hz}$	\\
		\bottomrule
	\end{tabular}
	\caption[Initial, hyper and neuron parameters per layer.]{Initial, hyper and neuron parameters per layer. Some parameters require a different setting for the hidden and the output layer.}
	\label{circlesinitparameters}
\end{table}

At the beginning of the experiment all weights of both layers are randomly initialized using a uniform distribution ranging from $-25$ to $25$ LSB. A random initialization of the weights is vital to the training performance (\citealp{Goodfellow-et-al-2016}). In a worst case scenario, the initialization of the weights can even prohibit the network from learning at all.

Initially, the biases of both layers are set to zero, i.e. $\gls{thres} = \gls{v_leak}$. With this setting it has been observed that some hidden units are ``silent" at the start of the training. Furthermore, the silent units remained inactive throughout the training process. As a workaround, a weight dependent term is added to the zero bias, such that $b_i^\text{init} \propto \sum_j w_{ij}$. This ensures an initial firing rate of every neuron at the beginning of the training and solved the issue of silently initialized hidden units.

During the tuning of the network's hyperparameters, the slope of the activation function had to be raised by a fair amount to increase the contrast of the nodes. By a steeper sigmoid, the network can create a more precise decision boundary than by a flat slope. The slope is changed by reducing the frequency of the noise spike train $\nu_\text{noise}$ to \SI{1.1}{\kilo \Hz}. %The impact of the noise spikes is further tuned by the synaptic weight $w_\text{noise}$ which is set to $15$ LSB for the excitatory and inhibitory noise sources.

A further increase of the slope of the activation function is achieved by increasing the dynamic range of the input rates. With frequencies ranging from \SIrange{0}{500}{\kilo \Hz} almost all available resources are exploited. The maximum input rate of a hidden unit is then given by the combined rate of the x and y input. In turn, each hidden unit produces an output rate of up to \SI{111.3}{\kilo \Hz}, generating a potential maximum input of $11 \times \SI{111.3}{\kilo \Hz}$ for the readout unit in the output layer.

The learning rates as chosen in \cref{circlesinitparameters} may appear unusually high. This is mainly due to a lack of normalization during the computation of the parameter updates. In order to avoid the risk of eliminating small contributions of variables during the on-chip computation of the parameter updates, the magnitude of the individual variables has not been normalized in advance. An individual learning rate per layer and parameter copes with the different magnitudes later on. Even without the necessity to norm resulting parameter updates, it is not unusual to tune learning rates separately, since different network layouts will cause varying update velocities for the individual network parameters (\citealp{Goodfellow-et-al-2016}). The final choice of the learning rates is then tuned such that a fair amount of learning activity is provided for both layers and parameters. In the appendix in \cref{monitoringplots} a detailed time evolution of the weight and bias updates per layer is shown.

\section{Training}
For a fully on-chip implementation of experiment the required input data for the training data set needs to be generated on the fly, since there is only limited memory on-chip available. In this way, each forward pass is performed with a randomly generated data point from either of the two rings from \cref{circlestask}. The backward pass is performed directly after the first training sample, i.e. the minibatch size is one.

\begin{figure}
	\begin{center}
		\input{figures/circles_learning_performance.pgf}
	\end{center}
	\caption[Training success of the Circles Classification.]{Training success of the Circles Classification. The performance is monitored by the accuracy (a) and the \acrfull{rmse} (b) over 2500 iterations.}
	\label{circles_acc}
\end{figure}

In the following the network is trained for 2500 iterations. For a better illustration of the training process, a balanced \emph{validation} dataset with $100$ randomly drawn data points per class is chosen to validate the current state of the network after every fifth iteration. The limitation to every fifth step was chosen to reduce the measurement time to a few hours and to cope with the increasing instability of the \gls{fpga} during long measurements. To measure the performance of the training process the \gls{rmse} and the accuracy are evaluated. The \gls{rmse} is given by
\begin{equation}
\text{\gls{rmse}} = \sqrt{\frac{\sum_p e^{(o)}(p)^2}{n_\text{points}}},
\end{equation}
with the error $e^{(o)}$ from \cref{circleserror} and the number of data points $n_\text{points}$ in the validation dataset and the accuracy is
\begin{equation}
\text{Accuracy} = \frac{n_\text{true}}{n_\text{points}},
\end{equation}
with the number of correctly classified points $n_\text{true}$. Whether or not a point is correctly identified is determined by decision boundary which was introduced in \cref{circlesdb}.

%\begin{figure}[htb!]
%	\centering
%	\include{learning_process_s5}
%	\vspace{-0.3in}
%	\caption[Initial state of the deep network.]{Initial state of the deep network. For simplicity only the hidden and output layer are displayed. The arrows connecting the hidden units with output units resemble the synaptic connections, with red being an excitatory synapse and the thickness of the arrow corresponding to the synaptic strength.}
%	%\label{learning_process_s5}
%\end{figure}

\begin{figure}[t!]
	\begin{subfigure}{\textwidth}
		\caption{}
		\vspace{-0.1in}
		\centering
		\include{learning_process_s5}
		\label{learning_process_s5}
		\vspace{-.5in}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\caption{}
		\vspace{-0.1in}
		\centering
		\include{learning_process_s500}
		\label{learning_process_s500}
		\vspace{-.5in}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\caption{}
		\vspace{-0.1in}
		\centering
		\include{learning_process_s2500}
		\label{learning_process_s2500}
	\end{subfigure}
	\vspace{-.4in}
	\caption[Evolution of the network's ability to separate the Circles data set.]{Evolution of the network's ability to separate the Circles data set. For simplicity only the hidden and output layer are displayed. The connecting arrows resemble the excitatory (\emph{red}) and inhibitory (\emph{blue}) synaptic connections and their thickness corresponds to the synaptic strength. \textbf{(\subref{learning_process_s5})} The initial untrained state of the network does not separate the inner and outer circle. \textbf{(\subref{learning_process_s500})} After 500 iterations the first improvements can be seen. With a decision boundary of roughly \SI{60}{\kilo \Hz}, the output unit starts to correctly identify the two classes. \textbf{(\subref{learning_process_s2500})} After 2500 iterations the network has succeed to separate both classes with an accuracy of almost 100 \%.}
\end{figure}

At the beginning of the training process, each hidden unit is stimulated with a linear combination of the inputs from \cref{circlesinputs}. The activation function then transforms the linear input into a non-linear output. The new representations of the input layer are again linearly combined into a net input for the output unit where another non-linear transformation is applied. The initial state of the network is shown in \cref{learning_process_s5}. At this stage of the training process the output unit cannot classify the task correctly.

%\begin{figure}
%	\label{learning_process_s500}
%	\include{learning_process_s500}
%	\caption[Network state after 500 iterations.]{Network state after 500 iterations. With a decision boundary of roughly \SI{60}{\kilo \Hz}, the output unit starts to correctly identify the two classes.}
%\end{figure}

The evolved network after 500 iterations is depicted in \cref{learning_process_s500} and shows first signs of improvement. The \gls{rmse} has already been reduced to half of its initial error and the accuracy has improved to roughly \SI{80}{\%} as shown in \cref{circles_acc}. Less useful representation in the hidden layer are further modified by changing their bias or the connected weights. The beneficial ones are rewarded with a stronger connection to the output layer and further refinement of their bias and input weights.

At 2500 iterations, see \cref{learning_process_s2500}, the network has converged. The accuracy has also stabilized at almost 100 \% (see \cref{circles_acc}). The average over the last 500 iterations yields a mean accuracy of $(98.2 \pm 2.9)\,\%$ and the average over the last 250 iterations improves to  $(99.6 \pm 0.8)\,\%$. The \gls{rmse} is also reduced to a minimum, i.e. the output neuron resembles the target well. The average output error over the last 250 iterations yields $(9.6 \pm 1.4)\,\si{\kilo \Hz}$. To provide a better understanding of how the network evolves to improve these performance measures, the evolution and contribution of the first hidden unit is discussed in more detail.

The random initial weights for the first hidden unit create a less beneficial representation of the input where more than half of the data points are yield a high output rate (\cref{learning_process_s5}). However, the initial contribution of the unit to the output unit is small. Over the next hundred iterations, the weights are adjusted such that the contour lines of the output rates are rotate clockwise by $45^\circ$ (\cref{learning_process_s500}). More importantly, the bias is reduced and removes any high output rates from the inner circle. The more useful representation is then rewarded with a stronger contribution to the readout unit. Until the final iteration, the nature of the representation does not change much anymore but the parameters are still fine-tuned (\cref{learning_process_s2500}). The bias is slightly increased such that a greater portion of the outer circles fires with at a high rate and the orientation of the contour lines is slightly rotated back. Furthermore, the contribution of the hidden unit is further fostered by the network.


%For further monitoring purposes, the evolution of the weights and biases are tracked for the hidden and output layer (see \cref{network_monitoring}).
%\begin{figure}
%	\centering
%    \input{figures/network_evolution_circles.pgf}
%	\caption{Describe what happens in the picture...}
%	\label{network_monitoring}
%\end{figure}