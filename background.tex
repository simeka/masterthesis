\glsreset{ann}
\glsreset{snn}
%\glsreset{ann}

\chapter{Background}
%Starting my final year project at the Electronic Vision(s) group, I soon realized the diversity of their research. 
The required knowledge to work in the field of neuromorphic computing is broad and manifold, ranging from the biological view of the human brain further to machine learning algorithms. In the next sections, the main concepts and the physical background, upon which the presented research is based on is, are introduced. Starting with deep learning and an overview of the biological neuron, the transition to neuron models, neuronal coding schemes and their training approaches will be given before introducing the neuromorphic \gls{bss2} platform. Throughout the thesis, vectors are indicated by an upright boldface $\mathbf{v}$ and matrices by capital letters $M$.
\section{Deep Learning}
\label{deeplearning}
The presented introduction to deep learning in this section is largely oriented on the well-known eponymous book from \cite{Goodfellow-et-al-2016} and indicated respectively if otherwise.

Deep learning is among the most useful and powerful tools machine learning has provided to the scientific community. Image or pattern recognition are in general hard to solve for traditional computation concepts. Deep learning abstracts such task in terms of a hierarchy of concepts. Each concept is based upon a combination of simpler ones. Going down on a hypothetical ladder towards the easiest concept available, creates a deep structure with many layers. This is why it is called deep learning.

A popular example for deep learning is the \gls{mlp}, a deep feedforward network. As the name suggests, the information is forwarded from one layer to another (see \cref{multilayernetwork}). At each layer $l$ the input $\mathbf{x}^{(l)}$ is mapped to an output $\mathbf{y}^{(l)} = \gls{activation}(\mathbf{x}^{(l)}, \mathbf{\theta}^{(l)})$ with the activation function \gls{activation} and a set of parameters $\mathbf{\theta}^{(l)}$. The output of the layer $l$ then determines the input of the next layer, i.e. $\mathbf{y}^{(l)} = \mathbf{x}^{(l+1)}$, and so forth. The layer structure is inspired by biological neural networks and therefore they are often referred to as \glspl{ann}. 

An \gls{ann} is trained by adapting the parameters $\mathbf{\theta}^{(l)}$ for all layers $l$ following a training algorithm. In machine learning, one discriminates between supervised, and unsupervised learning algorithms. Yet, drawing a consequent line to categorize machine learning methods is difficult, as the approaches are sometimes combined into hybrid forms too. Without a supervisor, an algorithm looks for structures and useful properties within the input data. To this end, unsupervised learning algorithms try to observe the underlying probability distribution of the input data. In supervised learning, on the other hand, each input vector $x$ is associated with a target vector $\textbf{y}^*$. In this case, the algorithm is trained to predict a target for a given input.


%Given the task to identify a picture of cat, any computer will have a hard time to map the essential information of the camera's sensor data to the class \textit{cat}. An \gls{ann} solves the problem by learning how to describe the raw data in terms of simpler representations and essentially by combining these simpler representations into a meaningful solution in the last layer - the output layer. The first layer is called input layer and all other layers in between are named hidden layers, since they are usually not visible from the outside.\\


\subsection{Supervised Training}
\label{supervisedtraining}
%\begin{wrapfigure}{R}{0.45\textwidth}
\begin{figure}
	\centering
	\input{figures/single_layer_network.tex}
	\caption[Deep artifical neural network]{Deep artifical neural network. An \acrfull{ann} can n hidden layers will always have an input $\mathbf{x}^{(1)})$ and one output layer $y^{(o)}$. The sizes of the input and the individual layers can vary depending on a chosen problem.} 
	\label{multilayernetwork}
\end{figure}
%\end{wrapfigure}
The supervised training of an \gls{ann} can be divided into a \emph{forward pass} where the output of all nodes is evaluated and a \emph{backward pass} which computes the respective parameter updates. In the \textbf{forward pass}, the input $\mathbf{x}^{(l)}$ of a layer $l$ sums to a net input $\mathbf{a}^{(l)}$ of
\begin{align*}
\mathbf{a}^{(l)} = W^{(l)} \, \mathbf{x}^{(l)} + \mathbf{b}^{(l)} + \left(\text{noise}\right), 
\end{align*}
with the weight matrix $W^{(l)}$, the bias $\mathbf{b}^{(l)}$ and an optional noise term. Depending on the task, the bias as well as the optional noise term can be vital: the individual biases for instance allow the network to adjust the dynamic range of each neuron and the injection of artificial noise can significantly increase the training performance (\citealp{an1996effects}).

The output $\mathbf{y}^{(l)}$ of the layer $l$ is then given by the activation function $\phi$, e.g. a sigmoid
\begin{equation*}
\mathbf{y}^{(l)} = \phi(\mathbf{a}^{(l)}) = \frac{1}{1 + e^{(-\beta \mathbf{a}^{(l)})}},
\end{equation*}
with the slope parameter $\beta$. Popular choices for the activation function besides a sigmoid are a \gls{relu} or the hyperbolic tangent ($\tanh$). In \cref{dltransfer} the various activation functions are shown for comparison. However, more important than the exact shape is that the chosen function is non-linear. A linear activation function makes any layer structure redundant, as the composition of linear linear functions yields again a linear function and therefore all layers of a deep network could be merged into a single one.

The same principle is then applied to all other layers to complete the forward pass, i.e. the result of the previous layer is the input for the current layer. The weight matrix $W^{\text{(l)}}$ connecting layer $l$ with $l-1$ has the appropriate shape to fit the number of input nodes $n^{\text{(l-1)}}_\text{nodes}$ and output nodes $n^{\text{(l)}}_\text{nodes}$.

\begin{figure}
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\inputpgf{figures}{deeplearning_activation_functions.pgf}
		\label{dltransfer}
	\end{subfigure}
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\inputpgf{figures}{deeplearning_activation_functions_derivative.pgf}	
		\label{dltransfergradient}
	\end{subfigure}
	\caption[Popular shapes for activation functions in deep learning.]{Popular shapes for activation functions in deep learning. \textbf{(\subref{dltransfer})}: Some of the most popular shapes for the activation function are a \gls{relu}, $\tanh$ or sigmoid. \textbf{(\subref{dltransfergradient})}: The derivative of the different activation function gives an insight how the impact of gradient descent changes for different functions. In the case of the sigmoid and the hyperbolic tangent, either a very high or low input leads to a zero gradient and thus to a vanishing parameter update.}
	\label{deeplearning_activation_functions}
\end{figure}

In deep learning, \emph{gradient descent} is probably the most popular algorithm to perform the \textbf{backward pass} on a network. A differentiable loss function $\loss(\mathbf{\mathbf{x}, y^*, \theta})$ for a given target $\textbf{y}^*$ is minimized. Here, a binary cross-entropy loss is chosen
\begin{equation*}
\mathcal{L} = - \frac{1}{N} \sum_{i=1}^N \textbf{y}^* \log(\textbf{y}_i + (1-\textbf{y}^*) \log(1 - \textbf{y}_i), 
\end{equation*}
with a minibatch of size $N$ and the target $\textbf{y}^*$ scaled between 0 and 1. In combination with a sigmoid-shaped activation function, this choice becomes convenient when computing the new set of parameters $\mathbf{\theta}'$. This is done by moving along the negative gradient of the loss with respect to the network's parameters $\mathbf{\theta}$
\begin{equation}
\mathbf{\theta'} = \mathbf{\theta} - \eta \, \nabla_\theta\loss(\mathbf{\mathbf{x}, y^*, \mathbf{\theta}}),
\label{stochasticgradientdescent}
\end{equation}
with the learning rate $\eta$. To avoid extensive computational costs, the gradient is estimated by a uniformly drawn subset of the full training data set, a \emph{minibatch}. In particular, if the size of the minibatch equals one, one speaks of \gls{sgd}. As an example, the updates of the weight matrices of a single hidden layer network are computed in the next paragraphs using \gls{sgd}.

First, the derivative of the cross entropy loss function in the output layer $l\equiv o$ is computed
\begin{equation*}
\frac{\partial\mathcal{L}}{\partial \mathbf{y}^{(o)}} = 
- \frac{\mathbf{y}^*}{\mathbf{y}^{(o)}} + 
\frac{1 - \mathbf{y}^*}{1 - \mathbf{y}^{(o)}},
\end{equation*}
The gradient of the loss can then be rewritten in terms of the error $\mathbf{e}^{(o)} = \mathbf{y}^* - \mathbf{y}^{(o)}$ by using the derivative of the activation function
\begin{align*}
\frac{\partial \mathbf{y}^{(o)}}{\partial \mathbf{a}^{(o)}} = \frac{\partial \gls{activation}(\mathbf{a}^{(o)})}{\partial \mathbf{a}^{(o)}} = \gls{activation} (1 - \gls{activation}),\\
\Rightarrow \quad \frac{\partial\mathcal{L}}{\partial \mathbf{a}^{(o)}} =
\frac{\partial\mathcal{L}}{\partial \mathbf{y}^{(o)}} 
\; \frac{\partial \mathbf{y}^{(o)}}{\partial \mathbf{a}^{(o)}} =
\mathbf{y}^* - \mathbf{y}^{(o)} = \mathbf{e}^{(o)}.
\end{align*}

According to \cref{stochasticgradientdescent}, the final update of the weight matrix is given by
\begin{equation}
\delta W^{(o)} = - \eta \frac{\partial \mathcal{L}}{\partial W^{(o)}} 
= - \eta \;
\frac{\partial\mathcal{L}}{\partial \mathbf{y}^{(o)}} \;
\frac{\partial \mathbf{a}^{(o)}}{\partial W^{(o)}}
= - \eta \, \left(\mathbf{e}^{(o)} \mathbf{x}^{(o),T}\right),
\label{backpropupdate}
\end{equation}
with the transpose of the input $x^{(o),T}$.

The computation for the hidden layer ($l\equiv h$) can be done in a similar fashion. Again, the gradient of the loss function is computed
\begin{equation*}
\frac{\partial\mathcal{L}}{\partial \mathbf{a}^{(h)}} = \mathbf{e}^{(h)} \;
\frac{\partial \mathbf{y}^{(h)} }{\partial \mathbf{a}^{(h)}}.
\end{equation*}
The error of the output layer $\mathbf{e}^{(o)}$ is propagated backwards with the transpose of the weight matrix $W^{(o),T}$ as $\mathbf{e}^{(h)}=W^{(o),T}\mathbf{e}^{(o)}$ yielding a total update of
\begin{equation*}
\delta W^{\text{(h)}} = - \eta \;
\left(W^{\text{(o)}T} \mathbf{e}^{(o)}\right) \;
\frac{\partial \mathbf{y}^{(h)} }{\partial \mathbf{a}^{(h)}} \; \mathbf{x}^{(h), T}.
\end{equation*}

The backward propagation of the error is eponymous for the method's name \textit{backpropagation}. Despite the great performance for many deep learning tasks, the biological plausibility of propagating the error signal backward has been questioned ever since (\citealp{grossberg1987competitive}). A simple but effective adjustment was suggested by \cite{lillicrap2016random} which is also known as \textit{feedback alignment}. Instead of the transpose of the feedforward weight matrix, a fixed random matrix $B$ is chosen to propagate the error backwards. Compared to the backpropagation variant from \cref{backpropupdate} the update in the hidden layer changes to
\begin{equation*}
\delta W^{(h)} = - \eta \;
(B \mathbf{e}^{(o)}) \;
\frac{\partial \mathbf{y}^{(h)}}{\partial \mathbf{a}^{(h)}} \;
\mathbf{x}^{(h),T}.
\end{equation*}
The only constraint to $B$ is that $\mathbf{e}^{(o)T} W^{(o)} B \mathbf{e}^{(o)} > 0$ has to be fulfilled on average, meaning that geometrically, the new feedback signal $B \mathbf{e}^{(o)}$ for the hidden layer lies within $90^{\circ}$ of the one used by backpropagation, $W^{\text{(o)}T} \mathbf{e}^{(o)}$.

%Q:Überleitung zu biologischem neuron hier?

\section{The Biological Neuron}

Biological neural networks have been a great inspiration for deep learning algorithms and \glspl{ann}. It is estimated that the human brain contains around $10^{11}$ neurons of different shape, size and functions (\citealp{numberofneurons}). By the use of \emph{synapses}, \emph{presynaptic} neurons connect to \emph{postsynaptic} ones and thereby create complex network structures throughout the brain. A postsynaptic neuron has usually around $10^4$ presynaptic partners (\citealp{numberofsynapses}). The connections vary from dense clusters with nearby neurons to linking distant brain regions with each other. 

Most neurons can be divided into three functional parts: \emph{dendrites}, \emph{soma} and \emph{axon} (c.f. \cref{biologicalneuron}). At the dendrites a postsynaptic neuron receives inputs from its presynaptic partners. These inputs are converted into \glspl{psp} using charged molecules corresponding to a variable graded potential in size and shape. The \glspl{psp} are then relayed to the soma where they are integrated over. Depending on the nature of synaptic connection, the stimulation has either an excitatory or inhibitory effect on the neuron, resulting in an increased or decreased membrane potential. Over time the unbalanced ion concentration in and outside the membrane is restored by the membranes permeability and additional ion pumps. In an equilibrated state the membrane potential is referred to as the \emph{resting potential}. (\citealp{gerstner2014dynamics})

Given a continuous excitatory stimulation current, the neuron will trigger a fire mechanism once a certain threshold potential is reached. The neuron's membrane becomes hyperpolarized and decreases even below the resting potential as shown in \cref{actionpotential} entering a \emph{refractory period}. During this period it remains hard but not impossible for the neuron to fire again due to the ongoing hyperpolarization of the membrane. This mechanism releases an electrochemical pulse a so-called action potential or spike, which is then relayed by the axon to the connected partners at the end of the axon. (\citealp{gerstner2014dynamics})

\begin{figure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\caption{}
		\vspace{0.75cm}
		\includegraphics[width=\linewidth, valign=t]{figures/Neuron.pdf}
		\vspace{1.25cm}	
		\label{biosynapse}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\caption{}
		\includegraphics[width=0.8\linewidth, valign=t]{figures/action_potential.png}
		\label{actionpotential}
	\end{subfigure}
	\caption[Schematics of a biological neuron and an action potential]{Schematics of a biological neuron and an action potential. \textbf{(\subref{biosynapse})} A biological neuron can be split into three main functional parts. The dendrites collect the inputs from presynaptic partners and relay them in the form of \glsreset{psp}\glspl{psp} to the soma. The soma integrates the \glspl{psp} and eventually triggers a fire response. The axon relays this response to the connected neurons at the end of the axon. Figure adapted from \citealp{picture_neuron}. \textbf{(\subref{actionpotential})} The fire response of a neuron after exceeding the threshold is called action potential or spike. After a phase of depolarization, the membrane repolarizes and enters the refractory periode during which it is hard but not impossible to fire again. Figure taken from \citealp{picture_actionpotential}.}
	\label{biologicalneuron}
	
\end{figure}

The action potential releases various neurotransmitters to overcome a small but physical gap at the synapses, the synaptic cleft. Once a transmitter has docked to a corresponding receptor on the other side, activated ion channels convert the chemical transmission back into an electrical signal resembling the \gls{psp}. Depending on the type of neuro\-transmitters the excitation can be excitatory or inhibitory (\citealp{gerstner2014dynamics}). According to Dale's principle a presynaptic neuron always releases the same type of neurotransmitter (\citealp{dale1935pharmacology}). To that end, a neuron's output is either excitatory or inhibitory but not both. The input of a neuron, on the other side, is not restricted to a single type of excitation, as various presynaptic partners can be connected. 

A wide-spread assumption in the field of neuroscience is that the exact shape of a spike doesn't carry any relevant information and therefore all spikes can be modeled by a stereotypical shape. However, recent research has already suggested that the small variations in the action potential contain vital information (\citealp{debanne2013mechanisms}). For the prior assumption, the communication between neurons is reduced to a temporal and spatial dimension. Temporal information is encoded either in the frequency (\emph{rate coding}) or the precise timing (\emph{time coding}) of spikes whereas the spatial dimension is filled with varying populations of neurons (\citealp{gerstner2014dynamics}). A more detailed description of neural coding schemes is presented in section \ref{neuralcoding}.

The brain has the ability to continuously change the topology of its synaptic wiring, to create new synapses, to alter the chemical properties of the synaptic receptors or to simply strengthen and weaken the synaptic efficacy. With these \emph{plasticity} mechanisms the brain is able to learn and adapt as a reaction to stimulation or even damage. (\citealp{gerstner2014dynamics})

One way of learning and forming memory is known as synaptic plasticity where the synaptic strength is changed over time. According to Hebb's theory ``neurons that fire together wire together" (\citealp{hebb1949organization}). An experimental proof of such activity-dependent plasticity was found by \cite{bliss1973long}, where they discovered that a short but high frequency stimulation leads to a long lasting change in the synapse's efficacy. This is also referred to as \gls{ltp}. Reducing the stimulus to a low frequency, on the other hand, resulted in the opposite effect: \gls{ltd}. In combination they can carve out a certain region in the brain which is related to a specific stimulus and thereby create memory (\citealp{nabavi2014engineering}). A better understanding of \gls{ltp} and \gls{ltd} was gained when \gls{stdp} was first observed (\citealp{markram1997regulation}, \citealp{poo98stdp}). \gls{stdp} shows in principle that presynaptic activity just before a postsynaptic response leads to an increased synaptic strength. If presynaptic activity occurs right after a postsynaptic spike, the synapses are weakened. 

Such plasticity mechanism are self-regulating and independent of any reward-giving structure. In analogy to the learning concepts introduced by deep learning, they are also referred to as unsupervised Hebbian learning. However, these methods fall short of successfully training the brain to master complex action-required tasks, as the outcome of a decision made by the brain is not part of the plasticity mechanism. Hence, a behavioral learning strategy requires knowledge over the outcome of taken decisions and needs to remember which decisions have lead to rewards, as the reward in real life is often delayed. (\citealp{gerstner2014dynamics})

Developing biologically inspired and plausible supervised learning algorithms is a dedicated goal in the field of modern neuroscience. Before discussing two such candidates in more detail, a practicable model of the biological neuron is presented first. This model is implemented in the analog core of the neuromorphic platform \gls{bss2} and resembles the basis of all experimental work presented in this thesis.

\section{The \acrlong{lif} Model}
\label{lifmodel}

An early but successful description of the biological neuron dynamics was accomplished by the \glsfirst{lif} neuron model, first described by \cite{lapicque1907recherches}. Despite some strong simplifications, the main dynamics of the membrane potential are well described by the model and it thus has been a popular and portable choice for neuromorphic hardware implementations.

In biology, the observation of similar shaped individual action potentials lead to the assumption that the shape of a spike does not transport any information. The \gls{lif} model is based upon this theory and thus every spike can be replaced by a stereotypical shape (\citealp{gerstner2014dynamics}). 

Another observation in biology is that neurons vary much in their shape and size fulfilling different functions. The spatial component plays an important role for the dynamics of a neuron. For instance, the strategic positioning of certain excitatory or inhibitory inputs on the dendrites, either closer or farther away from the soma, give rise to non-linear behavior in the course of the membrane potential. However, extensive spatial dependencies are difficult and costly to implement in a model. Therefore, the \gls{lif} neuron neglects the topology of the neuron and is approximated as a point-like integrator. (\citealp{gerstner2014dynamics})

In the model, the incoming spike train $S_j(t)$ from various presynaptic partners $j$ is described by a series of spikes $s$ at times $t_j^{(s)}$
\begin{equation*}
S_j(t) = \sum_s \delta(t - t_j^{(s)}),
\end{equation*}
with the $\delta$-function denoted as $\delta$. 

Each spike of the input spike train evokes a \gls{psp}. The impact of the \glspl{psp} depends on the individual synaptic weights $w_j$. For simplicity, the excitatory or inhibitory nature of the synapses is encoded by a sign in the synaptic weight as well. Summing over all input sources yields a total synaptic input current that is seen by the postsynaptic neuron
\begin{equation}
\gls{isyn}(t) = \sum_j w_j \left(\epsilon \ast S_j(t)\right),
\label{synaptic_input}
\end{equation}
with the convolution of a double exponential kernel $\epsilon$ with the input spike train $\epsilon \ast S_j$ describing the shape of a single \gls{psp}. The kernel can be defined as
\begin{align*}
\epsilon_\text{double}(t) 	&=\frac{1}{\mathcal{N}} \left(\epsilon_\text{rise} \ast \epsilon_\text{fall}\right)(t) \\
&=\frac{1}{\mathcal{N}}\exp \left(-\frac{t}{\tau_\text{rise}} \right)  \ast \exp \left(-\frac{t}{\tau_\text{fall}} \right), 
%\label{exponentialkernels)}
\end{align*}
with a rising and falling time constant $\tau_\text{rise}$ and $\tau_\text{fall}$ respectively. A constant $\mathcal{N}$ norms the kernel to unity. As the rising constant goes to zero $\tau_\text{rise} \rightarrow 0$ the double exponential turns into a single exponential kernel $\epsilon_\text{single} = \epsilon_\text{fall}$.

The membrane potential \gls{v_mem} changes with the continuous synaptic input causing an unbalanced ion concentration inside the membrane. Passive as well as active processes are permanently restoring the membrane potential back to its equilibrium state which is associated with the resting potential \gls{v_leak}. In the \gls{lif} model, the temporal scale of these restoring processes is defined by the membranes capacitance $C_\text{m}$ and the leakage conductance $g_\text{leak}$ yielding the membrane's time constant $\gls{tau_m} = \frac{C_\text{m}}{g_\text{leak}}$. The dynamics of the membrane are then given by a single differential equation
\begin{align}
\label{lifeq}
C_{\text{m}} \frac{d\gls{v_mem}}{dt} &= -g_{\text{leak}} (\gls{v_mem} - \gls{v_leak}) + \gls{isyn}.
\end{align}

As for a biological neuron, a \gls{lif} neuron triggers a spike once a certain threshold \gls{thres} is crossed following the condition
\begin{equation*}
\gls{v_mem}\big(t^{(s)}\big) = \gls{thres} \Leftrightarrow \text{neuron fires at time } t^{(s)}.
\end{equation*}

Then the membrane is set to a reset potential \gls{v_reset} where it remains unchanged for a refractory period of \gls{refrac}
\begin{equation*}
\gls{v_mem}(t) = \gls{v_reset} \quad \forall t \in \left(t^{(s)}, t^{(s)} + \gls{refrac}\right].
\end{equation*}
Unlike its biological counterpart, the modeled neuron cannot spike during the refractory period.

\begin{figure}[htb!]
	\centering
	\scalebox{0.93}{\input{figures/lif_adex_dynamics.pgf}}
	\caption[Membrane dynamics of the \gls{lif} and \gls{adex} given a constant input.]{Membrane dynamics of the \gls{lif} and \gls{adex} given a constant input. \textbf{(a)} Evolution of the membrane potential $V_\text{m}^\text{LIF}$ according to the \gls{lif} model in response to a different stimulation currents. The first current is not strong enough to trigger an action potential. A more intense stimulation yields a repetitive and equidistant spiking pattern. \textbf{(b)} Given a small box shaped current, the evolution of the \gls{adex} neuron's potential $V_\text{m}^\text{AdEx}$ is similar to the \gls{lif} model. At higher inputs, a negative adaption suppresses a repetitive spiking pattern after the first spike. The peak resembles the positive exponential voltage feedback simulating an action potential. \textbf{(c)} A box-shaped stimulation current $I_\text{stim}$ is used for both models to show the course of the membrane potential. Figure taken from \citealp{stradmann2019msc}}
	\label{lifvsadex}
\end{figure}

A \gls{lif} neuron doesn't keep track of any previous spikes once a spike is released, given that the time constant of the synaptic input is shorter than the one of the membrane potential, in particular if $\gls{tau_m} > \gls{tau_syn}$. These limitations make it impossible for the model to correctly describe neuronal behavior such as spike bursts (\citealp{gerstner2014dynamics}) and led to a demand of a more detailed modeling as the \glsfirst{adex} model. The \gls{adex} model resembles an extension to the \gls{lif} model featuring an additional adaption state variable that provides post-spike memory to the membrane. Depending on the sign of the adaption, the neuron is either inhibited or engaged to fire again after having spiked at least once (see \cref{lifvsadex}).

%Apart from the additional state variable the \gls{adex} neuron features a positive exponential voltage feedback. on the other hand, enables the neuron to have a more complex behavior in the (\gls{v_mem}, w?) phase space.
For the scope of this thesis a more advanced model is not yet required. All experiments are done using the simpler \gls{lif} model. However, the use of the \gls{adex} model for a spike-based learning rule such as SuperSpike is promising when targeting real-world applications. A respective project has already been planned in the future and will be explained in more detail in \cref{discussionandoutlook}.

A supervised learning algorithm based on a single \gls{lif} neuron will not be able to solve any difficult task yet. In the human brain millions of such neurons are split up into various areas which are each responsible to perform a certain type of work such as smell, speak or motor control. In the next section different ways of how neurons can communicate with each other in order to perform a certain task are discussed.

%(see \cref{lifvsadex}).
\section{Neural Coding with Spiking Neural Networks}
\label{neuralcoding}
\glsreset{snn}
The communication among biological neurons can be modeled by \glspl{snn} where individual neurons convey information by sending spikes to their connected partners. As mentioned before, spikes can be approximated to have a stereotypical shape, leaving the temporal to encode information. In the context of large multilayer networks, the spatial dimension of the network encodes information as well, i.e. the types of synapses, the synaptic strength and the network's topology.

%A neuron's primary way to exchange information with another is to send and receive spikes.
%\Glspl{snn} take the biological inspiration for \glspl{ann} further by conveying and processing information using spikes and neuronal models such as the \gls{lif} neuron. As mentioned before, spikes are assumed to have a stereotypical shape, leaving the temporal and spatial dimension to communicate. In the context of large multilayer networks, the topology itself, i.e. the types of synapses, the synaptic strength, the pre- and postsynaptic partners, encodes information as well.

In the following, the different coding schemes and supervised training approaches for a feedforward multi-layer \glspl{snn} using \gls{lif} neurons are presented.

\subsection{Rate Coding}
\label{ratecoding}
In an attempt to explain computational processes in the brain, the activation of an artificial neuron has been mapped onto the firing rate of a spiking neuron by \cite{rieke1999spikes}. The most apparent way to define a neuron's spike rate $\nu$ is to count the number of spikes $n_\text{spikes}$ fired within a period $T$
\begin{equation*}
\nu = \frac{n_\text{spikes}}{T}.
\label{eqratecoding}
\end{equation*}

% LATER SOMEWHERE IN THE THEORY PART 

From a practical point of view, this method is time consuming and can therefore not be the basis upon which fast decisions are taken by the brain. The measurement time can be reduced by shortening the period $T$ to $\Delta T$, but this will yield a more inaccurate firing rate. By repeating the measurement multiple times, the average rate improves the accuracy, but the total measurement duration is prolonged again. Moreover it is not feasible that the exact same input occurs multiple times in a real world problem.

To solve both issues, a population average rate $\nu_\text{pop}$ can be used. The averaged firing rate of a population with $n$ neurons $\left\langle\nu \right\rangle_\text{pop}$ yields an accurate rate despite the reduced measurement time $\Delta T$
\begin{equation*}
\left\langle\nu \right\rangle_\text{pop} = \frac{\sum_i n_{\text{spikes},i}}{n \Delta T},
\end{equation*}
with the number of spikes for a neuron $i$ within the population $n_{\text{spikes}, i}$.
%As this thesis targets neuronal networks of smaller size, the following paragraphs focus on the temporal dimension of neural coding.

In the terminology of the \gls{lif} model, a presynaptic spike train $S_j$ from a source $j$ can be associated with a mean firing rate $\nu_j$. This is based on the assumption that the spikes of the input follow a Poisson process and therefore the firing rate of a neuron is well described by a Poisson distribution in most cases (\citealp{averbeck2009poisson}), which in turn legitimates the use of a mean firing rate and a certain accuracy. In this way the time average of the synaptic input current for a \gls{lif} neuron can be expressed in terms of the incoming firing rates $\nu_{\text{in}, j}$ and their respective synaptic weight $w_j$
\begin{equation*}
\left\langle \gls{isyn} \right\rangle = \sum_j w_j \nu_{\text{in}, j}.
\end{equation*}

\paragraph{Training with Rate Coding}
With a rate coding approach, a spiking feedforward network obtains properties from an \glspl{ann}. In particular, the activation function of a node in an \gls{snn} associates a given input rate $\nu_{\text{in}}$ with an output rate $\nu_{\text{out}}$ 
\begin{equation*}
	\gls{activation}(\nu_{\text{in}}) = \nu_\text{out}.
\end{equation*}
Unlike for individual spikes, the gradient of the activation function can now be formulated with the use of rates.  This allows the use of typical deep learning methods such as \glsfirst{sgd} for rate-based \glspl{snn}.
 
%The similarity to \glspl{ann} implies that typical training methods such as \acrfull{sgd} will work with rate-based \glspl{snn} too. In a spiking feedforward network the output of a node is determined by the activation function \gls{activation}, which is given by the neuron model in place.
% In case of the \gls{lif} neuron, the activation function is similar to a \gls{relu}, with a significant difference, that the rate will no keep increasing but rather saturates quickly at a maximum frequency $\nu_\text{max}$ which is limited by the refractory period $\nu_\text{max} = \nicefrac{1}{\gls{refrac}}$.
%The choice for a sigmoid activation function in combination with a cross entropy loss have already been motivated in. With a few adjustments the activation function can be implemented for a \gls{lif} model and rate-based coding.

The choice for a sigmoidal activation function in combination with a cross entropy loss has already been motivated in \cref{supervisedtraining}. In the following, the necessary adjustments to shape the response of the \gls{lif} neuron model into a sigmoidal activation function are presented.

The stimulation of a \gls{lif} neuron can be expressed by the synaptic input current \gls{isyn} or the corresponding input firing rate $\nu_\text{in}$. In analogy to the activation of an \gls{ann} the synaptic current is split up into an input and bias term $\gls{isyn} = I_\text{in} + I_\text{bias}$. The input spike rate $\nu_\text{in}$ of the neuron scales linearly with the synaptic current and therefore $\nu_\text{in} \propto I_\text{in}$. The activation function of the \gls{lif} neuron is then approximated by 
\begin{equation}
\frac{1}{\gls{activation}(\nu_\text{in})} = \frac{1}{\nu_\text{out}} \approx \gls{refrac} + \gls{tau_m} \frac{\gls{thres} - \gls{v_reset}}{\gls{isyn}},
\label{fireratehigh}
\end{equation}
given that the input rate is high and the time constants of the membrane \gls{tau_m} and the synaptic input current \gls{tau_syn} are smaller than the refractory period \gls{refrac} (c.f. \citealp{brunel2000dynamics}). The output rate saturates at a maximum rate $\nu_\text{max} \approx \nicefrac{1}{\gls{refrac}}$ for a high enough input rate.

In the limit $\gls{thres} - \gls{isyn} \gg \sigma$, i.e. for low input rates, the activation function yields
\begin{equation}
\nu_\text{out} \approx \frac{(\gls{thres} - \gls{isyn})}{\gls{tau_m}\sigma \sqrt{\pi}} \exp\left(-\frac{(\gls{thres} - \gls{isyn})^2}{\sigma^2}\right),
\label{fireratelow}
\end{equation}
with the fluctuations of a single excitatory input source $\sigma$.

\begin{figure}[ht!]
	\begin{center}
		\input{figures/theoretical_free_membrane.pgf}
	\end{center}
	\caption[Simulation of the Gaussian free membrane potential distribution.]{Simulation of the Gaussian free membrane potential distribution, i.e. the distribution of the membrane potential in absence of any spiking mechanisms. The distribution $f_{\gls{v_mem}}$ centers around \gls{v_leak}. The width of the distribution correlates to the amount and synaptic weight of injected noise spikes. The part of the distribution colored in black, exceeds the threshold potential \gls{thres} indicated by a red vertical line and would lead to spikes.}
	\label{theoretical_vleak_w_noise}
\end{figure}

Without external noise, the fluctuations are small, reflecting only the variations of the input spike train and thus the activation function shows a steep incline. One way to smoothen the course of the function is to increase $\sigma$, e.g. by injecting additional Poisson spikes. A continuous noise stimulation with these noise spikes leads to a Gaussian free membrane potential distribution $f_{\gls{v_mem}}$ which is centered around the resting potential \gls{v_leak} as depicted in \cref{theoretical_vleak_w_noise}. The term \emph{free} refers to the absence of any spiking mechanisms of the membrane. In a naive approach, the part of the free distribution that exceeds a certain threshold potential correlates to the number of fired spikes. This neglects non-vanishing effects from the fire dynamics of the membrane which have been investigated in more detail by \cite{petrovici12phdthesis}. The impact of these dynamics can be reduced by the use of short time constants for the synaptic input and the membrane.

However, despite the strongly simplified picture, this view still offers a correct intuition of how the threshold, the leak potential and the strength of the noise effect the free membrane potential and in turn change the shape of the activation function: More noise leads to a broader distribution and thus a more gentle incline of the output rate. The synaptic input moves the distribution to either a lower or a higher mean value. And changing the threshold corresponds to adding a bias term to the synaptic input. The latter has been motivated by \cite{petrovici2016stochastic}. The bias term in the neuron's activation can be adequately replaced by adapting the relative distance $\delta V$ between the resting potential and the threshold
\begin{equation}
b \propto \delta V = \gls{v_leak} - \gls{thres}.
\label{biasproptothreseq}
\end{equation}
With the approximations made in \cref{fireratehigh} and \cref{fireratelow} as well as a suitable choice of neuron parameters, the activation function yields an sigmoidal shape (\cref{theoreticalactivationfunction}). The slope of the sigmoid can be easily adapted by changing the synaptic strength of the input spike trains. And the alignment along the x-axis can be done by the bias. Finding the appropriate neuron model parameters to create a well shaped sigmoid can be tricky. The parameters in-use are listed in \cref{hardwarevssimulationtable} in the appendix.

\begin{figure}[h!]
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\input{figures/theoretical_activation_function_variableweight_new.pgf}
		\label{theoreticalactivationfunctionweight}
	\end{subfigure}	
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\input{figures/theoretical_activation_function_variablebias_new.pgf}
		\label{theoreticalactivationfunctionbias}
	\end{subfigure}
	\caption[Simulation of a sigmoidal activation function using Poisson noise]{Simulation of a sigmoidal activation function using Poisson noise. The shape of the activation function \gls{activation} depends on several parameters of the \gls{lif} neuron and the Poisson input  spike trains. \textbf{(\subref{theoreticalactivationfunctionweight})} The input current that is seen by the neuron is directly proportional to the weight $\gls{isyn} \propto w_\text{in} \nu_\text{in}$. In this way, a lower weight causes a lower input current which in turn leads to a smaller incline of the output rate (\emph{black}) than for a higher input weight (\emph{red} or \emph{blue}). \textbf{(\subref{theoreticalactivationfunctionbias})} The \emph{red} curve is centered at zero input which requires a small threshold offset of \SI{0.75}{\V}. A positive potential difference $\delta V$ shifts the sigmoid (\emph{black}) to the left. With a lower threshold the neuron starts to fire earlier. The potential difference of the \emph{blue} and \emph{black} curve relative to the centered sigmoid equals $\pm \SI{5}{\V}$.}
	\label{theoreticalactivationfunction}
\end{figure}

%For zero input, i.e. $\nu_{\text{in}} = 0$ and the , apart from the injected noise, the threshold is set such that $\nu_\text{out} = \nicefrac{\nu_\text{max}}{2}$. Varying the strength of the synaptic input directly changes the shape of \gls{activation} as well (c.f. \cref{transferfunction}).
\pagebreak
\subsection{Temporal Coding}
\label{temporalcoding}
\glspl{snn} can also model more complex processes which depend on the \gls{isi} or a specific spike time. In comparison to rate coding, using the exact spike time to encode information is more exposed to noise. In turn, the response time of such coding is highly increased, since there is no need to establish a certain firing rate first. The use of temporal coding has been observed in biology multiple times and it has been proven to be of high importance (\citealp{gerstner1996neuronal} and \citealp{rieke1999spikes}). Another advantage over rate coding is the efficiency that comes with the use of single spikes as each spike costs energy. Sparse temporal coding condenses the necessary resources to solve a task to a minimum.

So far, training \glspl{snn} with hidden units has been proven to be a difficult task (\citealp{pfeiffer2018deep}). With the binary nature of spikes, the neuron's activity is non-differentiable on an individual spike level. Hence, well proven methods from conventional deep learning cannot be simply transfered to temporal coding with \gls{snn}. A promising workaround was suggested with \emph{SuperSpike}, where a supervised learning algorithm is implemented for \glspl{snn} by using a surrogate gradient (\citealp{zenke2018superspike}).

\paragraph{Supervised Training with Temporal Coding}
%Until now, only few people have successfully trained \glspl{snn} with hidden units. The main issue arises from the non-differentiable dynamics of spikes. A promising approach, solving this issue was proposed with \emph{SuperSpike}, a surrogate gradient descent implementation for \glspl{snn} (\citealp{zenke2018superspike}).

As before, only feedforward networks on the basis of \gls{lif} neurons are considered. The training routine of SuperSpike can again be split up into a forward and backward pass.

The \textbf{forward pass} changes only slightly compared to \glspl{ann}. Instead of a continuous input and output, the formalism of the \gls{lif} is used. The presynaptic activity of neuron $j$ is given by the spike train $S_j$ and the postsynaptic activity of neuron $i$ by the spike train $S_i$. Again the activation function \gls{activation} is determined by the dynamics of the \gls{lif} neuron.
%Todo: "surrogate gradient" is not really introduced but just used -> fix this
%Todo: spike train formalism (maybe in biological part with synapses, etc. ), presynaptic/postsynaptic 
%Todo: axonal delay is not considered for the application of the hardware
%Todo: auxilary function: threshold! -> f(x) = x/(1 + |x|) and f'(x) (1 + |x|)-2 => $\sigma'(U_i) = (1+|U_i - \vartheta|)^{-2}$

As stated in section \ref{supervisedtraining}, most training approaches involve the optimization of a certain loss function $\mathcal{L(\mathbf{\theta)}}$ that depends on the network's parameters $\mathbf{\theta}$. In the \textbf{backward pass} of the SuperSpike formalism the loss is given by the \emph{van Rossum distance} (\citealp{rossum01novel}) of a target spike train $S^*_i$ and the actual output spike train $S_i$
\begin{equation}
\label{vonrossumdistance}
\mathcal{L} = \frac{1}{2} \int^t_{-\infty}dt' \left[\left(\alpha \ast S^*_i - \alpha \ast S_i \right)(t')\right]^2,
\end{equation}
with a smooth double exponential kernel $\alpha$. 

The computation of the gradient for \ref{vonrossumdistance} with respect to $\mathbf{\theta}$ requires the derivative of a spike train $S_i$ for a neuron $i$. In particular, $\frac{\partial{S_i}}{\partial{w_{ij}}}$ which is undefined for the time of a spike and zero elsewhere. SuperSpike circumvents this issue by rendering the spike train with a smooth auxiliary function $\sigma(V_{\text{m},i})$ of the membrane potential $V_{\text{m},i}$ for a neuron $i$ and thus the ill-defined gradient of the spike train can be replaced by a surrogate derivative $\sigma'(V_{\text{m},i})$
\begin{equation*}
\frac{\partial S_i}{\partial w_{ij}} \quad \rightarrow \quad \sigma'(V_{\text{m},i})\frac{\partial V_{\text{m},i}}{\partial w_{ij}}.
\end{equation*}

In SuperSpike $\sigma(V_{\text{m},i})$ is chosen to be a fast sigmoid
\begin{equation}
\sigma(V_{\text{m},i}) = \frac{\beta (V_{\text{m},i} - \vartheta)}{1 + \beta |V_{\text{m},i} - \vartheta|},
\label{auxilliaryfunction}
\end{equation}
with a slope parameter $\beta$. The surrogate partial derivative yields $\sigma'(V_{\text{m},i}) = \frac{\beta}{(1 + \beta |V_{\text{m},i} - \vartheta|)^2}$. Other auxiliary functions will work too. Common choices are for example piecewise linear or exponential functions.

At a first glance, it appears that the problem has just been shifted to computing the partial gradient of the membrane potential instead. When the potential $V_{\text{m},i}$ is formulated as a spike response model for \gls{lif} neurons it again depends on the output spike train $S_i$ (\citealp{gerstner2014dynamics}). However, under the assumption of a low output rate the gradient can be approximated by $\frac{\partial V_{\text{m},i}}{\partial w_{ij}} \approx (\epsilon \ast S_j)$ with $\epsilon$ another double-exponential kernel corresponding to the shape of a \gls{psp}. Plugging in the approximation and the formulation of the gradient as a surrogate gradient yields

\begin{equation}
\frac{\partial w_{ij}}{\partial t} = \eta \int_{-\infty}^{t} dt'
\underbrace{\left(\alpha \ast (S^*_i - S_i)\right)}_{= e_i \; \text{(Error)}} 
\; \alpha \ast 
\Big(\underbrace{\sigma'(V_{\text{m},i})}_{\text{Post}} 
\underbrace{\left(\epsilon \ast S_j\right)}_{\text{Pre}}\Big),
\label{superspikeweightupdateeq}
\end{equation}
with the learning rate $\eta$. 

The formulation for a hidden layer is similar, except for the calculation of the hidden error. For simplicity the network is reduced to only one hidden layer structure. The error signal of the $i \text{-th}$ unit $e^\text{hidd}_i$ in the hidden layer is given by backpropagation of the error $e_k$ from the output layer
\begin{equation*}
e^\text{hidd}_i = \sum_{k} w_{ik} e_k,
\end{equation*}
with the feedforward weights $w_{ik}$ between the hidden and output layer. As for the gradient descent, feedback alignment can also be used within the SuperSpike formalism by using a random weight matrix. Despite the formal restriction to a single hidden layer, the method can be easily adapted for multiple hidden layers.

%ToDo: Literatur Book zu Deep Learning (siehe downloads UniRechner)
%Überleitung zu rate coding vs sparse coding (maybe read again SuperSpike17/19 first))
%A \gls{snn} offers many advantages compared to a classical \gls{ann}. On of the major differences is that sparse coding allows the user to compress a lot of information into a single spike. Also not spiking at a sppecifencodes information.  This efficient way of transporting information makes \glspl{snn} a contender for any energy sensitive form of computing. More importantly it also opens up the doors to understanding spike-based computing better and thus also the way the human brain works.

\section{Neuromorphic Hardware}

The great success of deep learning in the recent years has also increased the demand for new specialized hardware that handles the huge amount of parallel computing more efficient than the conventional von Neumann architecture. Among various approaches, neuromorphic hardware is arguably the closest form of computing to the human brain. As its inspiration, it is designed to be robust to noise or malfunctioning sectors, it is highly energy efficient and remains flexible throughout the learning process. These properties are of high interest for any deep learning applications. As a consequence, several platforms\footnote{Among others, SpiNNakker and BrainScaleS by EU's Human Brain Project, Loihi by Intel and Truenorth by IBM} have been launched by big industry corporations and academia over the last years.
 
\begin{figure}[hb!]
	\begin{subfigure}{0.49\textwidth}
		\centering
		\caption{}
		\includegraphics[width=0.98\textwidth]{figures/hxsetup_img_close_edited3.pdf}
		\label{cubesetup}
	\end{subfigure}	
	\begin{subfigure}{0.49\textwidth}
		\centering
		\caption{}
		\includegraphics[width=.98\textwidth]{figures/HXcloseup.JPG}
		\label{hxcloseup}
	\end{subfigure}
	\centering
	\caption[The cube setup \acrshort{hx}.]{The cube setup \acrshort{hx}. \textbf{(\subref{cubesetup})} The chip (\textit{red}) is mounted on the chip carrier board and is protected by a cover. The chip carrier board (\textit{gray}) is then mounted on the xBoard (\textit{yellow}) which is connected to the \acrshort{fpga} over the cube iBoard (\textit{blue}). \textbf{(\subref{hxcloseup})} Close-up of the newest \gls{bss2} single chip. The analog core of the neuromorphic chip is bonded before a protective cover is placed over it. Picture taken by M{\"u}ller, 2020.} 
	\label{hxsetup}
\end{figure}

In the context of this thesis, the presented learning strategies from \cref{neuralcoding} are implemented on an analog neuromorphic platform called \glsfirst{bss2}. This mixed-signal accelerated emulation for \glspl{snn} is based in Heidelberg and is the result of a long term cooperation with the EU, namely the Human Brain Project (HBP).
%
The \gls{bss2} platform is designed to perform various plasticity algorithms on-chip. The core of the platform is based upon a complete redesign of its predecessor the \gls{hicann}. By reducing the CMOS manufacturing process from \SI{180}{\nano \m} to \SI{65}{\nano \m} several new features could be included on the new core. One of the main renewals is a general purpose unit which can be used for any on-chip computation and a specialized vector unit that efficiently provides parallel access to observables from the analog core. The features have been implemented step by step on various prototype versions. The following paragraphs will focus on the specifications of the prototypes used for the experimental implementations of the discussed deep learning methods (c.f. \cref{bss2prototypes}).

On the \gls{dls} the redesign of the \gls{lif} neuron model is implemented. Moreover, the chip features a general purpose unit and a vector unit which are summarized as the \gls{ppu}. The \gls{dls} has been manufactured at a reduced size of $32$ neurons and $32 \times 32$ synapses to avoid unnecessary costs. The \gls{hx} is the first full-size prototype of the \gls{bss2} platform with 512 \gls{adex} neuron circuits and 256 possible synaptic connections per neuron. In addition to the \gls{ppu}, the chip features on-chip event routing and HAGEN extension, an early realization of a neuromorphic system which basically implements an on-chip analog matrix multiplication (\citealp{schemmel2020accelerated}). Another renewal are dedicated noise generators, which come in handy when working with sigmoidal activation functions and rate coding (c.f. \cref{ratecoding}).

\begin{table}[t!]\centering\ra{1.3}
	\begin{tabular}{@{}rcccc@{}}\toprule
		& 					& Neuron Model	& Neurons 	& Synapses 	\\ \midrule
		& \acrshort{dls}	& \gls{lif}		& 32 		& $32 \times 32$	\\
		& \acrshort{hx}		& \gls{adex}	& 512	 	& $512 \times 256$	\\
		\bottomrule
	\end{tabular}
	\caption[Overview of relevant \gls{bss2} prototypes.]{Overview of relevant \gls{bss2} prototypes. The \acrshort{dls} is produced at a reduced size to avoid unnecessary costs. The \acrshort{hx} is the first functioning full-size chip of the \gls{bss2} platform.}
	\label{bss2prototypes}
\end{table}

%The new features have been implemented step by step on various prototype versions. On the \gls{dls} the newly designed \gls{lif} neuron model was tested in combination with a in a reduced size of only $32$ neurons and a corresponding $32 \times 32$ synapse array allows all to all connectivity. The \gls{dls} and \gls{hx} chip. The \gls{hx} is the first full-size prototype of the \gls{bss2} platform with 512 \gls{adex} neuron circuits and 256 possible synaptic connections per neuron is especially designed to investigate various on-chip plasticity algorithms. Therefore it features two \glspl{ppu}, on-chip event routing and the HAGEN extension. The latter is an early realization of a neuromorphic system which basically implements analog matrix multiplication on-chip (\citealp{schemmel2020accelerated}).

%The new features have been tested step by step on smaller prototype systems, e.g. on the \gls{dls} the newly designed \gls{lif} neuron was tested. To avoid unnecessary costs, the size was reduced to $32$ neurons and a corresponding $32 \times 32$ synapse array allows all to all connectivity. Besides the Hagen extension and the \gls{adex} neuron, the main features of the \gls{bss2} platform, such as the \gls{ppu}, are already available on the prototyped versions.

%The experiments conducted within this thesis are done either on the \gls{hx} or the \gls{dls}. In the following section the individual parts of the chip are discussed.
%The manufacturing has been outsourced to the Taiwan Semiconductor Manufacturing Company (TSMC) using a standardized $65 \si{nm}$ low-power and low-leakage CMOS technology.

\subsection{Architecture of \gls{bss2}}

\begin{figure}
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\includegraphics[width=0.9\textwidth]{figures/bss2architecture_wtext.pdf}
		\label{hxstructure}
	\end{subfigure}	
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\includegraphics[width=0.9\textwidth]{figures/ppu_overview.pdf}
		\label{hxppu}
	\end{subfigure}
	\caption[Overview of the \gls{bss2} architecture.]{Overview of the \gls{bss2} architecture. \textbf{(\subref{hxstructure})} The neuromorphic platform is divided into a digital and analog core which is connected to an external host via an FPGA. Two plasticity processing units provide computational power for on-chip training.  Figure taken from \citealp{schemmel2017internal} \textbf{(\subref{hxppu})} The plasticity processing units are divided into a general purpose part which is based on a \SI{32}{\bit} architecture and a vector unit. The latter enables efficient parallel data processing of analog parameters and observables. Figure taken from \cite{friedmann2016hybridlearning}}
\end{figure}

The design of both prototypes can be divided into an \emph{analogue} and a \emph{digital} core (c.f. figure \ref{hxstructure}). The external communication is established by a \gls{fpga} accessing eight serial Low Voltage Differential Signaling (LVDS) links. The interface not only manages read and write instructions but handles any spike-event data in both directions.

Depending on the chip version, the analogue core contains the physical implementation of either a \gls{lif} or an \gls{adex} neuron model (\citealp{aamir2018dls2neuron} and \citealp{aamir2018mixed}). The analog neuron model parameters can be tuned by setting respective bias currents with a \SI{10}{\bit} \gls{dac}. Each neuron can be controlled and adjusted individually (\citealp{hock13analogmemory}). 

The biological time constants of neurons and synapses are usually in the order of 1 to 100 milliseconds. The \textit{in-silico} implementation of the neuron models causes a temporal speed-up compared to their \textit{in-vivo} counterpart, leading to chip-time constants of a few microseconds. This acceleration is possible due to the supra-threshold dynamics of CMOS transistors. 

\begin{figure}
	\begin{subfigure}[c]{0.7\textwidth}
		\centering
		\caption{}
		\includegraphics[width=\textwidth]{figures/synapse.png}
		\label{synapsecircuit}
	\end{subfigure}	
	\begin{subfigure}[c]{0.29\textwidth}
		\centering
		\caption{}
		%\vspace{-1cm}
		\input{dlsv2overview.tex}
		\label{synapsearraysketch}
		\vspace{1cm}
	\end{subfigure}
	\caption[Synapse circuit overview on \gls{dls}]{Synapse circuit overview on \gls{dls}. \textbf{(\subref{synapsecircuit})} Synapse drivers inject the presynaptic activity row-wise as either inhibitory or excitatory spikes. The \SI{6}{\bit} addresses of the presynaptic neuron is compared at each synapse with a local \SI{6}{\bit} address. If the addresses match, the spike is relayed to the corresponding neuron at the bottom of the synapse grid. The synaptic strength can be configured by a \SI{6}{\bit} weight. The two correlation sensors (causal and anti-causal) record \gls{stdp} traces. Figure taken from \cite{friedmann2016hybridlearning}. \textbf{(\subref{synapsearraysketch})} Overview of the row-wise synapse drivers and the synapse array. Figure adapted from \cite{billaudelle2019versatile}.}
	\label{synapseschematics}
\end{figure}

On the full-size chip, the neurons are connected by a grid of $512 \times 256$ synapses ($32 \times 32$ on the smaller one). The activity of presynaptic neurons is injected row-wise by dedicated synapse drivers as either excitatory or inhibitory spikes. Each synapse has access to a \SI{6}{\bit} decoder address and compares it to a \SI{6}{\bit} label of the incoming spikes (see \cref{synapseschematics}). If they match, the spike is relayed to the corresponding neuron at the bottom of the synapse grid. The efficacy is thereby configured by a \SI{6}{\bit} weight.

The analog core features several observables which are relevant for the implementation of plasticity rules. The firing rate of a neuron for instance is recorded by a \SI{10}{\bit} spike counter on the \gls{dls}. The counter has been replaced by a smaller \SI{8}{\bit} version on the full-size chip to save some space. In addition, every synapse features two correlation sensors (causal and anti-causal) which are designed to record \gls{stdp} traces. Other relevant observables are the traces of the synaptic input current or the membrane potential. The latter is key for the implementation of temporal-based plasticity rules such as SuperSpike.

When training a highly accelerated analog system such as \gls{bss2} platform, a fast computation of any plasticity rule is indispensable. A \gls{cadc} provides row-wise parallel access to the \gls{stdp} traces with a total of $2 \times 32$ respectively $2 \times 512$ \gls{cadc} channels (one channel per correlation sensor per synapse row). The on-chip vector unit then guarantees an efficient access to the \gls{cadc} readout by the use of \gls{simd} operations. On the newer \gls{hx}, the \gls{cadc} routing possibilities have been extended such that the membrane potential can be accessed as well. Before this renewal the membrane potential was recorded by a fast \gls{madc} which accesses only one neuron at a time. With the lack of parallelization a fast in-experiment use cannot be realized and is therefore feasible for training purposes. This limitation is compensated by a better accuracy and resolution making the \gls{madc} a useful debugging and observation tool.

Apart from dedicated spike counters, the digital neuron back end registers any spiking event and transfers them to the digital core logic, where the events are merged with any activity coming from the noise spike generators or \glspl{ppu} as well as from an external source. The events are then rerouted back into the synapse grid accordingly, enabling recurrent connections and multilayer network structures.

These observation features for the analog neuronal dynamics are then combined by the general purpose unit with which complex plasticity rules can be implemented on-chip (see \cref{hxppu}). The \gls{hx} features even a second \glspl{ppu} to provide enough computational power for the increased chip size. The software capabilities of the general purpose unit for the new chip are under continuous development. For instance a fast access to an external memory will be released in near future. The current hardware and software constraints of both prototypes will be discussed in a final chapter, after the experimental implementations have been presented (\cref{circles} and \cref{superspike}).

%On the newer \gls{hx}, the \gls{cadc} is equipped with more routing possibilities, with which the membrane potential can be accessed as well. This feature  Unlike the \gls{cadc} the \gls{madc} provides only access to one observable at a time. This limitation is compensated with a better accuracy, making it a useful debugging and observation tool, that can be accessed from the digital core of the chip. By the use of the \gls{madc} the membrane potential can also be recorded on the \gls{dls}. However, with the lack of parallelization an in-experiment use is not feasible.
%When training a highly accelerated analog system, such as \gls{bss2}, a fast computation of any plasticity rule is indispensable. To provide sufficient computational power, the chip is equipped with two \glspl{ppu}, each containing a general-purpose unit that is extended with a special function unit implementing \gls{simd} operations. The special function unit has vector-wise access to the synapse array as well as to the results of the \gls{cadc} and will be further referred to as \emph{vector unit}.
%The communication with an external host is streamed out to the controlling Field Programmable Gate Array (FPGA). The existing FPGA solution developed for \gls{bss1} was simply transfered to \gls{bss2}.
%The link between chip and external host is established via an \gls{fpga} accessing eight serial Low Voltage Differential Signaling (LVDS) links. This interface handles read/write instructions and manages spike event data in both directions. %The FPGA grants access for the PPU to greater memory storages than the one provided on-chip. Access to large training datasets is vital for most learning tasks.