\chapter{Introduction}

%\epigraph{``So that I may perceive whatever holds \\The world together in its inmost folds."}{--- \textup{Johann W. v. Goethe}, Faust I}

Humankind has been striving for a conceptional understanding of the physical world, even long before Faust famously questioned his knowledge about what it is that holds the world together. With the scientific progress and gained knowledge over within the last decades, new innovation have emerged, that changed the earth into a new digitally interconnected place. Yet, the understanding of our own human brain remains elusive.

From a biological point of view, neurons are responsible for processing and transmitting information in the brain by forming \glspl{snn} (\citealp{gerstner2014dynamics}). Studying these networks is promising approach to gain a better insight into the brain's mechanisms. Not only provide \glspl{snn} a guiding principle to study the brain, but are of great interest for an efficient implementation of deep neural networks on neuromorphic hardware (\citealp{pfeiffer2018deep}). They inherit multiple favorable characteristics from their biological inspiration such as parallelized and event-based information processing, noise tolerance and a low power consumption.

The success of deep learning is followed by an ever growing demand of specialized computational resources (\citealp{mayer2020scalable}). Besides building larger supercomputers using conventional hardware, more efficient solutions such as neuromorphic hardware have seen an increased interest. As of today, several well known tech companies have launched their own neuromorphic platforms. IBM started to work on \emph{TrueNorth} in 2008 (\citealp{akopyan2015truenorth}), Intel presented the \emph{Loihi} chip in 2018 (\citealp{davies2018loihi}) and Google began selling the \emph{Coral} dev-board in 2019 (\citealp{cass2019taking}). Even before neuromorphic hardware caught the attention of the big industry names, academic projects had already been started. Among others, the EU's Human Brain Project (HBP) funds two promising approaches: \emph{SpiNNaker}, a digital based neuromorphic supercomputer located in Manchester (\citealp{furber2014spinnaker}) and \emph{\gls{bss}}, a mixed-signal accelerated emulation for spiking neural networks based in Heidelberg (\citealp{brainscales_webpublic}).

In recent years, the field of neuromorphic computing has focused on finding novel training algorithms for \glspl{snn} that can compete with the widely successful conventional deep learning methods. The main problem arises from the non-differentiability of individuals spikes making well proven optimization methods such as gradient descent inapplicable. The ideas for workarounds range from imitating \glspl{ann} with rate coding to using surrogate gradients (\citealp{tavanaei2019deep}).

In this research two candidates for supervised training are presented and then implemented on different prototypes of the \gls{bss2} platform. In a first approach, a \gls{snn} is emulated as a classical deep \gls{ann} on the prototype chip \acrshort{dls} using rate coding. The network is then successfully trained by an on-chip implementation of gradient descent, i.e. only on-chip resources are used to compute and apply parameter changes. A second experiment is conducted on a revision of the first full-size prototype of the \gls{bss2} platform, the \acrshort{hx}. With an additional built-in observation feature of the membrane potential, a spiking variant of gradient descent called \emph{SuperSpike} (\cite{zenke2018superspike}) can be used to train a deep \gls{snn}. On the current revision a complete and high-performance on-chip implementation has not yet been feasible, due to several hardware bugs. Instead, SuperSpike is implemented by recording the analog neuron dynamics on-chip and processing them with the support of a host. Lastly, the challenges of the different learning methods and potential future improvements will be discussed.\\