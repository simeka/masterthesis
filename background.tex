\chapter{Background}
Starting my final year project at the Electronic Vision(s) group, I soon realized the diversity of their research. From the biological view of the human brain to electronic circuit laws further to machine learning algorithms, the required knowledge to work in this field is broad and manifold. In the next paragraphs, I will introduce the most important concepts and physical backgrounds upon which the presented research in this works is based on.

\section{The Biological Neuron}

It is estimated that the human brain contains around $10^11$ neurons. However, the cortex is not only made out of spiking neurons but also a high number of glia cells which provide energy and structural support to the neurons. The neuron itself can be divided into three different functional parts: dendrites, soma and axon (see figure -> ref to Figure). On a high level description the dendrites are responsible for managing all inputs from any neurons and transmit them further to the soma where the received information is processed. The output of the soma is then distributed via the axon to other neurons.  The input and output consists of short electrical pulses, spikes, which can either excite or inhibit the whole neuron membrane and thus lead to an in- or decrease of its electrical potential (\textit{membrane potential}). The link between two neurons is established by a synapse and usually a single neuron connects to around $10^4$ other ones. There is actually a small but physical gap between two connecting neurons called synapse cleft. When the \textit{presynaptic} neuron sends out a spike, a complex chain of reaction is triggered at the synaptic cleft, where various neurotransmitter will be released. Once a transmitter has connected with a corresponding receptor on the \textit{postsynaptic} side of the cleft, the receptor will activate ion channels which are responsible for raising or lowering the membrane potential of the postsynaptic neuron. By this the chemical reaction is converted back into an electrical signal.\\

In addition to the ion channels a neuron also contains ion pumps. These pumps restore the balance of the ion concentration in and outside of the membrane over time. The equilibrated state of the membrane potential is referred to as the \textit{resting potential}. Apart from an excited and a resting state, the neuron can also be \textit{refractory}. Once a spike has been fired, the neuron's membrane becomes hyperpolarized and the potential decrease even below the resting point. At the beginning of the refractory state it is impossible for the neuron to spike again. Due to the ongoing hyperpolarization also after this first period it remains hard but not impossible to send out a spike. The typical dynamics of the membrane are depicted in figure [ref to figure]. One should keep in mind that the shape of a spike doesn't carry any information.  The communication between neurons is rather encoded in the frequency and correct timing of exciting and inhibitory spikes. The various methods of spike coding are presented in section \ref{deeplearning}. 

\subsection{Leaky Integrate-and-Fire Model}

The input of a neuron is usually given by a number of spikes at certain times. This collection of spikes is called \textit{spiketrain}. In a simplified model, the neuron's membrane $V_{\text{m}}$ sums over the input current $I_{\text{stim}}$ generated by the spiketrain ("integrate") and triggers a response once a certain threshold potential $\mathcal{\theta}$ is crossed ("fire"). After releasing the response, the membrane is reset to $V_{\text{reset}}$ where it remains without any change for a refractory period of $\tau_{\text{ref}}$. Given there is no further input, the neuron will then leak slowly towards the resting potential. The dynamics of the membrane are given by a single differential equation:
\begin{align}
C_{\text{m}} \frac{dV_{\text{m}}}{dt} &= -g_{\text{leak}} (V_{\text{m}} - V_{\text{leak}}) + I_{\text{stim}} \\
\end{align}
The membrane's time constant is given by the ratio of $C_{\text{m}}$ the membrane capacitance and $g_{\text{leak}}$ the leakage conductance: $\tau_{\text{m}} = \frac{C_{\text{m}}}{g_{\text{leak}}}$. The firing part and reset behavior is formulated by the following conditions:
\begin{align}
V_{\text{m}}(t_{\text{fire}}) &\ge \theta \Leftrightarrow \text{Neuron fires at time } t_{\text{fire}} \\
V_{\text{m}}(t) &= V_{\text{reset}} \quad \forall t \in (t_{\text{fire}}, t_{\text{fire}} + \tau_{\text{ref}}] 
\end{align}


The Leaky Integrate-and-Fire (LIF) model makes use of the fact that the shape of an input spike stays approximately the same. It was first described by Lapicque in 1907 [REF missing]. The model describes no spatial structures of the neurons. A LIF neuron is thus a point-like integrator that neglects any non-linear dynamics coming from strategically positioned connections to sources of excitatory or inhibitory spikes. Moreover a LIF neuron doesn't keep any memory of previous spikes after firing, since the membrane will always be reset to $V_{\text{reset}}$. These limitations make it impossible for the model to correctly describe neuronal behavior for repetitive firing. However, the main neuron dynamics are well described by the LIF neuron and it thus has been a popular choice for neuromorphic hardware [ref to BSS1?].\\

The limitations of the LIF neuron led to a demand for a more sophisticated model, namely the Adaptive-Exponential Integrate-and-Fire short AdEx model. The two adaptations made are an additonal adaptive current the allows the membrane to remember its previous state after firing and an exponential voltage feedback. Depending on the sign of the adaptive current, the neuron will either be stunted to fire again after it has sent out its first spike or it is engaged to keep on firing. In the example given in figure [ref to figure] the former is the case. The exponential voltage feedback on the other hand, enables the neuron to generate a proper fire response.\\

\subsection{Synapses}
input

weights

plasticity



\subsection{Neuromorphic Hardware}

BSS-1 (DLSv2)
BSS-2 (HX)



\section{Deep Learning}
\label{deeplearning}
The last decade was filled with inventions and technologies of machine learning nature. Among the many tools machine learning has provided to the scientific community, deep learning is one where many still struggle to grasp its mechanism and capabilities. However, it has become a useful and powerful tool to solve complex tasks. One approach is to understand such a task in terms of a hierarchy of concepts. Each concept is based upon a combination of simpler ones. Going done on a hypothetical ladder towards the easiest concept available, creates a deep structure with many layers. This is why it's called \textit{deep learning} . [Deeplearning Book, Intro]

Another important aspect is the representation of information. Given a picture of a cat, any computer will have a hard time to map the important information of the sensor data to the class \textit{cat}. A multiple layer network on the other side can extract parts of the picture in its deeper layers and eventually gather all necessary information to detect the cat. On neuromorphic hardware spikes transmit data from one neuron to another. There are a number of different ways to use spikes for information encoding. \textbf{Rate coding} for instance uses a high number of spikes to establish a certain firing rate of the neuron. The information is not encoded in a single spike or the time interval between two spikes but in a number of spikes within a certain period defining the \textit{rate}. \textbf{Temporal coding} on the other hand focuses on the interspike interval (ISI). This type of coding is more exposed to noise but for some applications it might be simply too slow to establish a certain fire rate first. The ISI on the other can be adapted quickly. When only a small set of all available neurons is used to describe an input pattern, its called \textbf{sparse coding}.\\

The different types of neural coding require each an adapted training method. Rate coding can be treated similar to ANNs and thus classical gradient descent or feedback alignment will work just fine. Temporal or sparse coding use single spikes to encode information. Their strong non-linearity can be treated with a surrogate model. SuperSpike is a surrogate gradient descent method for SNNs that has been imposed by Friedemann Zenke et al 2017 and tackles the non-linearity of spikes.\\

\subsection{Classical Training Methods for SNNs}

% make sure SNNs and ANNs is somewhere before explained 
%maybe put this later (like experiment setup or so)
The learning process of a neural network can be divided into a forward and backward pass: During the forward pass the output value of each node in the network is evaluated. The backward pass calculates in which direction the networks parameters are updated. In the following, a single hidden layer network with a sigmoidal shaped transfer function $\phi$ and a cross-entropy loss function is assumed. \\

\subsubsection{Forward Pass}
The activation $\mathbf{a}$ of the hidden layer is given by the weighted input $\mathbf{x}$ plus a bias term $\mathbf{b}$. The output $\mathbf{y}$ is the result of the transfer function $\phi$:
\begin{align}
\mathbf{a} &= W^{\text{(h)}} \, \mathbf{x} + \mathbf{b} \\
\mathbf{y} &= \phi(\mathbf{a})
\end{align} 
The same principle is applied to the output layer. The weight matrices $W^{\text{(h)}}$ and $ W^{\text{(o)}}$ have the appropriate shape to fit the number of input, hidden and output nodes. The bias term is a vital parameter which allows the network to shift the dynamic range of a single neuron.

\subsubsection{Gradient Descent (Backward Pass)}

Most training approaches for neural networks involve optimizing a loss function $\loss(\mathbf{x})$ by changing the networks parameters $\mathbf{x}$ in the correct direction. This can be achieved by moving in the along the negative gradient of the $\loss$. The new set of parameters $\mathbf{x'}$ is then given by [compare deep learning book ch 4]
\begin{equation}
\mathbf{x'} = \mathbf{x} - \eta \, \nabla\loss(\mathbf{x}).
\end{equation}
whereby $\eta$, the learning rate, defines how fast the update is applied.\\

As an example the update of the weight $\delta W$ is calculated. First it necessary to know what the loss function looks like. This is why the computation starts in the output layer. Since we have used a sigmoidal transfer function and imposed a cross-entropy loss the following relation holds for the error in the output layer:
\begin{equation}
\mathbf{e} = ... = \frac{\partial\mathcal{L}}{\partial \mathbf{y}} \;	\frac{\partial \mathbf{y}}{\partial \mathbf{a} }
\end{equation}

Now the weight update in the output layer is

\begin{align}
\delta W =& - \eta \frac{\partial \mathcal{L}}{\partial W} 
= - \eta \;
\underbrace{\frac{\partial\mathcal{L}}{\partial \mathbf{y}} \;
	\frac{\partial \mathbf{y}}{\partial \mathbf{a} }}_{=\mathbf{e}\; \text{(error)}} \;
\frac{\partial \mathbf{a}}{\partial W}
= - \eta \, (\mathbf{e} \cdot \mathbf{x}^T)\\
\end{align}

where $\mathbf{y}$ and $\mathbf{a}$ denote the output respectivley the activation of the layer. The hidden layer computes similarly:

\begin{equation}
\delta W = - \eta \;
(W^T \cdot \mathbf{e}) \;
\nabla \phi(\mathbf{a}) \;
\mathbf{x}^T\\
\end{equation}

This procederes is often refered to as back propagation. Another method is to not use the transpose of the actual weight matrix but a randomized one 

Ref to lillicrap -> random weight matrix just exchange W with A. constraints on A (45° stuff and so on...one or two more sentences)

\begin{equation}
\text{feedback alignment:} \quad \delta W = - \eta \;
(B \cdot \mathbf{e}) \;
\nabla \phi(\mathbf{a}) \;
\mathbf{x}^T\;\\
\end{equation}

\subsection{Surrogate Gradient (SuperSpike)}

novel blabal


%ToDo: Literatur Book zu Deep Learning (siehe downloads UniRechner)
%
%Überleitung zu rate coding vs sparse coding (maybe read again SuperSpike17/19 first))
%A spiking neural network (SNN) offers many advantages compared to a classical artifical neural network (ANN). On of the major differences is that sparse coding allows the user to compress a lot of information into a single spike. Also not spiking at a sppecifencodes information.  This efficient way of transporting information makes SNNs a contender for any energy sensitive form of computing. More importantly it also opens up the doors to understanding spike-based computing better and thus also the way the human brain works.
