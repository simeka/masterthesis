\begin{abstract}
%\textbf{Novel Deep Learning Algorithms for Neuromorphic Hardware}
Neurons are the biological basis of how information is conveyed and processed in the brain. By forming spiking neural networks, the brain is able to learn and remember. Not only have these networks provided an insight into the brain's mechanisms but offer an alternative to artificial neural networks in deep learning. When implemented on analog neuromorphic hardware, they inherit a variety of favorable properties from their biological inspiration, such as parallel and event based information processing, noise robustness and a high energy efficiency. These characteristics are of great interest, since using conventional deep learning for real-world problems requires the resources of large supercomputer clusters. However, modern learning algorithms for neuromorphic hardware still have a hard time competing with the great success of deep learning. In this thesis, the performance of two candidates for training spiking neural networks is evaluated on the analog neuromorphic platform BrainScales-2. First, a rate-based spiking neural network is trained using gradient descent as a standalone on-chip experiment. Second, a surrogate gradient descent algorithm, SuperSpike, is implemented by processing the on-chip recorded analog neuron dynamics on a host.
\end{abstract}
\renewcommand{\abstractname}{Zusammenfassung}
\begin{abstract}
%\textbf{Neuartige Deep Learning Algorithmen f√ºr Neuromorphe Hardware}
Neuronen bilden die biologische Basis f\"{u}r den Austausch und die Verarbeitung von Informationen im Gehirn. Mithilfe von spikenden neuronalen Netze kann das Gehirn lernen und sich erinnern. Diese Netze dienen nicht nur dazu die Vorg\"{a}nge im Gehirn zu veranschaulichen, sondern sind auch eine Alternative zu Deep Learning mit k\"{u}nstlichen neuronalen Netzen. Im Einsatz auf analoger neuromorpher Hardware, \"{u}bernehmen spikende neuronale Netze einige vorteilhaften Eigenschaften ihrer biologischen Vorbilder, wie zum Beispiel paralleles und eventbasiertes Verarbeiten von Informationen, eine erh\"{o}hte Fehlertoleranz  sowie eine hohe Energieeffizienz. Unter dem Gesichtspunkt, dass die Ressourcen eines H\"{o}chstleitungsrechners ben\"{o}tigt werden, um mit klassischem Deep Learning echte Probleme l\"{o}sen zu k\"{o}nnen, sind die zuvor genannten Leistungsmerkmale besonders interessant. Allerdings k\"{o}nnen sich moderne Lern-Algorithmen f\"{u}r neurmorphe Hardware noch nicht mit dem Erfolg von k\"{u}nstlichen Neuronalen Netzen auf konventioneller Hardware messen. In dieser Arbeit wird die Funktion und Performance von zwei Algorithmen auf der analogen neuromorphen Plattform BrainScales-2 evaluiert. In einem ersten Experiment wird ein raten-basiertes spikindes neuronales Netz mit einem Gradientverfahren in einer alleinstehenden ``on-Chip" Implementierung trainiert. Das zweite Verfahren beruht auf einem Surrogate-Modell des Gradientenverfahrens, welches mit der Unterst\"{u}tzungen eines Hosts die analogen Ergebnisse der neuronalen Dynamik auf der Hardware prozessiert.
\end{abstract}y


