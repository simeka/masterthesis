\chapter{Classification of the Circles Data Set on \acrshort{bss2}}
\label{circles}
At the beginnings of the machine learing era, \cite{perceptron} have shown that a simplified single-layer neuronal network, the perceptron, cannot solve the XOR operator, which resembles a combination of the logical operators OR and NOT AND resulting in an ``exclusive or". The XOR operator represent a class of problems, that are non-linear and thus require a multi-layer network structure to be solved. In modern machine learning solving the XOR operator has therefore become sort of a sanity-check for novel network designs and algorithms.

Circles is the first of two deep learning experiments implemented on the \gls{bss2} platform and focuses on a rate coding approach which has been motivated in \cref{ratecoding}. After successfully testing the \acrlong{sgd} approach with a quick XOR implementation on the prototype \gls{dls}, the difficulty of the task has been increased with the circles data set. Additionally, the task is implemented as an on-chip experiment, i.e. the training implementation of circles is executed using only on-chip available resources, such as the dedicated \acrlong{ppu}. For monitoring purposes of the training process certain hardware parameters and training related variables are continuously read-out through the FPGA interface.


%A detailed description of the \textit{on-chip} implementation is presented in the next paragraphs.

%- single hidden layer network with 11 neurons (2 synapse lines necessary for each neuron -> 22)
%- 2 noise sources (exc/inh)
%- 2x2 input sources (exc/inh)
%(A single hidden layer network is trained using rate coding and feedback alignment.) maybe restructure the intro somehow...


\section{Circles Task}
\label{circlestask}
A region bounded by two concentric circles with radiuses $r_{\text{inner}}$ and $r_{\text{outer}}$ is called \emph{annulus}. The circles data set describes in principle a set of points $p = p(x,y)$ in a two-dimensional plane, which are in either of two disjunct annuli, each representing a class 

\begin{align}
\text{class}(p) =
\begin{cases}
0 ,&\quad \quad r_{\text{inner}}^2 < x^2 + y^2 < r_{\text{outer}}^2, \\
1 ,&\quad \quad R_{\text{inner}}^2 < x^2 + y^2 < R_{\text{outer}}^2,
\end{cases}
\end{align}
with the radiuses of the first and second annulus $r_{\text{inner}}$, $r_{\text{outer}}$ and  $R_{\text{inner}}$, $R_{\text{outer}}$ respectively. The goal of the task is to find a decision boundary that successfully separates the both annuli.

In the next step, the data points need to be translated into a fire rate. With the limitations of the hardware, the range of the coordinates of point $p$ is limited to a signed 8-bit integer ($x, y \in [-128,127]$). Each point is then mapped onto a frequency depending on either of the coordinates in order to maintain a two dimensional feature space. The input rate in ``x-direction" $\nu_{\text{in, x}}$ is given by
\begin{equation}\label{inputfrequency}
\nu_{\text{input, x}}(x) = \nu_\text{max} \cdot \frac{x + 128}{255}.
\end{equation}
with a maximum fire rate $\nu_\text{max}$ that is given by a dedicated Poisson spike train generator.

\begin{figure}
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\inputpgf{figures}{nu_x_input.pgf}
		\label{nuxinput}
	\end{subfigure}
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\inputpgf{figures}{nu_y_input.pgf}	
		\label{nuyinput}
	\end{subfigure}
	\caption[Rate coding for the circles experiment.]{Rate coding for the circles experiment.\textbf{(\subref{nuxinput})} Each point $p(x,y)$ of a potential circles data set with 100 points per class is mapped to a frequency depending on the ``x-direction". The value of the frequency is color coded according to the colorbar. The figure resembles one of two input nodes in the input layer of the network. \textbf{(\subref{nuyinput})} The second input node depends on the ``y-direction".  As an example let's assume a data point from the lower right corner e.g. $p(100,-100)$. The corresponding input rate generated by the first input node is given by $\nu_{\text{in, x}}(100) \approx \SI{450}{\kilo \Hz}$ and the second node yields $\nu_{\text{in, y}}(-100) \approx \SI{55}{\kilo \Hz}$.}
	\label{circlesinputs}
	
\end{figure}

With a slight modification of the task, the implementation effort on the \gls{ppu} can be reduced. The smallest radius $r_{\text{inner}}$ is set to zero and largest $R_{\text{outer}}$ is replaced by the 8-bit limitations of the input. The remaining two boundaries are set to $r_{\text{outer}} = \sqrt{8000}$ and $R_{\text{inner}} = \sqrt{13000}$, leaving an unused area in between. This area is where the network will try to place the decision boundary. The input frequencies and shape of a potential circles data set with 100 data points per class is illustrated in \cref{circlesinputs}. 

To identify the classes in the output layer only a single unit is required. Each data point $p$ yields a certain output fire rate $\gls{nuout}(p)$ in the output layer, which is compared to a target rate $\nu^*$ depending on the class of point $p$
\begin{align}
\label{circlestarget}
\nu^*(p) =
\begin{cases}
\nu_0^* = \SI{32.6}{\kilo \Hz} ,&\quad \quad \text{class}(p) = 0,\\
\nu_1^* = \SI{93.5}{\kilo \Hz} ,&\quad \quad \text{class}(p) = 1.
\end{cases}
\end{align}
The mismatch between the target and fire rate then determines the error of the output layer $e^{(o)}$
\begin{equation}
e^{(o)}(p) = \gls{nuout}(p) - \nu^*(p).
\end{equation}
A reasonable choice of the target rates is crucial to the training success. If the rates are either too close or too far from each other, the network will have troubles to solve the task. The decision boundary, the rate separating both classes, is chosen to be the mean of both target rates
\begin{equation}
\nu_\text{DB} = \frac{\nu_0^* + \nu_1^*}{2} = \SI{63.05}{\kilo \Hz}.
\end{equation}


\section{Poisson Spike Trains Generation}
\label{poissonspiketrains}
In the introduction to rate coding (\cref{ratecoding}) the mean fire rate $\nu$ of a neuron has been defined by the number of spikes $n_\text{spikes}$ sent within a period $T$
\begin{equation}
\nu = \frac{n_\text{spikes}}{T},
\end{equation}
and it was mentioned that this formalism is only well defined, if the temporal distribution of the spikes within a spike train is based on Poisson processes.

One way to numerically generate such a Poisson spike train is to repeatedly perform a Bernoulli process on a short time interval $\Delta t$ until a spike train of period $T$ is filled with spike or no-spike events. The Bernoulli process is equal to an unfair coin flip with probability $p$, which is set depending on the desired fire rate $\nu$ by $p = \nu \cdot \Delta t$.

Given the coin's probability equals one, the fire rate reaches its maximum at $\nu_\text{max} = \nicefrac{1}{\Delta t}$. With respect to the hardware limitations, in particular the time required to generate an artificial spike on the \gls{ppu}, the maximum fire rate is limited to $\nu_\text{max, ppu} = \nicefrac{1}{\SI{0.44}{\mu \s}} = \SI{2.27}{\mega \Hz}$. However, at maximum rate, the spike generation can no longer be assumed to follow a Poisson Distribution and should only be used for frequencies $\nu \ll \nu_\text{max, ppu}$.

% Stopped here, rewrite again not schlÃ¼ssig yet...
The use case of the spike generator in the final setup is to generate noise spike trains for the sigmoid transfer function and to map the translate the circles data set into input rates. Only the latter requires high input rates up to $\SI{500}{\kilo \Hz}$, the noise rate is only in order of a few $\si{\kilo \Hz}$.

%In theory the maximum fire rate of a \gls{lif} neuron $\nu_\text{max}$ is give by $\nu_\text{max} = \nicefrac{1}{\gls{refrac}}$.
%In the final experiment setup, the output rate of an individual neuron is further constraint by a chosen 8-bit resolution of the spike counts and a measurement period of a few milliseconds to a measurable rate in order of few $\SI{100}{\kilo \Hz}$.
%will be divided into four independent channels, each producing at max a fourth of $\nu_\text{max, hw}$.
On the \gls{ppu} the Bernoulli process is implemented by comparing a certain probability with a randomly drawn number. A popular method to generate random numbers on a system with limited memory and computational resources is the \emph{xorshift} (\citealp{marsaglia2003xorshift}). A random number is thereby generated by repeatedly applying the XOR operator on a seed variable and a bit-shifted versions of itself. The \emph{bit shift} of a variable $x$ to the left by $n$, written as $x\;{\scriptstyle<<}\;n$, corresponds to the multiplication by $2^n$. In analogy, a shift the right, $x\;{\scriptstyle>>}\;n$, is equivalent to the division by $2^n$. The xorshift can be implemented in C as follows.

%\begin{minted}{C}
%uint32_t xorshift32(uint32_t* seed)
%{
%*seed ^= *seed << 13;
%*seed ^= *seed >> 17;
%*seed ^= *seed << 5;
%return *seed;
%}
%\end{minted}

\section{Transfer Function on \gls{dls}}
%The continuous stimulation with Poisson spike trains leads to a Gaussian free membrane potential distribution $f_{\gls{v_mem}}$ centered around the resting potential \gls{v_leak} as depicted in \cref{vleak_w_noise}). In a naive approach, the part of the distribution that exceeds a certain threshold potential correlates to the number of fired spikes. This neglects non vanishing effects from the fire dynamics of the membrane which have been investigated in more detail by \citealp{petrovici12phdthesis}. The impact of these dynamics can be reduced by the use of very short time constants for the synaptic input \gls{tau_syn} and the membrane \gls{tau_m}.
%
%However, despite the strongly simplified picture, this view still offers a correct intuition how the threshold and leak potential as well as the strength of the noise effect the free membrane potential and in turn change the shape of the transfer function: more noise leads to a broader distribution and thus a more gently incline of the sigmoid; synaptic input moves the distribution to either to a lower or higher mean value; moving the threshold corresponds to an additional bias term.
%\begin{figure}
%	\begin{center}
%		\input{figures/activation_function_vmem_distr_with_thres.pgf}
%	\end{center}
%	\caption[Gaussian free membrane potential distribution on \gls{dls}.]{Gaussian free membrane potential distribution on \gls{dls}. The distribution of the membrane potential $f_{\gls{v_mem}}$ centers around \gls{v_leak}. The width of the distribution correlates to amount of injected noise spikes. Without additional noise, the induced spread from the intrinsic hardware noise is a magnitude lower. The part of the distribution that exceeds the threshold potential leads to spikes. The post fire dynamics of spiking activity changes the shape of the distribution, as the membrane is set to a reset potential before it leaks back to the resting potential (c.f. \citealp{petrovici12phdthesis}). Short time constants for the synaptic input and the membrane as well as sufficiently high input rates, reduce this effect.}
%	\label{vleak_w_noise}
%\end{figure}

The theoretical ground for a sigmoid transfer function for a \gls{lif} neuron has been motivated in section \cref{ratebasedtraining}. The overall transfer function then depends on several parameters
\begin{equation}
\gls{transfer} = \gls{transfer}(\gls{nuin}, b, \gls{refrac}, f_{\gls{v_mem}}),
\end{equation}
with the free membrane distribution $f_{\gls{v_mem}} = f_{\gls{v_mem}}(V; \gls{v_leak}, \nu_\text{noise})$. 
The bias term $b$ in the neuron's activation is again replaced by the relative distance $\delta V$ between the resting potential and the threshold, since $b \propto \delta V = \gls{v_leak} - \gls{thres}$. 

\subsubsection*{Calibration}\label{calibration}
In an uncalibrated state of the \gls{dls}, the transfer functions of the neurons do not align with regards to their maximum rate and the bias with which the sigmoid can be moved along the x-axis (see \cref{transferfunction_wout_calib}). Up to a certain degree the misalignment is self-corrected when training the neural network. In some cases, it can even be beneficial to the performance of training if the network is initialized with a randomly uncalibrated state. When training with \glspl{ann}, it is not unusual to inject artificial noise into the network to boost the performance. However, the dynamic range of the individual neurons on the neuromorphic chip is limited and are therefore calibrated such that they overlap with each other.

\begin{figure}
	%\captionsetup[subfigure]{justification=centering}
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\caption{}
		\input{figures/uncalibrated_activation_function.pgf}
		\label{transferfunction_wout_calib}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\textwidth}		
		\caption{}
		\input{figures/calibrated_activation_function.pgf}
		\label{transferfunction_w_calib}
	\end{subfigure}
	\caption[Calibration of the sigmoid activation function on \acrshort{dls}.]{Calibration of the activation function on \acrshort{dls}. \textbf{(\subref{transferfunction_wout_calib})}: In the uncalibrated state the maximum (\gls{refrac}) and center rate (\gls{v_leak}) deviates due to analog imperfections of the hardware. \textbf{(\subref{transferfunction_w_calib})}: The calibration aligns the dynamic range of individual neurons, which will helps the training.}
\end{figure}

The on-chip calibration uses a binary search algorithm to find suitable \gls{dac}-values for the analog neuron parameters (\citealp{binarysearchsource}). In a binary search, a target value is compared to the middle value of an sorted array. Depending on the outcome of the comparison the lower or the upper half is eliminated from searched space. The search is repeated with new boundaries set by the remaining half. In a worst case scenario, after $\mathcal{O}(\log(n))$ repetitions the algorithm finds the target value in an array with $n$ entries.

For the calibration of the transfer function, the sorted array is replaced by the range of the 10-bit \gls{dac}-value and instead of direct comparison, the measured output rate of a neuron using the updated analog parameter is compared to a target rate.

In case of the maximum fire rate, the calibration is only depends on one parameter, the refractory period \gls{refrac}. The maximum output rate is measured using a high input rate, zero bias and no noise
\begin{equation}
\gls{transfer}(\nu_\text{max, hw}, 0, \gls{refrac}, f_{\gls{v_mem}}) = \frac{1}{\gls{refrac}} \quad \text{ with } \nu_\text{noise} = 0.
\end{equation}
The target rate of the maximum output rate is set to $\gls{nuout}^* \approx \SI{111.3}{\kilo \Hz}$ which corresponds to $256$ recorded spikes over a measurement period of $T=\SI{2.3}{\milli \s}$.

The alignment along the x-axis is slightly more complicated. A neuron approximately yields half of its maximum rate if there is input except for the noise sources and the bias is set to zero, i.e. $\delta V = 0 \Leftrightarrow \gls{v_leak} = \gls{thres}$
\begin{equation}
	\gls{transfer}(0, 0, \gls{refrac}, f_{\gls{v_mem}}) = \frac{1}{2\gls{refrac}} \quad \text{ with } \nu_\text{noise} = \SI{70}{kHz}.
\end{equation}
In the binary search, the \gls{dac} value of the resting potential is calibrated while the threshold is kept fixed at $\gls{thres} \approx \SI{450}{\milli \V}$. The target center rate is set to $\nu_\text{center}^* = \nicefrac{\gls{nuout}^*}{2}$.

In theory, the choice of the potential for the fixed threshold is kind of arbitrary, but during testing, the neurmorphic hardware in use proved to have more and less stable working points. 

The calibration of the maximum and center rate is repeated several times to reduce the potential the influence of the other yet uncalibrated parameter. In \cref{transferfunction_w_calib} the final calibrated state is shown. 



\begin{figure}
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\input{figures/single_calibrated_transfer_function_w_various_weights.pgf}
		\label{dlsactivationfunctionweight}
	\end{subfigure}	
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\caption{}
		\input{figures/activation_function_w_bias.pgf}
		\label{dlsactivationfunctionbias}
	\end{subfigure}
	\caption[Measurement of the sigmoid transfer function on \gls{dls}]{Measurement of the sigmoid transfer function on \gls{dls}. A sigmoid activation function can be achieved by applying additional Poisson noise spike trains onto the membrane. \textbf{(\subref{dlsactivationfunctionweight})}: The dependency of the input weight is given due to its direct proportionality to the synaptic input current $\gls{isyn} \propto w \nu_\text{in}$. \textbf{(\subref{dlsactivationfunctionbias})}: Changing the value of \gls{thres} and thereby the potnetial difference $\delta V$ to resting potential is interchangeable with adding a bias to the synaptic input of the neuron. The activation function is thereby shifted in either direction along the x-axis. This has been measured for all neurons in use for the circles task. The shifted activation functions do not need to align per neuron, since the threshold parameter is set individually and can be easily adapted during the training.}
\end{figure}
%
%\begin{figure}
%	\begin{center}
%		\input{figures/activation_function_w_bias.pgf}
%	\end{center}
%	\caption[Sigmoid transfer function on \gls{dls}.]{Sigmoid transfer function on \gls{dls}. The resulting transfer function of the broadened free membrane distribution is set to  Shifting \gls{thres} and thereby changing the relative potential difference $\delta V$ is interchangeable with adding a bias term to the \gls{lif} neuron's activation.}
%	\label{transferfunction_with_bias}
%\end{figure}

%The bias term $b$ is again replaced by the relative distance $\delta V$ between the resting potential and the threshold $b \propto \delta V$. As the analog core brings more and less favorable operating points, the resting potential is kept constant and only the threshold is shifted to change the bias (c.f. \cref{transferfunction_with_bias}).



\section{Experiment Setup on \gls{dls}}
\label{circlesimplementation}
The circles task is solved by training a single hidden layer network on-chip using a \acrlong{sgd}. The controlling unit of the experiment is the \gls{ppu}. In analogy to the training approach the implementation is can be divided into a forward and a backward pass. %. After a proper initialization routine, the training loop is started: In a first step the input rates of a randomly drawn data point $p(x,y)$ are generated (c.f. \cref{poissonspiketrains}) and the forward pass is performed. The observables are then evaluated with the feedback alignment routine on the \gls{ppu} (backward pass) and the resulting in updates for weights and biases are applied, before the next iteration starts. 

\subsubsection*{Forward Pass}

The neuromorphic architecture on the \gls{dls} offers 32 neurons and a synapse grid of $32 \times 32$ synapses. Any input to the network is injected row-wise by dedicated synapse drivers as either excitatory or inhibitory spikes. At the respective column the spikes are relayed down to the designated neuron. 

The chosen training method requires a continuous evolution of the synaptic weights from negative to positive, but a single synapse cannot alternate between the signs of the input without changing the synapse driver's configuration which is time-consuming. Instead of a single input row, two rows are used for the same input - one in excitatory and one inhibitory mode. In each column one of the double-synapses is active and corresponds to the current sign of the synaptic weight. As the weight changes the sign, the active synapse is silenced and the silent one is reactivated. 

The first layer of the network, the input layer, is connected to the hidden units by using a total of four rows and eleven columns. A dedicated on-chip event routing injects the postsynaptic spike trains of the hidden neurons back into the network, with each hidden neuron requiring a new double-row. The inputs generated by the hidden layer is then summed by a single column over the respective $22$ rows into the output unit. Another excitatory and inhibitory row is designated for the noise input, connecting the generated Poisson spike trains to each neuron.

A Poisson spike train generator on the \gls{ppu} provides a continuous stream of random spikes which is split into four branches, two for the excitatory and inhibitory noise source and two for both input units. Each noise branch has to supply twelve neurons with Poisson spike trains. In an attempt to avoid cross-correlation between the individual neurons the branches are further divided into eight channels and thus only four will share the noise with other ones.

Todo: bias in forward pass?


%With the double-row structure, a total of six rows is used for the various inputs: two for the noise generation and four for the two-dimensional input. The recurrent connections of the hidden layer require another $22$ rows. In \cref{network_structure} the exact row assignment is sketched. Other than during the calibration process, where each neuron was calibrated individually, now the noise source needs to provide Poisson spike trains for all neurons in the hidden and output layer. As a compromise the source is divided further into eight channels. Four neurons will share the noise with other ones. The frequency of the noise in both layers is set to $\nu_\text{noise} \approx \SI{1.1}{\kilo \Hz}$ using weights with $w_\text{noise} = \pm 15$. The received input frequency varies for each layer. The input layer provides an input rate of up to $2 \times \SI{500}{\kilo \Hz}$ for each hidden unit. The output frequency of hidden units is limited by the spike counter and measurement period to $\gls{nuout} \approx \SI{110.9}{\kilo \Hz}$, leading to a theoretical maximum of $\gls{nuin}= 11 \times \gls{nuout} \approx 1219.6$ for the output layer. 
%A presynaptic spike train, e.g. generated by the \gls{ppu}, is injected row-wise into the $32\times32$ synapse array. Then, the synapses compare a dedicated address label of the incoming spike with their own. Once they match, the spike is weighted and forwarded to the column-wise connected neurons at the bottom as either inhibitory or excitatory current. Recurrent connections are made available by the use of a spike router, which routes the postsynaptic spike trains generated by a neuron back into the synapse array.
%A single input row cannot provide alternating negative and positive inputs to the same neuron. Changing the synapses configuration simply costs too much time. To ensure a continuous evolution of the weight from negative to positive, two rows are used simultaneously for the same input - one in excitatory and one inhibitory mode. As the weight changes sign, the unused row is set silent. At the start of the experiment the weights are randomly initialized using a uniform distribution limited to $w_{ij} \in [-25, 25]$.
%The initial conditions of the network have been proven to crucial for its learning success. A ``silent" hidden units has a hard to time become active. Adding a weight depending term to the initial zero bias $b_i^\text{init} \propto \sum_j w_{ij}$ establishes a certain initial fire rate and solves the problem for most cases.

\subsubsection*{Backward Pass}
The \textbf{backward pass} evaluates the output rates of each node in the network that have been obtained by the forward pass and computes an update for weights and biases according to the rules derived by \acrshort{sgd}.

The general-purpose unit of the \gls{ppu} operates with a 32-bit architecture without access to floating point types. To increase the precision of the parameter updates, the computation is bit-shifted to the left by five. The results are then stochastically rounded and and shifted back to right before being applied to the hardware. Stochastic rounding has been proven to be a viable workaround for deep learning with parameters of limited precision (\citealp{limitedprecisionpaper}.

A direct access to the analog weight parameters from the general-purpose unit is time-consuming and computation intensive due to the lack of parallelizations. With the feedback alignment approach for \acrshort{sgd} the information of the current weight becomes irrelevant to the plasticity rule and only the change of the weight needs to be applied. This can be implemented by using the vector unit. The \gls{simd} instructions allow parallel access to all weights and therefore it is not only used to efficiently apply the weight updates but also to silence and reactivated the respective synapses in the double-row structure of the synapse array. 

The limited range of the weights causes them to max out rather quickly. The implementation of a simply stochastic weight regularization using the parallel access of the vector unit solved the issue. The regularization of both, excitatory and inhibitory weights is given by
\begin{align}
w_{ij}^\text{inh},\; w_{ij}^\text{exc}\equiv w_{ij} \in [0,2^6) \quad  \Rightarrow \quad \text{Reg}(w_{ij}) &= w_{ij} - \left(\left(w_{ij} \ll 1\right) \gg 6\right) \nonumber \\
&= w_{ij} - \left\lfloor \frac{\lfloor 2 \cdot w_{ij}  \rfloor}{2^{6}} \right\rfloor,								
%	&= w_{ij} - \left\lfloor \lfloor 2 w_{ij} \rfloor \cdot 2^{-6} \right\rfloor
\end{align}
and the correction is only applied with the probability $p=\nicefrac{1}{4}$.
%The \emph{Assembly} code-base for this implementation has been written by Sebastian Billaudelle and Benjamin Cramer.

The bias is set by lowering or raising the \gls{dac}-value of the threshold potential. The resolution of the capacitive parameter memory equals to roughly $\SI{1.75}{\milli \V}$, which in turn translates to a change of the input frequency $\delta \nu \approx \SI{9.9}{\kilo \Hz}$. As a comparison the maximum input frequency of a neuron is around $\SI{1}{\mega \Hz}$, but a more realistic magnitude of the average presynaptic activity are a few $\SI{100}{\kilo \Hz}$. Therefore a change of several \gls{dac}-lsb results in a rather drastic update. 

Compared to the speed of the accelerated neuromorphic core, the \acrlong{dac} is slow. Instead of an instant and large change of the bias, a continuous and slow implementation can actually be beneficial in this case.

\subsubsection*{Initial Conditions and Hyper-Parameters}
The final experiment setup involves the setting of initial conditions and the tuning of so called hyper-parameters. The hyper-parameters describe a set of parameters, that is not changed by the training process but their choice often decides whether or not the training succeeds. The a subset of the most relevant hyper-parameters is listed below.

In the final setting, the frequency of a noise channel is set to $\nu_\text{noise} \approx \SI{1.1}{\kilo \Hz}$, using only a fraction of the maximum frequency. The input units, on the other hand, exploit most of the available resources with fire rates up to $\SI{500}{\kilo \Hz}$.

All weights of both layers are therefore randomly initialized using a uniform distribution that is limited to $w_{ij}^{(l)} \in [-25, 25]$. The input of the noise is also weighted $w_\text{noise} = \pm 15$.

A ``silent" hidden unit has a hard to time become active during the training process. Adding a weight depending term to the initial zero bias $b_i^\text{init} \propto \sum_j w_{ij}$ establishes a certain initial fire rate and solves the problem for most cases.

\begin{table}\centering\ra{1.3}
	\begin{tabular}{@{}rcllcllcll@{}}\toprule
		&layer	 & & $w_{ij}^\text{init}$		& $b_i^\text{init}$ & $\nu_{\text{in}}$	& & $\eta_\text{w}$	& $\eta_\text{b}$\\ \midrule
		&hidden  & & $\in[-25, 25]$	& $\propto \frac{1}{2} \sum_j w_{ij}$ & $\SI{500}{\kilo \Hz}$	& & $165e4$			& $2.3e4$		\\
		&output  & & $\in[-25, 25]$	& $0$ & $\SI{110.9}{\kilo\Hz}$& & $5.5e4$			& $0.23e4$		\\ \bottomrule
	\end{tabular}
	\caption{The different settings for the layers is listed in the table. The weights are initialized as a uniform distribution within the given boundaries. The bias set to zero, that means $\gls{thres} = \gls{v_leak}$ but for the hidden layer a weight dependent factor $\frac{1}{2} \sum_j w_{ij}$ ensures that a certain fire rate is established at the initial state. The input rates are given per input, such that the total possible input is multiplied by the number of input connections. The total possible input for a hidden unit is thus $2 \times \nu_\text{in}$. The learning rates may appear unusually high but they incorporate the norming as well and are tuned in a way that both layers are active (see \cref{network_monitoring}).}
\end{table}


\section{Training}
For a better illustration of the training process, a balanced data set of $200$ randomly drawn points is chosen to reflecting the current state at each node (see \cref{learning_process_s5}). Depending on the configuration of weights and bias, each hidden unit forms a tilted or even inverted version of the input schemes in \cref{circlesinputs}. These variations are then composed into a combination that is displayed in the output layer. At this stage of the learning process, the output unit doesn't reflect the target well, resulting in a high error and therefore also high updates for weights and biases.
\begin{figure}
	\label{learning_process_s5}
	\include{learning_process_s5}
	\caption{The initial state of the network shows several variations of the inputs in the hidden layer and a combination of them in the output layer, that does not yet compare to the target. Similar to the contour lines on a map one can guess the sigmoidal step in the hidden units. }
\end{figure}

 After the first 500 iterations, the total loss has almost been cut in half and the network starts to find a setting of avail. Less useful variants in the hidden layer are further modified, in particular their bias and the weights of their input connections. The beneficial ones are rewarded with a stronger connection to the output layer and further refinement of their bias and input weights.

\begin{figure}
	\label{learning_process_s500}
	\include{learning_process_s500}
	\caption{After 500 iterations some training success are noticeable: The hidden units six and seven show an interesting behavior. Despite of their relatively same shape only one unit keeps the shape (seven). The other one changes the input weights and the bias by quite a bit which could mean, that the randomly assigned weight of the feedback matrix is negative and thus results in an inverted feedback. However, the network still has plenty of other hidden units to choose from.}
\end{figure}

At 2500 iterations, see \cref{learning_process_s2500}, the network found a set of hidden units and a combination with which the target can be recreated in the output unit. The accuracy has almost reached $100 \%$.

\begin{figure}
	\label{learning_process_s2500}
	\include{learning_process_s2500}
	\caption{At 2500 iterations the accuracy of the networks is almost at $100 \%$. The network}
\end{figure}

The same data set that has been used to visualize the training process is now used to monitor the  performance of the training after each iteration by calculating the \gls{rmse} and the accuracy (see \cref{circles_acc}). The \gls{rmse} is given by
\begin{equation}
	\text{\gls{rmse}} = \sqrt{\frac{\sum_p e(p)^2}{n_\text{points}}}
\end{equation}

The accuracy is derived by comparing the number of correctly identified points $n_{true}$ to the total number of points in the set. Whether a point is correctly identified is determined by the decision boundary (see \cref{circlestask}).

\begin{equation}
\text{Accuracy} = \frac{n_\text{true}}{n_\text{points}}
\end{equation}

\begin{figure}
	\label{circles_acc}
	\begin{center}
		\input{figures/circles_learning_performance.pgf}
	\end{center}
	\caption{The learning performance of stochastic gradient descent on the circles task, i.e. batch size is equal to one, is monitored by two measures: the accuracy and the root mean square error (RMSE).}
\end{figure}

For further monitoring purposes, the evolution of the weights and biases are tracked for the hidden and output layer (see \cref{network_monitoring}).
\begin{figure}
	\centering
	\label{network_monitoring}
    \input{figures/network_evolution_circles.pgf}
	\caption{Describe what happens in the picture...}
\end{figure}